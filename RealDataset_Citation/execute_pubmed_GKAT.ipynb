{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"execute_pubmed_GKAT.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyOe2TwjodbGaE2i+63K53E5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"59LJ5du07Q3k"},"source":["# link colab to google drive directory where this project data is placed\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PiIoZchw7R8_"},"source":["%cd  /content/gdrive/My Drive/GraphAttnProject/CoraCiteseer/CitationExperiment/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qadH6ryk7pOH"},"source":["import numpy as np\n","\n","# this project is based on tensorflow 2\n","import tensorflow as tf \n","print(tf.__version__)\n","\n","from process import *\n","#from models import *\n","from gat import *\n","\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E1UdPb6D7R6c"},"source":["\n","# parameter settings for cora dataset\n","class args_config_pubmed():\n","    \n","    def __init__(self):\n","      \n","        self.nb_epochs = 100000\n","        self.patience = 100\n","        self.lr = 0.005  # learning rate\n","        self.l2_coef = 0.0005  # weight decay\n","        self.hid_units = [8] # numbers of hidden units per each attention head in each layer\n","        self.n_heads = [8, 1] # additional entry for the output layer\n","        self.residual = False\n","        self.nonlinearity = tf.nn.elu\n","        self.batch_size = 1\n","        \n","        self.nb_nodes = 19717 # number of nodes in cora dataset\n","        self.nb_features = 500 # number of features \n","        self.out_sz = 3 # number of classes \n","       \n","        self.concat_similarity = False  # uses the graph attention method in paper Graph Attention Networks: https://arxiv.org/pdf/1710.10903.pdf\n","        self.dot_product_similarity = True # uses the graph attention method in paper Attention Is All you Need: https://arxiv.org/pdf/1706.03762.pdf\n","\n","        self.sparse = False # this parameter is set as False for Cora and Citeseer, and probably need to set as True for Pubmed due to memory issue\n","        \n","\n","        self.num_repeats = 15 # number of repeats\n","\n","        # for deep walk algorithm\n","        \n","        self.walk_length = 4 # random walk length for GKAT\n","        self.number_walks = 100 # number of random walks from each node \n","        \n","        self.ffd_drop = 0.6 # in_drop\n","        self.attn_drop = 0.6 # coef_drop\n","        \n","        self.optimizer = tf.keras.optimizers.Adam(lr = self.lr)\n","        \n","        self.seeds = [{\"tf_seed\": 649737, \"np_seed\": 29820},\n","                      {\"tf_seed\": 395408, \"np_seed\": 185228},\n","                      {\"tf_seed\": 252356, \"np_seed\": 703889},\n","                      {\"tf_seed\": 343053, \"np_seed\": 999360},\n","                      {\"tf_seed\": 743746, \"np_seed\": 67440},\n","                      {\"tf_seed\": 175343, \"np_seed\": 378945},\n","                      {\"tf_seed\": 856516, \"np_seed\": 597688},\n","                      {\"tf_seed\": 474313, \"np_seed\": 349903},\n","                      {\"tf_seed\": 838382, \"np_seed\": 897904},\n","                      {\"tf_seed\": 202003, \"np_seed\": 656146},\n","\n","                      {\"tf_seed\": 773885, \"np_seed\": 189288},\n","                      {\"tf_seed\": 849634, \"np_seed\": 419482},\n","                      {\"tf_seed\": 175379, \"np_seed\": 760273},\n","                      {\"tf_seed\": 65097, \"np_seed\": 662295},\n","                      {\"tf_seed\": 636040, \"np_seed\": 440640},\n","                      {\"tf_seed\": 792273, \"np_seed\": 707957},\n","                      {\"tf_seed\": 802029, \"np_seed\": 591393},\n","                      {\"tf_seed\": 283051, \"np_seed\": 599978},\n","                      {\"tf_seed\": 327575, \"np_seed\": 266086},\n","                      {\"tf_seed\": 422558, \"np_seed\": 735819}]\n","                      \n","                            \n","        \n","        \n","        \n","        \n","    def print_args(self):\n","        print(f\"patience: {self.patience} | lr: {self.lr} | l2_coef: {self.l2_coef}\")\n","        print(f\"hid_units: {self.hid_units} | n_heads: {self.n_heads}\")\n","        print(f\"residual: {self.residual} | nonlinearity: {self.nonlinearity}\")\n","        #print(f\"data load seed: {self.data_load_seed}\")\n","        print(f\"dotproduct similarity: {self.dot_product_similarity} | concat similarity: {self.concat_similarity}\")\n","        print(f\"ffd_drop: {self.ffd_drop} | attn_drop: {self.attn_drop}\")\n","        print(f\"num repeats: {self.num_repeats}\")\n","        print(f\"repeats seeds: {self.seeds}\")\n","        print(f\"walk length: {self.walk_length}\")\n","        \n","        \n","        \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gG51fO5tX48e"},"source":["# **Main**"]},{"cell_type":"code","metadata":{"id":"tDfsWcSq8HWd"},"source":["dataset_str = 'pubmed'\n","args = args_config_pubmed()\n","\n","\n","\n","adj, walker, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(dataset_str, args)\n","\n","\n","\n","features, spars = preprocess_features(features)\n","\n","nb_nodes = features.shape[0]\n","ft_size = features.shape[1]\n","nb_classes = y_train.shape[1]\n","\n","features = features[np.newaxis]\n","y_train = y_train[np.newaxis]\n","y_val = y_val[np.newaxis]\n","y_test = y_test[np.newaxis]\n","train_mask = train_mask[np.newaxis]\n","val_mask = val_mask[np.newaxis]\n","test_mask = test_mask[np.newaxis]\n","\n","\n","\n","\n","adj = adj.todense()\n","adj[range(nb_nodes), range(nb_nodes)] = 1\n","adj = adj.astype('float32')\n","\n","\n","\n","# model.walker.walks_dict:\n","  # keys: starting node\n","  # values: list with length = # random walk repeats from each node. Each position stores a single random walk from the starting node\n","\n","# the following code changes walks_dict to frequency matrix, which will be used as GKAT kernel later. \n","freq_mat = np.zeros([args.nb_nodes, args.nb_nodes])\n","\n","for key in walker.walks_dict:\n","  for i in range(len(walker.walks_dict[key])):\n","    for j in range(args.walk_length):\n","      freq_mat[int(key),int(walker.walks_dict[key][i][j])] +=1\n","freq_mat /= args.number_walks\n","\n","dot_prod = np.matmul(freq_mat, np.transpose(freq_mat))\n","\n","# divide the dot_prod kernel by the norm of the kernel\n","deno = np.matmul(np.diagonal(dot_prod)[:, None], np.transpose(np.diagonal(dot_prod)[:, None]))\n","dot_kernel = dot_prod / np.sqrt(deno)  #np.diagonal(dot_prod)[:, None]\n","\n","\n","biases = dot_kernel # if we set biases as dot_kernel, then it is our GKAT\n","biases = adj # if we set biases as adj, then it will be pure GAT\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ZcrvNDzSmZr"},"source":["'''\n","if we set args.sparse == True, then we can run the following code\n","\n","import scipy\n","\n","biases[biases<0.05] =0 # we can replace values less than a threshold (0.05 here) by zero to make the dot_product kernel sparser to save computation time\n","\n","# change the dense biases matrix into sparse matrix, otherwise it will run out of memory\n","if args.sparse:\n","  biases = scipy.sparse.csr_matrix(biases, dtype = np.float32)\n","'''\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r8_TREGW2jfJ"},"source":["\n","\n","args.print_args()\n","\n","\n","train_loss_list_all = []\n","train_acc_list_all = []\n","val_loss_list_all = []\n","val_acc_list_all = []\n","ts_loss_list = []\n","ts_acc_list = []\n","\n","\n","for nr in range(args.num_repeats):\n","\n","    tf.random.set_seed(args.seeds[nr]['tf_seed'])\n","    np.random.seed(args.seeds[nr]['np_seed'])\n","    \n","    model = GAT(args.hid_units, args.n_heads, args.out_sz, args.nb_nodes, False, ffd_drop = args.ffd_drop, attn_drop = args.attn_drop, activation = tf.nn.elu, residual=False)\n","\n","    train_loss_list = []\n","    train_acc_list = []\n","    val_loss_list = []\n","    val_acc_list = []\n","\n","\n","    vlss_mn = np.inf\n","    vacc_mx = 0.0\n","    curr_step = 0\n","\n","    train_loss_avg = 0\n","    train_acc_avg = 0\n","    val_loss_avg = 0\n","    val_acc_avg = 0\n","\n","    model_number = 0\n","\n","    for epoch in range(args.nb_epochs):\n","\n","        ###Training Segment###\n","        tr_step = 0\n","        tr_size = features.shape[0]\n","        while tr_step * args.batch_size < tr_size:                \n","            bbias = biases #[tr_step*args.batch_size:(tr_step+1)*args.batch_size]               \n","            logits_tr, acc_tr,loss_value_tr = train( model, inputs= features[tr_step*args.batch_size:(tr_step+1)*args.batch_size],  bias_mat= bbias, lbl_in = y_train[tr_step*args.batch_size:(tr_step+1)*args.batch_size], msk_in = train_mask[tr_step*args.batch_size:(tr_step+1)*args.batch_size], training=True, args = args)\n","            train_loss_avg += loss_value_tr\n","            train_acc_avg += acc_tr\n","            tr_step += 1\n","            train_loss_list.append(train_loss_avg.numpy())\n","            train_acc_list.append(train_acc_avg.numpy())\n","             \n","        ###Validation Segment###\n","        vl_step = 0\n","        vl_size = features.shape[0]\n","        while vl_step * args.batch_size < vl_size:          \n","            bbias = biases #[vl_step*args.batch_size:(vl_step+1)*args.batch_size]          \n","            _, acc_vl,loss_value_vl = evaluate(model, inputs= features[vl_step*args.batch_size:(vl_step+1)*args.batch_size],  bias_mat= bbias, lbl_in = y_val[vl_step*args.batch_size:(vl_step+1)*args.batch_size], msk_in = val_mask[vl_step*args.batch_size:(vl_step+1)*args.batch_size], training=False, args = args)\n","            val_loss_avg += loss_value_vl\n","            val_acc_avg += acc_vl\n","            vl_step += 1\n","            val_loss_list.append(val_loss_avg.numpy())\n","            val_acc_list.append(val_acc_avg.numpy())\n","            \n","        if epoch % 1 ==0:\n","            print('Seed: %d | Epoch: %d |Training: loss = %.5f, acc = %.5f | Val: loss = %.5f, acc = %.5f' % (nr, epoch, train_loss_avg/tr_step, train_acc_avg/tr_step, val_loss_avg/vl_step, val_acc_avg/vl_step))\n","        \n","      \n","        ###Early Stopping Segment###      \n","        if val_acc_avg/vl_step >= vacc_mx or val_loss_avg/vl_step <= vlss_mn:\n","            if val_acc_avg/vl_step >= vacc_mx and val_loss_avg/vl_step <= vlss_mn:\n","                    vacc_early_model = val_acc_avg/vl_step\n","                    vlss_early_model = val_loss_avg/vl_step            \n","                    working_weights = model.get_weights()\n","            vacc_mx = np.max((val_acc_avg/vl_step, vacc_mx))\n","            vlss_mn = np.min((val_loss_avg/vl_step, vlss_mn))\n","            curr_step = 0\n","        else:\n","            curr_step += 1\n","            if curr_step == args.patience:\n","                    print('Early stop! Min loss: ', vlss_mn, ', Max accuracy: ', vacc_mx)\n","                    print('Early stop model validation loss: ', vlss_early_model, ', accuracy: ', vacc_early_model)\n","                    model.set_weights(working_weights)\n","                    break\n","\n","        train_loss_avg = 0\n","        train_acc_avg = 0\n","        val_loss_avg = 0\n","        val_acc_avg = 0\n","\n","    train_loss_list_all.append(train_loss_list)\n","    train_acc_list_all.append(train_acc_list)\n","    val_loss_list_all.append(val_loss_list)\n","    val_acc_list_all.append(val_acc_list)\n","\n","\n","    ###Testing Segment### Outside of the epochs\n","\n","    ts_step = 0\n","    ts_size = features.shape[0]\n","    ts_loss = 0.0\n","    ts_acc = 0.0\n","    while ts_step * args.batch_size < ts_size:   \n","        bbias = biases #[ts_step*args.batch_size:(ts_step+1)*args.batch_size]   \n","        _, acc_ts,loss_value_ts = evaluate( model, inputs= features[ts_step*args.batch_size:(ts_step+1)*args.batch_size],  bias_mat= bbias, lbl_in = y_test[ts_step*args.batch_size:(ts_step+1)*args.batch_size], msk_in = test_mask[ts_step*args.batch_size:(ts_step+1)*args.batch_size], training=False, args = args)   \n","        ts_loss += loss_value_ts\n","        ts_acc += acc_ts\n","        ts_step += 1\n","\n","    print('Test loss:', ts_loss/ts_step, '; Test accuracy:', ts_acc/ts_step)\n","    ts_loss_list.append((ts_loss/ts_step).numpy())\n","    ts_acc_list.append((ts_acc/ts_step).numpy())\n","\n","\n","    plt.plot(train_loss_list, label='train loss')\n","    plt.plot(val_loss_list, label='val loss')\n","    plt.legend()\n","    plt.xlabel('epoch')\n","    plt.ylabel(\"loss\")\n","    plt.title(\"Train Validation Loss\")\n","    plt.show()\n","\n","\n","    plt.plot(train_acc_list, label='train acc')\n","    plt.plot(val_acc_list, label='val acc')\n","    plt.legend()\n","    plt.xlabel('epoch')\n","    plt.ylabel(\"acc\")\n","    plt.title(\"Train Validation Accuracy\")\n","    plt.show()\n","\n","\n","# the final test accuracy over N repeats are printed here\n","print(ts_acc_list)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pPjWoIKsX7r5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jzTEGq3kX7i-"},"source":[""],"execution_count":null,"outputs":[]}]}