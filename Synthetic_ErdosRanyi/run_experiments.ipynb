{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run_experiments.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"cb6a562eac22488f9420502fe1219eb3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_edfc6af42f5349fb93e86c82a5708b53","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c6fa36c9920e495085c2f15f12089794","IPY_MODEL_a504f28572dd4db09b8afd9ca9357375"]}},"edfc6af42f5349fb93e86c82a5708b53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c6fa36c9920e495085c2f15f12089794":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9d7998c69ba1406ba288d5512b8f2a13","_dom_classes":[],"description":"  2%","_model_name":"FloatProgressModel","bar_style":"","max":200,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":3,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f9d1edfbfeaf42f9bd397f609022c91c"}},"a504f28572dd4db09b8afd9ca9357375":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5e5e393dab4a4d178d9412a0914a70ac","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 3/200 [9:15:25&lt;606:06:31, 11076.10s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8a52bb9c34a7434fac84cbd8a45dcde8"}},"9d7998c69ba1406ba288d5512b8f2a13":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f9d1edfbfeaf42f9bd397f609022c91c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5e5e393dab4a4d178d9412a0914a70ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8a52bb9c34a7434fac84cbd8a45dcde8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KstwZ4h7Pmoo","executionInfo":{"status":"ok","timestamp":1621807538732,"user_tz":-480,"elapsed":12483,"user":{"displayName":"Han Lin","photoUrl":"","userId":"10980112682964758327"}},"outputId":"05ce2af4-9649-45d1-d171-f3d4ba30da0d"},"source":["# link colab to google drive directory where this project data is placed\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","import numpy as np\n","import tensorflow as tf\n","print(tf.__version__)\n","\n","\n","# set project path\n","projectpath = \"/content/gdrive/My Drive/GraphAttnProject/ErdosRanyiSubmission/\"\n","\n","print(projectpath)\n","#print(datareadpath)\n","\n","\n","!pip install dgl\n","\n","\n","import os\n","os.chdir(projectpath)\n","os.getcwd()\n","\n","from CodeZip_ER import *\n","\n","from tqdm.notebook import tqdm, trange\n","import networkx as nx\n","import pickle\n","import torch\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","2.4.1\n","/content/gdrive/My Drive/GraphAttnProject/ErdosRanyiSubmission/\n","Requirement already satisfied: dgl in /usr/local/lib/python3.7/dist-packages (0.6.1)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.5.1)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.19.5)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n","Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.1->dgl) (4.4.2)\n"],"name":"stdout"},{"output_type":"stream","text":["DGL backend not selected or invalid.  Assuming PyTorch for now.\n"],"name":"stderr"},{"output_type":"stream","text":["Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"],"name":"stdout"},{"output_type":"stream","text":["Using backend: pytorch\n"],"name":"stderr"},{"output_type":"stream","text":["2.4.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eazKo0wjQvtQ"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"gjVy5soDlHi_"},"source":["name = 'Caveman'\n","walk_len = 4 # set walk length for GKAT "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rZ7DiJL-j-H2"},"source":["\n","num_classes = 2\n","num_features = 32\n","num_heads = 2\n","feature_drop = 0\n","atten_drop = 0\n","runtimes = 15\n","\n","epsilon = 1e-4\n","\n","start_tol = 499\n","tolerance = 80\n","max_epoch = 500\n","batch_size = 128\n","learning_rate = 0.001\n","h_size = 5\n","normalize = None\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_MwQmHAWk9Nn"},"source":["# load all train and validation graphs\n","train_graphs = pickle.load(open(f'graph_data/{name}/train_graphs.pkl', 'rb'))\n","val_graphs = pickle.load(open(f'graph_data/{name}/val_graphs.pkl', 'rb'))\n","\n","# load all labels\n","train_labels = np.load(f'graph_data/{name}/train_labels.npy')\n","val_labels = np.load(f'graph_data/{name}/val_labels.npy')\n","\n","\n","# here we load the pre-calculated GKAT kernel\n","train_GKAT_kernel = pickle.load(open(f'graph_data/{name}/GKAT_dot_kernels_train_len={walk_len}.pkl', 'rb'))\n","val_GKAT_kernel = pickle.load(open(f'graph_data/{name}/GKAT_dot_kernels_val_len={walk_len}.pkl', 'rb'))\n","\n","train_GAT_masking = pickle.load(open(f'graph_data/{name}/GAT_masking_train.pkl', 'rb'))\n","val_GAT_masking = pickle.load(open(f'graph_data/{name}/GAT_masking_val.pkl', 'rb'))\n","\n","train_GKAT_kernel = [torch.from_numpy(g) for g in train_GKAT_kernel]\n","val_GKAT_kernel = [torch.from_numpy(g) for g in val_GKAT_kernel]\n","\n","\n","\n","for bg in train_graphs:\n","  bg.remove_nodes_from(list(nx.isolates(bg)))\n","for bg in val_graphs:\n","  bg.remove_nodes_from(list(nx.isolates(bg)))\n","\n","\n","def generate_knn_degrees(bg, h_size):\n","  bg_h = np.zeros([bg.number_of_nodes(), h_size])\n","  degree_dict = bg.degree\n","\n","  for node in bg.nodes():\n","      nbr_degrees = []\n","      nbrs = bg.neighbors(node)\n","      for nb in nbrs:\n","          nbr_degrees.append( degree_dict[nb] )\n","      nbr_degrees.sort(reverse = True)\n","\n","      if len(nbr_degrees)==0:\n","        nbr_degrees.append(1e-3)\n","\n","      bg_h[node] = (nbr_degrees + h_size*[0])[:h_size] \n","      \n","  return bg_h\n","\n","\n","h_size = 5\n","train_h = [generate_knn_degrees(bg, h_size) for bg in train_graphs]\n","val_h = [generate_knn_degrees(bg, h_size) for bg in val_graphs]\n","\n","train_graphs = [ dgl.from_networkx(g) for g in train_graphs]\n","val_graphs = [ dgl.from_networkx(g) for g in val_graphs]\n","\n","\n","GKAT_masking = [train_GKAT_kernel, val_GKAT_kernel]\n","GAT_masking = [train_GAT_masking, val_GAT_masking]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z0JiTSi1n2MC"},"source":["# GKAT and GAT"]},{"cell_type":"code","metadata":{"id":"LXYUu12ZnnhC"},"source":["\n","class GKATLayer(nn.Module):\n","    def __init__(self, in_dim, out_dim, feat_drop=0., attn_drop=0., alpha=0.2, agg_activation=F.elu):\n","        super(GKATLayer, self).__init__()\n","\n","        self.feat_drop = nn.Dropout(feat_drop)\n","        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n","        #torch.nn.init.xavier_uniform_(self.fc.weight)\n","        #torch.nn.init.zeros_(self.fc.bias)\n","        self.attn_l = nn.Parameter(torch.ones(size=(out_dim, 1)))\n","        self.attn_r = nn.Parameter(torch.ones(size=(out_dim, 1)))\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.activation = nn.LeakyReLU(alpha)\n","        self.softmax = nn.Softmax(dim = 1)\n","        self.agg_activation=agg_activation\n","\n","    def forward(self, feat, bg, counting_attn):\n","        self.g = bg\n","        h = self.feat_drop(feat)\n","        head_ft = self.fc(h).reshape((h.shape[0], -1))\n","        \n","        a1 = torch.mm(head_ft, self.attn_l)    # V x 1\n","        a2 = torch.mm(head_ft, self.attn_r)     # V x 1\n","        a = self.attn_drop(a1 + a2.transpose(0, 1))\n","        a = self.activation(a)\n","\n","        a_ = a #- maxes\n","        a_nomi = torch.mul(torch.exp(a_), counting_attn.float())\n","        a_deno = torch.sum(a_nomi, 1, keepdim=True)\n","        a_nor = a_nomi/(a_deno+1e-9)\n","\n","        ret = torch.mm(a_nor, head_ft)\n","        if self.agg_activation is not None:\n","            ret = self.agg_activation(ret)\n","\n","        return ret\n","\n","\n","\n","class GKATLayer(nn.Module):\n","    def __init__(self, in_dim, out_dim, feat_drop=0., attn_drop=0., alpha=0.2, agg_activation=F.elu):\n","        super(GKATLayer, self).__init__()\n","\n","        self.feat_drop = feat_drop  #nn.Dropout(feat_drop, training=self.training)\n","        self.attn_drop = attn_drop  #nn.Dropout(attn_drop)\n","        \n","        self.fc_Q = nn.Linear(in_dim, out_dim, bias=False)\n","        self.fc_K = nn.Linear(in_dim, out_dim, bias=False)\n","        self.fc_V = nn.Linear(in_dim, out_dim, bias=False)\n","        \n","        self.softmax = nn.Softmax(dim = 1)\n","\n","        self.agg_activation=agg_activation\n","\n","            \n","    def forward(self, feat, bg, counting_attn):\n","        h = F.dropout(feat, p=self.feat_drop, training=self.training)\n","\n","        Q = self.fc_Q(h).reshape((h.shape[0], -1))\n","        K = self.fc_K(h).reshape((h.shape[0], -1))\n","        V = self.fc_V(h).reshape((h.shape[0], -1))\n","        \n","        logits = F.dropout( torch.matmul( Q, torch.transpose(K,0,1) ) , p=self.attn_drop, training=self.training) / np.sqrt(Q.shape[1])\n","\n","        maxes = torch.max(logits, 1, keepdim=True)[0]\n","        logits =  logits - maxes\n","        \n","        a_nomi = torch.mul(torch.exp( logits  ), counting_attn.float())\n","        a_deno = torch.sum(a_nomi, 1, keepdim=True)\n","        a_nor = a_nomi/(a_deno+1e-9)\n","\n","        ret = torch.mm(a_nor, V)\n","        if self.agg_activation is not None:\n","            ret = self.agg_activation(ret)\n","\n","        return ret\n","\n","\n","\n","class GKATClassifier_ER(nn.Module):\n","    def __init__(self, in_dim, hidden_dim, num_heads, n_classes, feat_drop_=0., attn_drop_=0.,):\n","        super(GKATClassifier_ER, self).__init__()\n","\n","        self.num_heads = num_heads\n","        self.hidden_dim = hidden_dim\n","        self.layers = nn.ModuleList([\n","            nn.ModuleList([GKATLayer(in_dim, hidden_dim, feat_drop = feat_drop_, attn_drop = attn_drop_, agg_activation=F.elu) for _ in range(num_heads)]),\n","            nn.ModuleList([GKATLayer(hidden_dim * num_heads, hidden_dim, feat_drop = feat_drop_, attn_drop = attn_drop_, agg_activation=F.elu) for _ in range(num_heads)]), ])\n","        self.classify = nn.Linear(hidden_dim * num_heads, n_classes)\n","        self.softmax = nn.Softmax(dim = 1)\n","\n","    def forward(self, bg, bg_h, counting_attn, normalize = 'normal'):\n","        h = torch.tensor(bg_h).float()\n","        num_nodes = h.shape[0]\n","        \n","        if normalize == 'normal':\n","            features = h.numpy() #.flatten()\n","            mean_ = np.mean(features, -1).reshape(-1,1)\n","            std_ = np.std(features, -1).reshape(-1,1)\n","            h = (h - mean_)/std_\n","\n","        for i, gnn in enumerate(self.layers):\n","            all_h = []\n","            for j, att_head in enumerate(gnn):\n","                all_h.append(att_head(h, bg, counting_attn))   \n","            h = torch.squeeze(torch.cat(all_h, dim=1))\n","\n","        bg.ndata['h'] = h\n","        hg = dgl.mean_nodes(bg, 'h')\n","\n","        return self.classify(hg)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ah2MpCqFp0if"},"source":["# GCN"]},{"cell_type":"code","metadata":{"id":"onOrAwn7pwZW"},"source":["\n","class GCNClassifier_ER(nn.Module):\n","    def __init__(self, in_dim, hidden_dim, n_classes):\n","        super(GCNClassifier_ER, self).__init__()\n","        self.conv1 = GraphConv(in_dim, hidden_dim)\n","        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n","        self.classify = nn.Linear(hidden_dim, n_classes)\n","\n","    def forward(self, g, bg_h, normalize = 'normal'):\n","       \n","        h = torch.tensor(bg_h).float()       \n","        num_nodes = h.shape[0]\n","        \n","        if normalize == 'normal':\n","            features = h.numpy()\n","            mean_ = np.mean(features, -1).reshape(-1,1)\n","            std_ = np.std(features, -1).reshape(-1,1)\n","            h = (h - mean_)/std_\n","\n","        # Perform graph convolution and activation function.\n","        h = F.relu(self.conv1(g, h))\n","        h = F.relu(self.conv2(g, h))\n","        g.ndata['h'] = h\n","        hg = dgl.mean_nodes(g, 'h')\n","        return self.classify(hg)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0s84JcKXp872"},"source":["# SGC"]},{"cell_type":"code","metadata":{"id":"59qM3sg1JttC"},"source":["def cal_Laplacian(graph):\n","\n","    N = nx.adjacency_matrix(graph).shape[0]\n","    D = np.sum(nx.adjacency_matrix(graph), 1)\n","    D_hat = np.diag((np.array(D).flatten()+1e-5)**(-0.5))\n","    return np.identity(N) - np.dot(D_hat, nx.to_numpy_matrix(graph)).dot(D_hat)  \n","\n","def rescale_L(L, lmax=2):\n","    \"\"\"Rescale Laplacian eigenvalues to [-1,1]\"\"\"\n","    M, M = L.shape\n","    I = torch.diag(torch.ones(M))\n","    L /= lmax * 2\n","    L = torch.tensor(L)\n","    L -= I\n","    return L \n","\n","def lmax_L(L):\n","    \"\"\"Compute largest Laplacian eigenvalue\"\"\"\n","    return scipy.sparse.linalg.eigsh(L, k=1, which='LM', return_eigenvectors=False)[0]\n","\n","\n","\n","\n","\n","train_L_original = [cal_Laplacian(bg) for bg in train_graphs]\n","val_L_original = [cal_Laplacian(bg) for bg in val_graphs]\n","\n","train_L_max = [lmax_L(L) for L in train_L_original]\n","val_L_max = [lmax_L(L) for L in val_L_original]\n","\n","train_L = []\n","for iter, L in tqdm(enumerate(train_L_original)):\n","  train_L.append(rescale_L(L, train_L_max[iter]))\n","\n","val_L = []\n","for iter, L in tqdm(enumerate(val_L_original)):\n","  val_L.append(rescale_L(L, val_L_max[iter]))\n","\n","\n","class Graph_ConvNet_LeNet5(nn.Module):\n","    \n","    def __init__(self, net_parameters):\n","        \n","        print('Graph ConvNet: LeNet5')\n","        \n","        super(Graph_ConvNet_LeNet5, self).__init__()\n","        \n","        # parameters\n","        h_size, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F = net_parameters\n","        \n","        # graph CL1\n","        self.cl1 = nn.Linear(h_size*CL1_K, CL1_F) \n","        Fin = CL1_K; Fout = CL1_F;\n","        scale = np.sqrt( 2.0/ (Fin+Fout) )\n","        self.cl1.weight.data.uniform_(-scale, scale)\n","        self.cl1.bias.data.fill_(0.0)\n","        self.CL1_K = CL1_K; self.CL1_F = CL1_F; \n","        \n","        # graph CL2\n","        self.cl2 = nn.Linear(CL2_K*CL1_F, CL2_F) \n","        Fin = CL2_K*CL1_F; Fout = CL2_F;\n","        scale = np.sqrt( 2.0/ (Fin+Fout) )\n","        self.cl2.weight.data.uniform_(-scale, scale)\n","        self.cl2.bias.data.fill_(0.0)\n","        self.CL2_K = CL2_K; self.CL2_F = CL2_F;\n","        \n","        # FC1\n","        self.fc1 = nn.Linear(CL2_F, FC1_F) \n","        Fin = CL2_F; Fout = FC1_F;\n","        scale = np.sqrt( 2.0/ (Fin+Fout) )\n","        self.fc1.weight.data.uniform_(-scale, scale)\n","        self.fc1.bias.data.fill_(0.0)\n","\n","        # nb of parameters\n","        nb_param = h_size* CL1_K* CL1_F + CL1_F          # CL1\n","        nb_param += CL2_K* CL1_F* CL2_F + CL2_F  # CL2\n","        nb_param += CL2_F* FC1_F + FC1_F        # FC1\n","        print('nb of parameters=',nb_param,'\\n')\n","        \n","        \n","    def init_weights(self, W, Fin, Fout):\n","\n","        scale = np.sqrt( 2.0/ (Fin+Fout) )\n","        W.uniform_(-scale, scale)\n","\n","        return W\n","        \n","    def graph_conv_cheby(self, x, cl, L, Fout, K):\n","\n","        # parameters\n","        # B = batch size\n","        # V = nb vertices\n","        # Fin = nb input features\n","        # Fout = nb output features\n","        # K = Chebyshev order & support size\n","        B, V, Fin = x.size(); B, V, Fin = int(B), int(V), int(Fin) \n","\n","        # rescale Laplacian\n","        \n","        # transform to Chebyshev basis\n","        x0 = x.permute(1,2,0).contiguous().cuda()  # V x Fin x B\n","        x0 = x0.view([V, Fin*B])            # V x Fin*B\n","        x = x0.unsqueeze(0)                 # 1 x V x Fin*B\n","        \n","        def concat(x, x_):\n","            x_ = x_.unsqueeze(0)            # 1 x V x Fin*B\n","            return torch.cat((x, x_), 0)    # K x V x Fin*B  \n","   \n","        x1 = torch.mm(L.double().cuda(),x0.double())              # V x Fin*B\n","        x = torch.cat((x, x1.unsqueeze(0)),0)  # 2 x V x Fin*B\n","\n","        for k in range(2, K):\n","            x2 = 2 * torch.mm(L.cuda(),x1) - x0  \n","            x = torch.cat((x, x2.unsqueeze(0)),0)  # M x Fin*B\n","            x0, x1 = x1, x2  \n","        \n","        x = x.view([K, V, Fin, B])           # K x V x Fin x B     \n","        x = x.permute(3,1,2,0).contiguous()  # B x V x Fin x K       \n","        x = x.view([B*V, Fin*K])             # B*V x Fin*K\n","        \n","        # Compose linearly Fin features to get Fout features\n","        #print(x.shape)\n","        x = cl(x.float())                            # B*V x Fout  \n","        x = x.view([B, V, Fout])             # B x V x Fout\n","        #print(x.shape)\n","        \n","        return x\n","        \n","    def forward(self, x, L):\n","        \n","        # graph CL1\n","        x = torch.tensor(x).unsqueeze(0) # B x V x Fin=1  \n","        x = self.graph_conv_cheby(x, self.cl1, L, self.CL1_F, self.CL1_K)\n","        x = F.relu(x)\n","\n","        # graph CL2\n","        x = self.graph_conv_cheby(x, self.cl2, L, self.CL2_F, self.CL2_K)\n","        x = F.relu(x)\n","        \n","        # FC1\n","        x = self.fc1(x)\n","        x = torch.mean(x, axis = 1)\n","            \n","        return x  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"np-PjTd1_SS7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WFr4rPFqqU_g"},"source":["# Start Training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["cb6a562eac22488f9420502fe1219eb3","edfc6af42f5349fb93e86c82a5708b53","c6fa36c9920e495085c2f15f12089794","a504f28572dd4db09b8afd9ca9357375","9d7998c69ba1406ba288d5512b8f2a13","f9d1edfbfeaf42f9bd397f609022c91c","5e5e393dab4a4d178d9412a0914a70ac","8a52bb9c34a7434fac84cbd8a45dcde8"]},"id":"9AsqLRdLj-My","outputId":"aa9bb4ba-2f9d-47d1-d741-66dd28bfd465"},"source":["\n","\n","all_GGG_train_losses = []\n","all_GGG_train_acc = []\n","all_GGG_val_losses = []\n","all_GGG_val_acc = []\n","GGG_test_acc_end = []\n","GGG_test_acc_ckpt = []\n","\n","\n","\n","\n","from prettytable import PrettyTable\n","\n","def count_parameters(model):\n","    table = PrettyTable([\"Modules\", \"Parameters\"])\n","    total_params = 0\n","    for name, parameter in model.named_parameters():\n","        if not parameter.requires_grad: continue\n","        param = parameter.numel()\n","        table.add_row([name, param])\n","        total_params+=param\n","    print(table)\n","    print(f\"Total Trainable Params: {total_params}\")\n","    return total_params\n","    \n","\n","\n","\n","\n","for runtime in trange(runtimes):\n","\n","    for method in ['GAT', 'GKAT', 'GCN', 'ChebyGNN']:\n","\n","      ckpt_file = f'results/{name}/ckpt/{method}__ckpt.pt'\n","\n","      \n","      \n","      if method == 'GKAT':\n","          num_features = 9\n","          train_GGG_masking, val_GGG_masking = GKAT_masking\n","          model = GKATClassifier_ER(h_size, num_features, num_heads, num_classes, feat_drop_ = feature_drop, attn_drop_ = atten_drop)\n","      if method == 'GAT':\n","          num_features = 9\n","          train_GGG_masking, val_GGG_masking = GAT_masking\n","          model = GKATClassifier_ER(h_size, num_features, num_heads, num_classes, feat_drop_ = feature_drop, attn_drop_ = atten_drop)\n","      if method == 'GCN':\n","          num_features = 32\n","          model = GCNClassifier_ER(h_size, num_features, num_classes)\n","      if method == 'ChebyGNN':\n","          CL1_F = 32\n","          CL1_K = 2\n","          CL2_F = 32\n","          CL2_K = 2\n","          FC1_F = 2\n","          net_parameters = [h_size, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F]\n","          # instantiate the object net of the class \n","          model = Graph_ConvNet_LeNet5(net_parameters)\n","\n","\n","\n","      for p in model.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform(p)\n","\n","      count_parameters(model)\n","\n","      #model.apply(init_weights)\n","      loss_func = nn.CrossEntropyLoss()\n","      optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","      model.train()\n","\n","      epoch_train_losses_GGG = []\n","      epoch_train_acc_GGG = []\n","      epoch_val_losses_GGG = []\n","      epoch_val_acc_GGG = []\n","\n","      num_batches = int(len(train_graphs)/batch_size)\n","\n","      epoch = 0\n","      nan_found = 0\n","      tol = 0 \n","\n","      while True:\n","          if nan_found:\n","            break\n","          \n","          epoch_loss = 0\n","          epoch_acc = 0\n","\n","          ''' Training '''\n","          for iter in range(num_batches):\n","          #for iter in range(2): \n","              predictions = []\n","              labels = torch.empty(batch_size)\n","              rand_indices = np.random.choice(len(train_graphs), batch_size, replace=False)\n","\n","              for b in range(batch_size): \n","\n","                  if method == 'GCN':\n","                      predictions.append(model(train_graphs[rand_indices[b]], train_h[rand_indices[b]][:,:h_size], normalize = normalize ))\n","                  elif method == 'GAT':\n","                      predictions.append(model(train_graphs[rand_indices[b]], train_h[rand_indices[b]][:,:h_size], train_GGG_masking[rand_indices[b]], normalize = normalize ))\n","                  elif method == 'GKAT':\n","                      predictions.append(model(train_graphs[rand_indices[b]], train_h[rand_indices[b]][:,:h_size], train_GGG_masking[rand_indices[b]], normalize = normalize ))\n","                  elif method == 'ChebyGNN':\n","                      predictions.append(model(train_h[rand_indices[b]], train_L[rand_indices[b]]))\n","                \n","                                  \n","\n","                  if torch.isnan(predictions[b][0])[0]:\n","                    print('NaN found.')\n","                    break\n","                  \n","                  labels[b] = train_labels[rand_indices[b]]\n","              \n","              acc = 0\n","              for k in range(len(predictions)):\n","                if predictions[k][0][0]>predictions[k][0][1] and labels[k]==0:\n","                  acc += 1\n","                elif predictions[k][0][0]<=predictions[k][0][1] and labels[k]==1:\n","                  acc += 1\n","              acc /= len(predictions)  \n","              epoch_acc += acc  \n","              \n","              predictions = torch.squeeze(torch.stack(predictions))\n","              if torch.any(torch.isnan(predictions)):\n","                    print('NaN found.')\n","                    nan_found = 1\n","                    break\n","              \n","              loss = loss_func(predictions, labels.long())\n","              optimizer.zero_grad()\n","              loss.backward()\n","              optimizer.step()\n","              epoch_loss += loss.detach().item()\n","\n","          epoch_acc /= (iter + 1)\n","          epoch_loss /= (iter + 1)\n","\n","          val_acc = 0\n","          val_loss = 0\n","          predictions_val = []\n","\n","          for b in range(len(val_graphs)): \n","\n","              if method == 'GCN':\n","                  predictions_val.append(model(val_graphs[b], val_h[b][:,:h_size], normalize = normalize ))\n","              elif method == 'GAT':\n","                  predictions_val.append(model(val_graphs[b], val_h[b][:,:h_size], val_GGG_masking[b], normalize = normalize ))\n","              elif method == 'GKAT':\n","                  predictions_val.append(model(val_graphs[b], val_h[b][:,:h_size], val_GGG_masking[b], normalize = normalize ))\n","              elif method == 'ChebyGNN':\n","                  predictions_val.append(model(val_h[b], val_L[b]))\n","                                        \n","          \n","          for k in range(len(predictions_val)):\n","            if predictions_val[k][0][0]>predictions_val[k][0][1] and val_labels[k]==0:\n","              val_acc += 1\n","            elif predictions_val[k][0][0]<=predictions_val[k][0][1] and val_labels[k]==1:\n","              val_acc += 1\n","              \n","          val_acc /= len(val_graphs)\n","\n","          predictions_val = torch.squeeze(torch.stack(predictions_val))    \n","          loss = loss_func(predictions_val, torch.tensor(val_labels).long())\n","          val_loss += loss.detach().item()\n","\n","          \n","\n","          if len(epoch_val_losses_GGG) ==0:\n","            try:\n","              os.remove(f'{projectpath}{ckpt_file}')\n","            except:\n","              pass\n","            torch.save(model, f'{projectpath}{ckpt_file}')\n","            print('Epoch {}, acc{:.2f}, loss {:.4f}, tol {}, val_acc{:.2f}, val_loss{:.4f} -- checkpoint saved'.format(epoch, epoch_acc, epoch_loss, tol, val_acc, val_loss))\n","          elif (np.min(epoch_val_losses_GGG) >= val_loss) and (np.max(epoch_val_acc_GGG) <= val_acc): \n","            torch.save(model, f'{projectpath}{ckpt_file}')\n","            print('Epoch {}, acc{:.2f}, loss {:.4f}, tol {}, val_acc{:.2f}, val_loss{:.4f} -- checkpoint saved'.format(epoch, epoch_acc, epoch_loss, tol, val_acc, val_loss))\n","          else:\n","            print('Epoch {}, acc{:.2f}, loss {:.4f}, tol {}, val_acc{:.2f}, val_loss{:.4f}'.format(epoch, epoch_acc, epoch_loss, tol, val_acc, val_loss))\n","\n","\n","          if epoch > start_tol:\n","            if np.min(epoch_val_losses_GGG) <= val_loss: \n","              tol += 1\n","              if tol == tolerance: \n","                  print('Loss do not decrease')\n","                  break\n","            else:\n","              if np.abs(epoch_val_losses_GGG[-1] - val_loss)<epsilon:\n","                  print('Converge steadily')\n","                  break\n","              tol = 0\n","\n","              \n","          if epoch > max_epoch:\n","              print(\"Reach Max Epoch Number\")\n","              break            \n","\n","          epoch += 1\n","          epoch_train_acc_GGG.append(epoch_acc)\n","          epoch_train_losses_GGG.append(epoch_loss)\n","          epoch_val_acc_GGG.append(val_acc)\n","          epoch_val_losses_GGG.append(val_loss)\n","\n","      all_GGG_train_acc.append(epoch_train_acc_GGG)\n","      all_GGG_train_losses.append(epoch_train_losses_GGG)\n","      all_GGG_val_acc.append(epoch_val_acc_GGG)\n","      all_GGG_val_losses.append(epoch_val_losses_GGG)\n","\n","\n","\n","      np.save(f'{projectpath}results/{name}/epoch_train_acc_{method}_run{runtime}.npy', epoch_train_acc_GGG)\n","      np.save(f'{projectpath}results/{name}/epoch_val_acc_{method}_run{runtime}.npy', epoch_val_acc_GGG)\n","      np.save(f'{projectpath}results/{name}/epoch_train_losses_{method}_run{runtime}.npy', epoch_train_losses_GGG)\n","      np.save(f'{projectpath}results/{name}/epoch_val_losses_{method}_run{runtime}.npy', epoch_val_losses_GGG)\n","\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cb6a562eac22488f9420502fe1219eb3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=200.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n","Epoch 482, acc0.93, loss 0.1903, tol 0, val_acc0.93, val_loss0.2273\n","Epoch 483, acc0.94, loss 0.1862, tol 0, val_acc0.91, val_loss0.2272\n","Epoch 484, acc0.94, loss 0.1795, tol 0, val_acc0.92, val_loss0.2301\n","Epoch 485, acc0.95, loss 0.1490, tol 0, val_acc0.91, val_loss0.2359\n","Epoch 486, acc0.94, loss 0.1678, tol 0, val_acc0.91, val_loss0.2274\n","Epoch 487, acc0.94, loss 0.1800, tol 0, val_acc0.91, val_loss0.2379\n","Epoch 488, acc0.94, loss 0.1898, tol 0, val_acc0.91, val_loss0.2471\n","Epoch 489, acc0.94, loss 0.1769, tol 0, val_acc0.92, val_loss0.2331\n","Epoch 490, acc0.94, loss 0.1745, tol 0, val_acc0.91, val_loss0.2337\n","Epoch 491, acc0.93, loss 0.1842, tol 0, val_acc0.91, val_loss0.2326\n","Epoch 492, acc0.94, loss 0.1941, tol 0, val_acc0.92, val_loss0.2473\n","Epoch 493, acc0.92, loss 0.2014, tol 0, val_acc0.92, val_loss0.2533\n","Epoch 494, acc0.93, loss 0.1839, tol 0, val_acc0.91, val_loss0.2483\n","Epoch 495, acc0.93, loss 0.2016, tol 0, val_acc0.91, val_loss0.2275\n","Epoch 496, acc0.94, loss 0.1820, tol 0, val_acc0.90, val_loss0.2353\n","Epoch 497, acc0.94, loss 0.1769, tol 0, val_acc0.91, val_loss0.2298\n","Epoch 498, acc0.94, loss 0.1815, tol 0, val_acc0.91, val_loss0.2332\n","Epoch 499, acc0.94, loss 0.1707, tol 0, val_acc0.90, val_loss0.2369\n","Epoch 500, acc0.94, loss 0.1608, tol 0, val_acc0.92, val_loss0.2275\n","Epoch 501, acc0.93, loss 0.1926, tol 1, val_acc0.91, val_loss0.2329\n","Reach Max Epoch Number\n","+------------------------+------------+\n","|        Modules         | Parameters |\n","+------------------------+------------+\n","| layers.0.0.fc_Q.weight |     45     |\n","| layers.0.0.fc_K.weight |     45     |\n","| layers.0.0.fc_V.weight |     45     |\n","| layers.0.1.fc_Q.weight |     45     |\n","| layers.0.1.fc_K.weight |     45     |\n","| layers.0.1.fc_V.weight |     45     |\n","| layers.1.0.fc_Q.weight |    162     |\n","| layers.1.0.fc_K.weight |    162     |\n","| layers.1.0.fc_V.weight |    162     |\n","| layers.1.1.fc_Q.weight |    162     |\n","| layers.1.1.fc_K.weight |    162     |\n","| layers.1.1.fc_V.weight |    162     |\n","|    classify.weight     |     36     |\n","|     classify.bias      |     2      |\n","+------------------------+------------+\n","Total Trainable Params: 1280\n","Epoch 0, acc0.50, loss 0.7449, tol 0, val_acc0.50, val_loss0.7188 -- checkpoint saved\n","Epoch 1, acc0.50, loss 0.7072, tol 0, val_acc0.49, val_loss0.6997\n","Epoch 2, acc0.50, loss 0.7025, tol 0, val_acc0.48, val_loss0.7010\n","Epoch 3, acc0.47, loss 0.7011, tol 0, val_acc0.47, val_loss0.6977\n","Epoch 4, acc0.48, loss 0.6987, tol 0, val_acc0.48, val_loss0.6959\n","Epoch 5, acc0.52, loss 0.6928, tol 0, val_acc0.48, val_loss0.6947\n","Epoch 6, acc0.51, loss 0.6929, tol 0, val_acc0.50, val_loss0.6940 -- checkpoint saved\n","Epoch 7, acc0.53, loss 0.6935, tol 0, val_acc0.53, val_loss0.6923 -- checkpoint saved\n","Epoch 8, acc0.54, loss 0.6920, tol 0, val_acc0.54, val_loss0.6917 -- checkpoint saved\n","Epoch 9, acc0.52, loss 0.6942, tol 0, val_acc0.51, val_loss0.6932\n","Epoch 10, acc0.52, loss 0.6968, tol 0, val_acc0.55, val_loss0.6902 -- checkpoint saved\n","Epoch 11, acc0.52, loss 0.6911, tol 0, val_acc0.54, val_loss0.6903\n","Epoch 12, acc0.54, loss 0.6884, tol 0, val_acc0.51, val_loss0.6930\n","Epoch 13, acc0.54, loss 0.6908, tol 0, val_acc0.54, val_loss0.6888\n","Epoch 14, acc0.53, loss 0.6898, tol 0, val_acc0.56, val_loss0.6881 -- checkpoint saved\n","Epoch 15, acc0.54, loss 0.6906, tol 0, val_acc0.54, val_loss0.6880\n","Epoch 16, acc0.55, loss 0.6876, tol 0, val_acc0.55, val_loss0.6873\n","Epoch 17, acc0.56, loss 0.6870, tol 0, val_acc0.56, val_loss0.6861 -- checkpoint saved\n","Epoch 18, acc0.53, loss 0.6911, tol 0, val_acc0.51, val_loss0.6935\n","Epoch 19, acc0.53, loss 0.6917, tol 0, val_acc0.56, val_loss0.6863\n","Epoch 20, acc0.56, loss 0.6879, tol 0, val_acc0.53, val_loss0.6875\n","Epoch 21, acc0.55, loss 0.6862, tol 0, val_acc0.54, val_loss0.6864\n","Epoch 22, acc0.54, loss 0.6852, tol 0, val_acc0.56, val_loss0.6844 -- checkpoint saved\n","Epoch 23, acc0.55, loss 0.6851, tol 0, val_acc0.52, val_loss0.6914\n","Epoch 24, acc0.56, loss 0.6831, tol 0, val_acc0.56, val_loss0.6847\n","Epoch 25, acc0.53, loss 0.6938, tol 0, val_acc0.54, val_loss0.6839\n","Epoch 26, acc0.54, loss 0.6839, tol 0, val_acc0.54, val_loss0.6854\n","Epoch 27, acc0.56, loss 0.6827, tol 0, val_acc0.54, val_loss0.6845\n","Epoch 28, acc0.54, loss 0.6896, tol 0, val_acc0.56, val_loss0.6820 -- checkpoint saved\n","Epoch 29, acc0.57, loss 0.6809, tol 0, val_acc0.55, val_loss0.6839\n","Epoch 30, acc0.56, loss 0.6854, tol 0, val_acc0.56, val_loss0.6815\n","Epoch 31, acc0.56, loss 0.6840, tol 0, val_acc0.55, val_loss0.6813\n","Epoch 32, acc0.56, loss 0.6825, tol 0, val_acc0.54, val_loss0.6836\n","Epoch 33, acc0.58, loss 0.6803, tol 0, val_acc0.59, val_loss0.6787 -- checkpoint saved\n","Epoch 34, acc0.59, loss 0.6793, tol 0, val_acc0.59, val_loss0.6782\n","Epoch 35, acc0.55, loss 0.6813, tol 0, val_acc0.58, val_loss0.6784\n","Epoch 36, acc0.58, loss 0.6730, tol 0, val_acc0.59, val_loss0.6773\n","Epoch 37, acc0.60, loss 0.6715, tol 0, val_acc0.54, val_loss0.6834\n","Epoch 38, acc0.56, loss 0.6785, tol 0, val_acc0.58, val_loss0.6759\n","Epoch 39, acc0.58, loss 0.6770, tol 0, val_acc0.53, val_loss0.6914\n","Epoch 40, acc0.56, loss 0.6810, tol 0, val_acc0.59, val_loss0.6738\n","Epoch 41, acc0.56, loss 0.6811, tol 0, val_acc0.61, val_loss0.6735 -- checkpoint saved\n","Epoch 42, acc0.56, loss 0.6761, tol 0, val_acc0.59, val_loss0.6776\n","Epoch 43, acc0.57, loss 0.6755, tol 0, val_acc0.58, val_loss0.6727\n","Epoch 44, acc0.59, loss 0.6686, tol 0, val_acc0.59, val_loss0.6722\n","Epoch 45, acc0.60, loss 0.6731, tol 0, val_acc0.58, val_loss0.6729\n","Epoch 46, acc0.59, loss 0.6701, tol 0, val_acc0.58, val_loss0.6737\n","Epoch 47, acc0.59, loss 0.6673, tol 0, val_acc0.62, val_loss0.6682 -- checkpoint saved\n","Epoch 48, acc0.61, loss 0.6665, tol 0, val_acc0.61, val_loss0.6677\n","Epoch 49, acc0.59, loss 0.6695, tol 0, val_acc0.59, val_loss0.6681\n","Epoch 50, acc0.59, loss 0.6614, tol 0, val_acc0.60, val_loss0.6649\n","Epoch 51, acc0.59, loss 0.6688, tol 0, val_acc0.57, val_loss0.6758\n","Epoch 52, acc0.62, loss 0.6594, tol 0, val_acc0.63, val_loss0.6608 -- checkpoint saved\n","Epoch 53, acc0.59, loss 0.6655, tol 0, val_acc0.59, val_loss0.6684\n","Epoch 54, acc0.59, loss 0.6659, tol 0, val_acc0.63, val_loss0.6602 -- checkpoint saved\n","Epoch 55, acc0.61, loss 0.6568, tol 0, val_acc0.63, val_loss0.6566 -- checkpoint saved\n","Epoch 56, acc0.61, loss 0.6595, tol 0, val_acc0.63, val_loss0.6552 -- checkpoint saved\n","Epoch 57, acc0.60, loss 0.6571, tol 0, val_acc0.64, val_loss0.6536 -- checkpoint saved\n","Epoch 58, acc0.63, loss 0.6557, tol 0, val_acc0.65, val_loss0.6522 -- checkpoint saved\n","Epoch 59, acc0.62, loss 0.6638, tol 0, val_acc0.65, val_loss0.6498 -- checkpoint saved\n","Epoch 60, acc0.59, loss 0.6626, tol 0, val_acc0.58, val_loss0.6652\n","Epoch 61, acc0.62, loss 0.6502, tol 0, val_acc0.63, val_loss0.6581\n","Epoch 62, acc0.63, loss 0.6483, tol 0, val_acc0.61, val_loss0.6610\n","Epoch 63, acc0.63, loss 0.6475, tol 0, val_acc0.65, val_loss0.6525\n","Epoch 64, acc0.63, loss 0.6557, tol 0, val_acc0.65, val_loss0.6420 -- checkpoint saved\n","Epoch 65, acc0.63, loss 0.6449, tol 0, val_acc0.61, val_loss0.6444\n","Epoch 66, acc0.64, loss 0.6394, tol 0, val_acc0.64, val_loss0.6383\n","Epoch 67, acc0.67, loss 0.6258, tol 0, val_acc0.67, val_loss0.6334 -- checkpoint saved\n","Epoch 68, acc0.66, loss 0.6302, tol 0, val_acc0.66, val_loss0.6309\n","Epoch 69, acc0.67, loss 0.6234, tol 0, val_acc0.67, val_loss0.6290 -- checkpoint saved\n","Epoch 70, acc0.65, loss 0.6347, tol 0, val_acc0.67, val_loss0.6299\n","Epoch 71, acc0.68, loss 0.6228, tol 0, val_acc0.68, val_loss0.6231 -- checkpoint saved\n","Epoch 72, acc0.67, loss 0.6274, tol 0, val_acc0.68, val_loss0.6209\n","Epoch 73, acc0.68, loss 0.6181, tol 0, val_acc0.68, val_loss0.6292\n","Epoch 74, acc0.69, loss 0.6079, tol 0, val_acc0.67, val_loss0.6165\n","Epoch 75, acc0.70, loss 0.5939, tol 0, val_acc0.69, val_loss0.6145 -- checkpoint saved\n","Epoch 76, acc0.69, loss 0.6072, tol 0, val_acc0.70, val_loss0.6105 -- checkpoint saved\n","Epoch 77, acc0.69, loss 0.6076, tol 0, val_acc0.69, val_loss0.6038\n","Epoch 78, acc0.70, loss 0.5905, tol 0, val_acc0.71, val_loss0.5985 -- checkpoint saved\n","Epoch 79, acc0.72, loss 0.5762, tol 0, val_acc0.71, val_loss0.5935 -- checkpoint saved\n","Epoch 80, acc0.72, loss 0.5696, tol 0, val_acc0.70, val_loss0.5918\n","Epoch 81, acc0.74, loss 0.5585, tol 0, val_acc0.73, val_loss0.5746 -- checkpoint saved\n","Epoch 82, acc0.74, loss 0.5558, tol 0, val_acc0.75, val_loss0.5567 -- checkpoint saved\n","Epoch 83, acc0.75, loss 0.5392, tol 0, val_acc0.75, val_loss0.5418 -- checkpoint saved\n","Epoch 84, acc0.77, loss 0.5183, tol 0, val_acc0.76, val_loss0.5267 -- checkpoint saved\n","Epoch 85, acc0.77, loss 0.5028, tol 0, val_acc0.77, val_loss0.5129 -- checkpoint saved\n","Epoch 86, acc0.78, loss 0.5024, tol 0, val_acc0.78, val_loss0.4933 -- checkpoint saved\n","Epoch 87, acc0.79, loss 0.4845, tol 0, val_acc0.78, val_loss0.4833\n","Epoch 88, acc0.78, loss 0.4782, tol 0, val_acc0.79, val_loss0.4696 -- checkpoint saved\n","Epoch 89, acc0.80, loss 0.4586, tol 0, val_acc0.80, val_loss0.4630 -- checkpoint saved\n","Epoch 90, acc0.83, loss 0.4350, tol 0, val_acc0.76, val_loss0.4866\n","Epoch 91, acc0.81, loss 0.4491, tol 0, val_acc0.77, val_loss0.4710\n","Epoch 92, acc0.82, loss 0.4369, tol 0, val_acc0.81, val_loss0.4452 -- checkpoint saved\n","Epoch 93, acc0.84, loss 0.4129, tol 0, val_acc0.82, val_loss0.4343 -- checkpoint saved\n","Epoch 94, acc0.86, loss 0.3849, tol 0, val_acc0.82, val_loss0.4322 -- checkpoint saved\n","Epoch 95, acc0.83, loss 0.4189, tol 0, val_acc0.82, val_loss0.4246\n","Epoch 96, acc0.83, loss 0.4043, tol 0, val_acc0.82, val_loss0.4233 -- checkpoint saved\n","Epoch 97, acc0.86, loss 0.3662, tol 0, val_acc0.82, val_loss0.4184 -- checkpoint saved\n","Epoch 98, acc0.85, loss 0.4006, tol 0, val_acc0.82, val_loss0.4152 -- checkpoint saved\n","Epoch 99, acc0.85, loss 0.3695, tol 0, val_acc0.83, val_loss0.4105 -- checkpoint saved\n","Epoch 100, acc0.85, loss 0.3611, tol 0, val_acc0.83, val_loss0.4139\n","Epoch 101, acc0.85, loss 0.3557, tol 0, val_acc0.81, val_loss0.4405\n","Epoch 102, acc0.87, loss 0.3594, tol 0, val_acc0.82, val_loss0.4100\n","Epoch 103, acc0.87, loss 0.3515, tol 0, val_acc0.82, val_loss0.4195\n","Epoch 104, acc0.85, loss 0.3503, tol 0, val_acc0.81, val_loss0.4091\n","Epoch 105, acc0.86, loss 0.3688, tol 0, val_acc0.81, val_loss0.4159\n","Epoch 106, acc0.83, loss 0.4059, tol 0, val_acc0.81, val_loss0.4184\n","Epoch 107, acc0.85, loss 0.3914, tol 0, val_acc0.82, val_loss0.4155\n","Epoch 108, acc0.85, loss 0.3601, tol 0, val_acc0.83, val_loss0.4085\n","Epoch 109, acc0.86, loss 0.3715, tol 0, val_acc0.82, val_loss0.3990\n","Epoch 110, acc0.87, loss 0.3478, tol 0, val_acc0.82, val_loss0.4014\n","Epoch 111, acc0.87, loss 0.3389, tol 0, val_acc0.82, val_loss0.4032\n","Epoch 112, acc0.87, loss 0.3457, tol 0, val_acc0.83, val_loss0.4059\n","Epoch 113, acc0.87, loss 0.3380, tol 0, val_acc0.82, val_loss0.3994\n","Epoch 114, acc0.87, loss 0.3310, tol 0, val_acc0.83, val_loss0.4045\n","Epoch 115, acc0.88, loss 0.3397, tol 0, val_acc0.83, val_loss0.4030\n","Epoch 116, acc0.88, loss 0.3296, tol 0, val_acc0.83, val_loss0.4027\n","Epoch 117, acc0.88, loss 0.3232, tol 0, val_acc0.83, val_loss0.4062\n","Epoch 118, acc0.88, loss 0.3251, tol 0, val_acc0.84, val_loss0.4155\n","Epoch 119, acc0.87, loss 0.3333, tol 0, val_acc0.83, val_loss0.4030\n","Epoch 120, acc0.87, loss 0.3375, tol 0, val_acc0.82, val_loss0.3997\n","Epoch 121, acc0.87, loss 0.3479, tol 0, val_acc0.83, val_loss0.4005\n","Epoch 122, acc0.87, loss 0.3443, tol 0, val_acc0.82, val_loss0.4102\n","Epoch 123, acc0.88, loss 0.3138, tol 0, val_acc0.82, val_loss0.4072\n","Epoch 124, acc0.89, loss 0.2991, tol 0, val_acc0.83, val_loss0.4020\n","Epoch 125, acc0.87, loss 0.3348, tol 0, val_acc0.83, val_loss0.3983\n","Epoch 126, acc0.88, loss 0.3242, tol 0, val_acc0.83, val_loss0.3987\n","Epoch 127, acc0.85, loss 0.3602, tol 0, val_acc0.82, val_loss0.4034\n","Epoch 128, acc0.87, loss 0.3258, tol 0, val_acc0.82, val_loss0.4130\n","Epoch 129, acc0.88, loss 0.3194, tol 0, val_acc0.83, val_loss0.3993\n","Epoch 130, acc0.88, loss 0.3069, tol 0, val_acc0.83, val_loss0.3941\n","Epoch 131, acc0.89, loss 0.2956, tol 0, val_acc0.83, val_loss0.4062\n","Epoch 132, acc0.88, loss 0.3297, tol 0, val_acc0.82, val_loss0.4075\n","Epoch 133, acc0.89, loss 0.3052, tol 0, val_acc0.83, val_loss0.3960\n","Epoch 134, acc0.88, loss 0.3205, tol 0, val_acc0.83, val_loss0.3949\n","Epoch 135, acc0.89, loss 0.3098, tol 0, val_acc0.83, val_loss0.4011\n","Epoch 136, acc0.88, loss 0.3156, tol 0, val_acc0.83, val_loss0.3977\n","Epoch 137, acc0.88, loss 0.3042, tol 0, val_acc0.82, val_loss0.4058\n","Epoch 138, acc0.88, loss 0.3187, tol 0, val_acc0.82, val_loss0.4045\n","Epoch 139, acc0.89, loss 0.3131, tol 0, val_acc0.84, val_loss0.4030\n","Epoch 140, acc0.87, loss 0.3368, tol 0, val_acc0.84, val_loss0.3894 -- checkpoint saved\n","Epoch 141, acc0.87, loss 0.3326, tol 0, val_acc0.83, val_loss0.3895\n","Epoch 142, acc0.87, loss 0.3226, tol 0, val_acc0.82, val_loss0.4027\n","Epoch 143, acc0.88, loss 0.3074, tol 0, val_acc0.84, val_loss0.3861\n","Epoch 144, acc0.88, loss 0.3275, tol 0, val_acc0.84, val_loss0.3968\n","Epoch 145, acc0.88, loss 0.3219, tol 0, val_acc0.81, val_loss0.4070\n","Epoch 146, acc0.88, loss 0.3142, tol 0, val_acc0.84, val_loss0.3848\n","Epoch 147, acc0.89, loss 0.3146, tol 0, val_acc0.84, val_loss0.3863\n","Epoch 148, acc0.88, loss 0.3156, tol 0, val_acc0.84, val_loss0.3777 -- checkpoint saved\n","Epoch 149, acc0.89, loss 0.2895, tol 0, val_acc0.84, val_loss0.3801\n","Epoch 150, acc0.88, loss 0.2989, tol 0, val_acc0.83, val_loss0.3913\n","Epoch 151, acc0.88, loss 0.3084, tol 0, val_acc0.84, val_loss0.3809\n","Epoch 152, acc0.88, loss 0.3189, tol 0, val_acc0.84, val_loss0.3856\n","Epoch 153, acc0.89, loss 0.2899, tol 0, val_acc0.83, val_loss0.3876\n","Epoch 154, acc0.89, loss 0.2820, tol 0, val_acc0.82, val_loss0.3975\n","Epoch 155, acc0.89, loss 0.3124, tol 0, val_acc0.85, val_loss0.3744 -- checkpoint saved\n","Epoch 156, acc0.89, loss 0.3090, tol 0, val_acc0.84, val_loss0.3756\n","Epoch 157, acc0.88, loss 0.3163, tol 0, val_acc0.84, val_loss0.3752\n","Epoch 158, acc0.90, loss 0.2773, tol 0, val_acc0.84, val_loss0.3721\n","Epoch 159, acc0.89, loss 0.2936, tol 0, val_acc0.85, val_loss0.3725\n","Epoch 160, acc0.90, loss 0.2913, tol 0, val_acc0.83, val_loss0.3798\n","Epoch 161, acc0.89, loss 0.3011, tol 0, val_acc0.84, val_loss0.3650\n","Epoch 162, acc0.90, loss 0.2737, tol 0, val_acc0.84, val_loss0.3751\n","Epoch 163, acc0.90, loss 0.2868, tol 0, val_acc0.84, val_loss0.3770\n","Epoch 164, acc0.89, loss 0.2860, tol 0, val_acc0.84, val_loss0.3766\n","Epoch 165, acc0.90, loss 0.2991, tol 0, val_acc0.85, val_loss0.3682\n","Epoch 166, acc0.89, loss 0.2832, tol 0, val_acc0.86, val_loss0.3610 -- checkpoint saved\n","Epoch 167, acc0.89, loss 0.3116, tol 0, val_acc0.86, val_loss0.3673\n","Epoch 168, acc0.89, loss 0.3039, tol 0, val_acc0.84, val_loss0.3745\n","Epoch 169, acc0.89, loss 0.2815, tol 0, val_acc0.85, val_loss0.3635\n","Epoch 170, acc0.91, loss 0.2703, tol 0, val_acc0.84, val_loss0.3694\n","Epoch 171, acc0.89, loss 0.3164, tol 0, val_acc0.85, val_loss0.3625\n","Epoch 172, acc0.90, loss 0.2702, tol 0, val_acc0.85, val_loss0.3576\n","Epoch 173, acc0.90, loss 0.2936, tol 0, val_acc0.85, val_loss0.3584\n","Epoch 174, acc0.89, loss 0.2983, tol 0, val_acc0.86, val_loss0.3595\n","Epoch 175, acc0.88, loss 0.3084, tol 0, val_acc0.86, val_loss0.3541 -- checkpoint saved\n","Epoch 176, acc0.90, loss 0.2882, tol 0, val_acc0.86, val_loss0.3553\n","Epoch 177, acc0.89, loss 0.2906, tol 0, val_acc0.85, val_loss0.3549\n","Epoch 178, acc0.89, loss 0.3014, tol 0, val_acc0.85, val_loss0.3512\n","Epoch 179, acc0.90, loss 0.2851, tol 0, val_acc0.86, val_loss0.3502 -- checkpoint saved\n","Epoch 180, acc0.90, loss 0.2771, tol 0, val_acc0.86, val_loss0.3525\n","Epoch 181, acc0.90, loss 0.2819, tol 0, val_acc0.85, val_loss0.3615\n","Epoch 182, acc0.89, loss 0.2991, tol 0, val_acc0.86, val_loss0.3524\n","Epoch 183, acc0.90, loss 0.2762, tol 0, val_acc0.86, val_loss0.3471\n","Epoch 184, acc0.90, loss 0.2793, tol 0, val_acc0.85, val_loss0.3503\n","Epoch 185, acc0.90, loss 0.2863, tol 0, val_acc0.85, val_loss0.3515\n","Epoch 186, acc0.90, loss 0.2899, tol 0, val_acc0.84, val_loss0.3548\n","Epoch 187, acc0.91, loss 0.2722, tol 0, val_acc0.85, val_loss0.3525\n","Epoch 188, acc0.91, loss 0.2603, tol 0, val_acc0.86, val_loss0.3483\n","Epoch 189, acc0.89, loss 0.2895, tol 0, val_acc0.82, val_loss0.4116\n","Epoch 190, acc0.89, loss 0.3012, tol 0, val_acc0.84, val_loss0.3789\n","Epoch 191, acc0.89, loss 0.2792, tol 0, val_acc0.85, val_loss0.3475\n","Epoch 192, acc0.90, loss 0.2622, tol 0, val_acc0.85, val_loss0.3513\n","Epoch 193, acc0.90, loss 0.2676, tol 0, val_acc0.85, val_loss0.3507\n","Epoch 194, acc0.90, loss 0.2699, tol 0, val_acc0.86, val_loss0.3492\n","Epoch 195, acc0.90, loss 0.2737, tol 0, val_acc0.87, val_loss0.3403 -- checkpoint saved\n","Epoch 196, acc0.89, loss 0.2918, tol 0, val_acc0.85, val_loss0.3526\n","Epoch 197, acc0.90, loss 0.2693, tol 0, val_acc0.83, val_loss0.3672\n","Epoch 198, acc0.90, loss 0.2793, tol 0, val_acc0.87, val_loss0.3399 -- checkpoint saved\n","Epoch 199, acc0.88, loss 0.3121, tol 0, val_acc0.86, val_loss0.3433\n","Epoch 200, acc0.90, loss 0.2725, tol 0, val_acc0.85, val_loss0.3482\n","Epoch 201, acc0.90, loss 0.2791, tol 0, val_acc0.85, val_loss0.3474\n","Epoch 202, acc0.90, loss 0.2616, tol 0, val_acc0.84, val_loss0.3492\n","Epoch 203, acc0.89, loss 0.2878, tol 0, val_acc0.86, val_loss0.3283\n","Epoch 204, acc0.89, loss 0.2844, tol 0, val_acc0.87, val_loss0.3409\n","Epoch 205, acc0.89, loss 0.2722, tol 0, val_acc0.85, val_loss0.3437\n","Epoch 206, acc0.90, loss 0.2821, tol 0, val_acc0.86, val_loss0.3441\n","Epoch 207, acc0.91, loss 0.2676, tol 0, val_acc0.85, val_loss0.3432\n","Epoch 208, acc0.90, loss 0.2841, tol 0, val_acc0.87, val_loss0.3330\n","Epoch 209, acc0.91, loss 0.2703, tol 0, val_acc0.86, val_loss0.3402\n","Epoch 210, acc0.90, loss 0.2588, tol 0, val_acc0.86, val_loss0.3407\n","Epoch 211, acc0.92, loss 0.2482, tol 0, val_acc0.86, val_loss0.3448\n","Epoch 212, acc0.91, loss 0.2454, tol 0, val_acc0.85, val_loss0.3519\n","Epoch 213, acc0.89, loss 0.2912, tol 0, val_acc0.86, val_loss0.3362\n","Epoch 214, acc0.89, loss 0.2794, tol 0, val_acc0.86, val_loss0.3265\n","Epoch 215, acc0.90, loss 0.2600, tol 0, val_acc0.86, val_loss0.3367\n","Epoch 216, acc0.90, loss 0.2698, tol 0, val_acc0.85, val_loss0.3381\n","Epoch 217, acc0.91, loss 0.2599, tol 0, val_acc0.87, val_loss0.3318\n","Epoch 218, acc0.89, loss 0.2804, tol 0, val_acc0.87, val_loss0.3246\n","Epoch 219, acc0.89, loss 0.2718, tol 0, val_acc0.86, val_loss0.3341\n","Epoch 220, acc0.91, loss 0.2570, tol 0, val_acc0.87, val_loss0.3288\n","Epoch 221, acc0.91, loss 0.2740, tol 0, val_acc0.86, val_loss0.3346\n","Epoch 222, acc0.89, loss 0.2909, tol 0, val_acc0.85, val_loss0.3299\n","Epoch 223, acc0.89, loss 0.2809, tol 0, val_acc0.85, val_loss0.3266\n","Epoch 224, acc0.89, loss 0.2915, tol 0, val_acc0.87, val_loss0.3243 -- checkpoint saved\n","Epoch 225, acc0.90, loss 0.2681, tol 0, val_acc0.86, val_loss0.3293\n","Epoch 226, acc0.89, loss 0.2868, tol 0, val_acc0.87, val_loss0.3268\n","Epoch 227, acc0.90, loss 0.2699, tol 0, val_acc0.87, val_loss0.3257\n","Epoch 228, acc0.90, loss 0.2680, tol 0, val_acc0.87, val_loss0.3261\n","Epoch 229, acc0.91, loss 0.2499, tol 0, val_acc0.86, val_loss0.3264\n","Epoch 230, acc0.89, loss 0.2799, tol 0, val_acc0.86, val_loss0.3283\n","Epoch 231, acc0.90, loss 0.2651, tol 0, val_acc0.86, val_loss0.3239\n","Epoch 232, acc0.90, loss 0.2853, tol 0, val_acc0.86, val_loss0.3254\n","Epoch 233, acc0.90, loss 0.2641, tol 0, val_acc0.86, val_loss0.3350\n","Epoch 234, acc0.91, loss 0.2408, tol 0, val_acc0.84, val_loss0.3462\n","Epoch 235, acc0.91, loss 0.2594, tol 0, val_acc0.86, val_loss0.3269\n","Epoch 236, acc0.91, loss 0.2472, tol 0, val_acc0.87, val_loss0.3217\n","Epoch 237, acc0.91, loss 0.2597, tol 0, val_acc0.87, val_loss0.3211\n","Epoch 238, acc0.90, loss 0.2544, tol 0, val_acc0.85, val_loss0.3229\n","Epoch 239, acc0.90, loss 0.2625, tol 0, val_acc0.85, val_loss0.3402\n","Epoch 240, acc0.91, loss 0.2645, tol 0, val_acc0.86, val_loss0.3200\n","Epoch 241, acc0.90, loss 0.2539, tol 0, val_acc0.85, val_loss0.3310\n","Epoch 242, acc0.89, loss 0.2706, tol 0, val_acc0.87, val_loss0.3148\n","Epoch 243, acc0.92, loss 0.2343, tol 0, val_acc0.86, val_loss0.3205\n","Epoch 244, acc0.90, loss 0.2767, tol 0, val_acc0.86, val_loss0.3178\n","Epoch 245, acc0.91, loss 0.2604, tol 0, val_acc0.87, val_loss0.3182\n","Epoch 246, acc0.91, loss 0.2394, tol 0, val_acc0.86, val_loss0.3176\n","Epoch 247, acc0.91, loss 0.2400, tol 0, val_acc0.86, val_loss0.3147\n","Epoch 248, acc0.92, loss 0.2390, tol 0, val_acc0.87, val_loss0.3167\n","Epoch 249, acc0.91, loss 0.2553, tol 0, val_acc0.85, val_loss0.3282\n","Epoch 250, acc0.91, loss 0.2450, tol 0, val_acc0.86, val_loss0.3168\n","Epoch 251, acc0.90, loss 0.2417, tol 0, val_acc0.88, val_loss0.3135 -- checkpoint saved\n","Epoch 252, acc0.90, loss 0.2811, tol 0, val_acc0.87, val_loss0.3077\n","Epoch 253, acc0.91, loss 0.2558, tol 0, val_acc0.88, val_loss0.3052 -- checkpoint saved\n","Epoch 254, acc0.91, loss 0.2522, tol 0, val_acc0.87, val_loss0.3142\n","Epoch 255, acc0.90, loss 0.2601, tol 0, val_acc0.87, val_loss0.3065\n","Epoch 256, acc0.90, loss 0.2727, tol 0, val_acc0.87, val_loss0.3075\n","Epoch 257, acc0.90, loss 0.2582, tol 0, val_acc0.86, val_loss0.3045\n","Epoch 258, acc0.90, loss 0.2497, tol 0, val_acc0.86, val_loss0.3096\n","Epoch 259, acc0.91, loss 0.2342, tol 0, val_acc0.86, val_loss0.3167\n","Epoch 260, acc0.92, loss 0.2433, tol 0, val_acc0.85, val_loss0.3288\n","Epoch 261, acc0.92, loss 0.2234, tol 0, val_acc0.86, val_loss0.3108\n","Epoch 262, acc0.92, loss 0.2386, tol 0, val_acc0.87, val_loss0.3118\n","Epoch 263, acc0.91, loss 0.2561, tol 0, val_acc0.87, val_loss0.3041\n","Epoch 264, acc0.89, loss 0.2932, tol 0, val_acc0.88, val_loss0.2933 -- checkpoint saved\n","Epoch 265, acc0.91, loss 0.2482, tol 0, val_acc0.88, val_loss0.2981\n","Epoch 266, acc0.90, loss 0.2445, tol 0, val_acc0.87, val_loss0.3029\n","Epoch 267, acc0.89, loss 0.2660, tol 0, val_acc0.87, val_loss0.3004\n","Epoch 268, acc0.91, loss 0.2241, tol 0, val_acc0.87, val_loss0.3043\n","Epoch 269, acc0.91, loss 0.2680, tol 0, val_acc0.88, val_loss0.3048\n","Epoch 270, acc0.92, loss 0.2475, tol 0, val_acc0.88, val_loss0.2922\n","Epoch 271, acc0.90, loss 0.2642, tol 0, val_acc0.86, val_loss0.3059\n","Epoch 272, acc0.91, loss 0.2353, tol 0, val_acc0.85, val_loss0.3216\n","Epoch 273, acc0.91, loss 0.2522, tol 0, val_acc0.87, val_loss0.2999\n","Epoch 274, acc0.92, loss 0.2309, tol 0, val_acc0.87, val_loss0.2987\n","Epoch 275, acc0.92, loss 0.2361, tol 0, val_acc0.87, val_loss0.3035\n","Epoch 276, acc0.91, loss 0.2357, tol 0, val_acc0.88, val_loss0.3065\n","Epoch 277, acc0.91, loss 0.2510, tol 0, val_acc0.88, val_loss0.2958\n","Epoch 278, acc0.92, loss 0.2460, tol 0, val_acc0.86, val_loss0.3064\n","Epoch 279, acc0.91, loss 0.2645, tol 0, val_acc0.88, val_loss0.2960\n","Epoch 280, acc0.90, loss 0.2471, tol 0, val_acc0.87, val_loss0.2941\n","Epoch 281, acc0.92, loss 0.2332, tol 0, val_acc0.87, val_loss0.3002\n","Epoch 282, acc0.92, loss 0.2314, tol 0, val_acc0.88, val_loss0.3000\n","Epoch 283, acc0.92, loss 0.2323, tol 0, val_acc0.88, val_loss0.2969\n","Epoch 284, acc0.90, loss 0.2614, tol 0, val_acc0.87, val_loss0.2931\n","Epoch 285, acc0.91, loss 0.2449, tol 0, val_acc0.88, val_loss0.2908\n","Epoch 286, acc0.92, loss 0.2372, tol 0, val_acc0.88, val_loss0.2892 -- checkpoint saved\n","Epoch 287, acc0.91, loss 0.2443, tol 0, val_acc0.88, val_loss0.2897\n","Epoch 288, acc0.90, loss 0.2610, tol 0, val_acc0.88, val_loss0.2916\n","Epoch 289, acc0.92, loss 0.2148, tol 0, val_acc0.87, val_loss0.2980\n","Epoch 290, acc0.91, loss 0.2596, tol 0, val_acc0.87, val_loss0.2948\n","Epoch 291, acc0.91, loss 0.2509, tol 0, val_acc0.87, val_loss0.2856\n","Epoch 292, acc0.90, loss 0.2594, tol 0, val_acc0.87, val_loss0.2849\n","Epoch 293, acc0.90, loss 0.2513, tol 0, val_acc0.88, val_loss0.2942\n","Epoch 294, acc0.91, loss 0.2519, tol 0, val_acc0.89, val_loss0.2911\n","Epoch 295, acc0.92, loss 0.2282, tol 0, val_acc0.88, val_loss0.2911\n","Epoch 296, acc0.91, loss 0.2515, tol 0, val_acc0.88, val_loss0.3011\n","Epoch 297, acc0.90, loss 0.2569, tol 0, val_acc0.88, val_loss0.2933\n","Epoch 298, acc0.91, loss 0.2439, tol 0, val_acc0.87, val_loss0.2907\n","Epoch 299, acc0.92, loss 0.2239, tol 0, val_acc0.88, val_loss0.2875\n","Epoch 300, acc0.91, loss 0.2331, tol 0, val_acc0.88, val_loss0.2890\n","Epoch 301, acc0.91, loss 0.2195, tol 0, val_acc0.89, val_loss0.2812 -- checkpoint saved\n","Epoch 302, acc0.91, loss 0.2362, tol 0, val_acc0.86, val_loss0.2845\n","Epoch 303, acc0.91, loss 0.2284, tol 0, val_acc0.88, val_loss0.2757\n","Epoch 304, acc0.92, loss 0.2214, tol 0, val_acc0.88, val_loss0.2853\n","Epoch 305, acc0.91, loss 0.2307, tol 0, val_acc0.88, val_loss0.2837\n","Epoch 306, acc0.91, loss 0.2385, tol 0, val_acc0.87, val_loss0.2855\n","Epoch 307, acc0.90, loss 0.2478, tol 0, val_acc0.87, val_loss0.2843\n","Epoch 308, acc0.91, loss 0.2547, tol 0, val_acc0.88, val_loss0.2821\n","Epoch 309, acc0.91, loss 0.2494, tol 0, val_acc0.87, val_loss0.2843\n","Epoch 310, acc0.91, loss 0.2533, tol 0, val_acc0.86, val_loss0.2915\n","Epoch 311, acc0.91, loss 0.2510, tol 0, val_acc0.87, val_loss0.2791\n","Epoch 312, acc0.91, loss 0.2506, tol 0, val_acc0.89, val_loss0.2740 -- checkpoint saved\n","Epoch 313, acc0.92, loss 0.2374, tol 0, val_acc0.89, val_loss0.2845\n","Epoch 314, acc0.92, loss 0.2274, tol 0, val_acc0.88, val_loss0.2830\n","Epoch 315, acc0.92, loss 0.2381, tol 0, val_acc0.87, val_loss0.2870\n","Epoch 316, acc0.91, loss 0.2408, tol 0, val_acc0.87, val_loss0.2910\n","Epoch 317, acc0.92, loss 0.2279, tol 0, val_acc0.88, val_loss0.2785\n","Epoch 318, acc0.91, loss 0.2442, tol 0, val_acc0.85, val_loss0.3107\n","Epoch 319, acc0.91, loss 0.2338, tol 0, val_acc0.89, val_loss0.2856\n","Epoch 320, acc0.92, loss 0.2382, tol 0, val_acc0.90, val_loss0.2853\n","Epoch 321, acc0.91, loss 0.2353, tol 0, val_acc0.89, val_loss0.2851\n","Epoch 322, acc0.91, loss 0.2568, tol 0, val_acc0.88, val_loss0.2778\n","Epoch 323, acc0.92, loss 0.2414, tol 0, val_acc0.88, val_loss0.2806\n","Epoch 324, acc0.91, loss 0.2446, tol 0, val_acc0.89, val_loss0.2736\n","Epoch 325, acc0.93, loss 0.2063, tol 0, val_acc0.89, val_loss0.2775\n","Epoch 326, acc0.92, loss 0.2352, tol 0, val_acc0.87, val_loss0.2851\n","Epoch 327, acc0.93, loss 0.2174, tol 0, val_acc0.87, val_loss0.3025\n","Epoch 328, acc0.92, loss 0.2398, tol 0, val_acc0.86, val_loss0.3027\n","Epoch 329, acc0.92, loss 0.2260, tol 0, val_acc0.89, val_loss0.2765\n","Epoch 330, acc0.92, loss 0.2278, tol 0, val_acc0.90, val_loss0.2797\n","Epoch 331, acc0.92, loss 0.2227, tol 0, val_acc0.88, val_loss0.2793\n","Epoch 332, acc0.92, loss 0.2218, tol 0, val_acc0.87, val_loss0.2849\n","Epoch 333, acc0.92, loss 0.2233, tol 0, val_acc0.88, val_loss0.2760\n","Epoch 334, acc0.92, loss 0.2275, tol 0, val_acc0.87, val_loss0.2821\n","Epoch 335, acc0.93, loss 0.2083, tol 0, val_acc0.88, val_loss0.2844\n","Epoch 336, acc0.92, loss 0.2134, tol 0, val_acc0.88, val_loss0.2759\n","Epoch 337, acc0.92, loss 0.2450, tol 0, val_acc0.89, val_loss0.2732\n","Epoch 338, acc0.92, loss 0.2286, tol 0, val_acc0.86, val_loss0.2848\n","Epoch 339, acc0.91, loss 0.2611, tol 0, val_acc0.88, val_loss0.2666\n","Epoch 340, acc0.91, loss 0.2464, tol 0, val_acc0.88, val_loss0.2690\n","Epoch 341, acc0.93, loss 0.2217, tol 0, val_acc0.88, val_loss0.2741\n","Epoch 342, acc0.92, loss 0.2345, tol 0, val_acc0.87, val_loss0.2885\n","Epoch 343, acc0.92, loss 0.2355, tol 0, val_acc0.88, val_loss0.2799\n","Epoch 344, acc0.92, loss 0.2399, tol 0, val_acc0.88, val_loss0.2730\n","Epoch 345, acc0.93, loss 0.2220, tol 0, val_acc0.87, val_loss0.2792\n","Epoch 346, acc0.91, loss 0.2319, tol 0, val_acc0.89, val_loss0.2798\n","Epoch 347, acc0.93, loss 0.2105, tol 0, val_acc0.89, val_loss0.2765\n","Epoch 348, acc0.91, loss 0.2481, tol 0, val_acc0.88, val_loss0.2888\n","Epoch 349, acc0.92, loss 0.2255, tol 0, val_acc0.89, val_loss0.2772\n","Epoch 350, acc0.92, loss 0.2364, tol 0, val_acc0.88, val_loss0.2719\n","Epoch 351, acc0.91, loss 0.2403, tol 0, val_acc0.89, val_loss0.2698\n","Epoch 352, acc0.92, loss 0.2335, tol 0, val_acc0.89, val_loss0.2728\n","Epoch 353, acc0.92, loss 0.2334, tol 0, val_acc0.89, val_loss0.2718\n","Epoch 354, acc0.92, loss 0.2350, tol 0, val_acc0.87, val_loss0.2785\n","Epoch 355, acc0.92, loss 0.2410, tol 0, val_acc0.89, val_loss0.2681\n","Epoch 356, acc0.92, loss 0.2434, tol 0, val_acc0.89, val_loss0.2710\n","Epoch 357, acc0.93, loss 0.2101, tol 0, val_acc0.88, val_loss0.2808\n","Epoch 358, acc0.93, loss 0.2050, tol 0, val_acc0.88, val_loss0.2774\n","Epoch 359, acc0.91, loss 0.2455, tol 0, val_acc0.89, val_loss0.2662\n","Epoch 360, acc0.93, loss 0.2125, tol 0, val_acc0.88, val_loss0.2678\n","Epoch 361, acc0.92, loss 0.2230, tol 0, val_acc0.89, val_loss0.2715\n","Epoch 362, acc0.92, loss 0.2266, tol 0, val_acc0.89, val_loss0.2642\n","Epoch 363, acc0.93, loss 0.2059, tol 0, val_acc0.89, val_loss0.2674\n","Epoch 364, acc0.92, loss 0.2200, tol 0, val_acc0.89, val_loss0.2720\n","Epoch 365, acc0.92, loss 0.2278, tol 0, val_acc0.88, val_loss0.2711\n","Epoch 366, acc0.92, loss 0.2246, tol 0, val_acc0.89, val_loss0.2756\n","Epoch 367, acc0.92, loss 0.2492, tol 0, val_acc0.88, val_loss0.2684\n","Epoch 368, acc0.92, loss 0.2281, tol 0, val_acc0.88, val_loss0.2807\n","Epoch 369, acc0.92, loss 0.2430, tol 0, val_acc0.88, val_loss0.2729\n","Epoch 370, acc0.93, loss 0.2008, tol 0, val_acc0.88, val_loss0.2769\n","Epoch 371, acc0.92, loss 0.2190, tol 0, val_acc0.89, val_loss0.2662\n","Epoch 372, acc0.94, loss 0.2123, tol 0, val_acc0.89, val_loss0.2614\n","Epoch 373, acc0.94, loss 0.1765, tol 0, val_acc0.90, val_loss0.2645\n","Epoch 374, acc0.93, loss 0.2167, tol 0, val_acc0.89, val_loss0.2712\n","Epoch 375, acc0.93, loss 0.1992, tol 0, val_acc0.89, val_loss0.2674\n","Epoch 376, acc0.91, loss 0.2387, tol 0, val_acc0.88, val_loss0.3088\n","Epoch 377, acc0.91, loss 0.2435, tol 0, val_acc0.88, val_loss0.2810\n","Epoch 378, acc0.90, loss 0.2676, tol 0, val_acc0.89, val_loss0.2710\n","Epoch 379, acc0.92, loss 0.2280, tol 0, val_acc0.90, val_loss0.2715\n","Epoch 380, acc0.91, loss 0.2547, tol 0, val_acc0.90, val_loss0.2633\n","Epoch 381, acc0.93, loss 0.2149, tol 0, val_acc0.88, val_loss0.2744\n","Epoch 382, acc0.92, loss 0.2297, tol 0, val_acc0.90, val_loss0.2650\n","Epoch 383, acc0.92, loss 0.2346, tol 0, val_acc0.89, val_loss0.2650\n","Epoch 384, acc0.92, loss 0.2261, tol 0, val_acc0.89, val_loss0.2622\n","Epoch 385, acc0.92, loss 0.2334, tol 0, val_acc0.89, val_loss0.2632\n","Epoch 386, acc0.93, loss 0.2165, tol 0, val_acc0.89, val_loss0.2701\n","Epoch 387, acc0.93, loss 0.2047, tol 0, val_acc0.88, val_loss0.2679\n","Epoch 388, acc0.93, loss 0.1978, tol 0, val_acc0.88, val_loss0.2726\n","Epoch 389, acc0.93, loss 0.2124, tol 0, val_acc0.88, val_loss0.2846\n","Epoch 390, acc0.93, loss 0.2160, tol 0, val_acc0.89, val_loss0.2643\n","Epoch 391, acc0.93, loss 0.1944, tol 0, val_acc0.88, val_loss0.2732\n","Epoch 392, acc0.92, loss 0.2158, tol 0, val_acc0.88, val_loss0.2666\n","Epoch 393, acc0.93, loss 0.2154, tol 0, val_acc0.90, val_loss0.2624\n","Epoch 394, acc0.92, loss 0.2220, tol 0, val_acc0.89, val_loss0.2636\n","Epoch 395, acc0.94, loss 0.2043, tol 0, val_acc0.90, val_loss0.2605\n","Epoch 396, acc0.93, loss 0.2142, tol 0, val_acc0.89, val_loss0.2664\n","Epoch 397, acc0.93, loss 0.2204, tol 0, val_acc0.89, val_loss0.2617\n","Epoch 398, acc0.92, loss 0.2221, tol 0, val_acc0.89, val_loss0.2596\n","Epoch 399, acc0.92, loss 0.2216, tol 0, val_acc0.89, val_loss0.2703\n","Epoch 400, acc0.93, loss 0.2195, tol 0, val_acc0.90, val_loss0.2573\n","Epoch 401, acc0.93, loss 0.2399, tol 0, val_acc0.90, val_loss0.2577\n","Epoch 402, acc0.93, loss 0.2119, tol 0, val_acc0.89, val_loss0.2673\n","Epoch 403, acc0.91, loss 0.2426, tol 0, val_acc0.89, val_loss0.2569\n","Epoch 404, acc0.92, loss 0.2292, tol 0, val_acc0.89, val_loss0.2682\n","Epoch 405, acc0.93, loss 0.2123, tol 0, val_acc0.88, val_loss0.2666\n","Epoch 406, acc0.93, loss 0.2046, tol 0, val_acc0.89, val_loss0.2627\n","Epoch 407, acc0.94, loss 0.1879, tol 0, val_acc0.90, val_loss0.2604\n","Epoch 408, acc0.93, loss 0.2215, tol 0, val_acc0.89, val_loss0.2606\n","Epoch 409, acc0.93, loss 0.2038, tol 0, val_acc0.89, val_loss0.2664\n","Epoch 410, acc0.92, loss 0.2159, tol 0, val_acc0.88, val_loss0.2736\n","Epoch 411, acc0.93, loss 0.2148, tol 0, val_acc0.88, val_loss0.2686\n","Epoch 412, acc0.92, loss 0.2320, tol 0, val_acc0.88, val_loss0.2856\n","Epoch 413, acc0.94, loss 0.1942, tol 0, val_acc0.89, val_loss0.2737\n","Epoch 414, acc0.93, loss 0.2134, tol 0, val_acc0.90, val_loss0.2635\n","Epoch 415, acc0.93, loss 0.1972, tol 0, val_acc0.90, val_loss0.2664\n","Epoch 416, acc0.92, loss 0.2345, tol 0, val_acc0.89, val_loss0.2625\n","Epoch 417, acc0.94, loss 0.1889, tol 0, val_acc0.90, val_loss0.2689\n","Epoch 418, acc0.93, loss 0.2130, tol 0, val_acc0.90, val_loss0.2647\n","Epoch 419, acc0.94, loss 0.1909, tol 0, val_acc0.90, val_loss0.2526\n","Epoch 420, acc0.93, loss 0.2209, tol 0, val_acc0.89, val_loss0.2608\n","Epoch 421, acc0.93, loss 0.2076, tol 0, val_acc0.89, val_loss0.2702\n","Epoch 422, acc0.93, loss 0.2002, tol 0, val_acc0.89, val_loss0.2678\n","Epoch 423, acc0.94, loss 0.2007, tol 0, val_acc0.90, val_loss0.2629\n","Epoch 424, acc0.93, loss 0.2126, tol 0, val_acc0.89, val_loss0.2600\n","Epoch 425, acc0.95, loss 0.1838, tol 0, val_acc0.89, val_loss0.2745\n","Epoch 426, acc0.93, loss 0.2076, tol 0, val_acc0.90, val_loss0.2727\n","Epoch 427, acc0.94, loss 0.1958, tol 0, val_acc0.89, val_loss0.2571\n","Epoch 428, acc0.93, loss 0.1883, tol 0, val_acc0.90, val_loss0.2660\n","Epoch 429, acc0.93, loss 0.2133, tol 0, val_acc0.90, val_loss0.2567\n","Epoch 430, acc0.94, loss 0.1842, tol 0, val_acc0.89, val_loss0.2650\n","Epoch 431, acc0.93, loss 0.2148, tol 0, val_acc0.90, val_loss0.2619\n","Epoch 432, acc0.93, loss 0.1872, tol 0, val_acc0.89, val_loss0.2694\n","Epoch 433, acc0.93, loss 0.2131, tol 0, val_acc0.90, val_loss0.2606\n","Epoch 434, acc0.94, loss 0.2050, tol 0, val_acc0.90, val_loss0.2692\n","Epoch 435, acc0.94, loss 0.1915, tol 0, val_acc0.90, val_loss0.2640\n","Epoch 436, acc0.92, loss 0.2284, tol 0, val_acc0.90, val_loss0.2643\n","Epoch 437, acc0.94, loss 0.2112, tol 0, val_acc0.89, val_loss0.2688\n","Epoch 438, acc0.94, loss 0.2093, tol 0, val_acc0.90, val_loss0.2563\n","Epoch 439, acc0.94, loss 0.1921, tol 0, val_acc0.90, val_loss0.2595\n","Epoch 440, acc0.93, loss 0.1909, tol 0, val_acc0.90, val_loss0.2598\n","Epoch 441, acc0.93, loss 0.2023, tol 0, val_acc0.89, val_loss0.2652\n","Epoch 442, acc0.94, loss 0.1918, tol 0, val_acc0.89, val_loss0.2637\n","Epoch 443, acc0.93, loss 0.2312, tol 0, val_acc0.90, val_loss0.2579\n","Epoch 444, acc0.95, loss 0.1646, tol 0, val_acc0.90, val_loss0.2621\n","Epoch 445, acc0.93, loss 0.1936, tol 0, val_acc0.89, val_loss0.2676\n","Epoch 446, acc0.94, loss 0.1829, tol 0, val_acc0.89, val_loss0.2590\n","Epoch 447, acc0.94, loss 0.1856, tol 0, val_acc0.89, val_loss0.2716\n","Epoch 448, acc0.93, loss 0.2118, tol 0, val_acc0.90, val_loss0.2556\n","Epoch 449, acc0.94, loss 0.1966, tol 0, val_acc0.90, val_loss0.2672\n","Epoch 450, acc0.93, loss 0.2067, tol 0, val_acc0.90, val_loss0.2591\n","Epoch 451, acc0.93, loss 0.2115, tol 0, val_acc0.90, val_loss0.2605\n","Epoch 452, acc0.94, loss 0.1922, tol 0, val_acc0.90, val_loss0.2692\n","Epoch 453, acc0.94, loss 0.2083, tol 0, val_acc0.90, val_loss0.2503\n","Epoch 454, acc0.93, loss 0.1955, tol 0, val_acc0.89, val_loss0.2617\n","Epoch 455, acc0.95, loss 0.1737, tol 0, val_acc0.90, val_loss0.2583\n","Epoch 456, acc0.94, loss 0.1943, tol 0, val_acc0.90, val_loss0.2690\n","Epoch 457, acc0.94, loss 0.1843, tol 0, val_acc0.90, val_loss0.2700\n","Epoch 458, acc0.95, loss 0.1827, tol 0, val_acc0.89, val_loss0.2745\n","Epoch 459, acc0.94, loss 0.1922, tol 0, val_acc0.90, val_loss0.2735\n","Epoch 460, acc0.94, loss 0.1795, tol 0, val_acc0.89, val_loss0.2629\n","Epoch 461, acc0.93, loss 0.2005, tol 0, val_acc0.89, val_loss0.2646\n","Epoch 462, acc0.93, loss 0.2086, tol 0, val_acc0.91, val_loss0.2523\n","Epoch 463, acc0.94, loss 0.2011, tol 0, val_acc0.90, val_loss0.2554\n","Epoch 464, acc0.93, loss 0.2108, tol 0, val_acc0.90, val_loss0.2539\n","Epoch 465, acc0.93, loss 0.1993, tol 0, val_acc0.90, val_loss0.2547\n","Epoch 466, acc0.94, loss 0.1930, tol 0, val_acc0.90, val_loss0.2647\n","Epoch 467, acc0.95, loss 0.1757, tol 0, val_acc0.90, val_loss0.2652\n","Epoch 468, acc0.94, loss 0.1865, tol 0, val_acc0.90, val_loss0.2579\n","Epoch 469, acc0.93, loss 0.2174, tol 0, val_acc0.89, val_loss0.2779\n","Epoch 470, acc0.93, loss 0.1973, tol 0, val_acc0.90, val_loss0.2557\n","Epoch 471, acc0.93, loss 0.2065, tol 0, val_acc0.90, val_loss0.2641\n","Epoch 472, acc0.94, loss 0.1851, tol 0, val_acc0.89, val_loss0.2592\n","Epoch 473, acc0.94, loss 0.1742, tol 0, val_acc0.89, val_loss0.2939\n","Epoch 474, acc0.93, loss 0.1954, tol 0, val_acc0.90, val_loss0.2629\n","Epoch 475, acc0.95, loss 0.1732, tol 0, val_acc0.91, val_loss0.2524\n","Epoch 476, acc0.95, loss 0.1684, tol 0, val_acc0.90, val_loss0.2521\n","Epoch 477, acc0.94, loss 0.1910, tol 0, val_acc0.90, val_loss0.2548\n","Epoch 478, acc0.95, loss 0.1596, tol 0, val_acc0.90, val_loss0.2718\n","Epoch 479, acc0.93, loss 0.2038, tol 0, val_acc0.89, val_loss0.2750\n","Epoch 480, acc0.93, loss 0.2091, tol 0, val_acc0.90, val_loss0.2544\n","Epoch 481, acc0.94, loss 0.1830, tol 0, val_acc0.90, val_loss0.2560\n","Epoch 482, acc0.93, loss 0.1907, tol 0, val_acc0.90, val_loss0.2605\n","Epoch 483, acc0.94, loss 0.1785, tol 0, val_acc0.89, val_loss0.2561\n","Epoch 484, acc0.94, loss 0.2030, tol 0, val_acc0.90, val_loss0.2472\n","Epoch 485, acc0.93, loss 0.1904, tol 0, val_acc0.89, val_loss0.2598\n","Epoch 486, acc0.94, loss 0.1829, tol 0, val_acc0.90, val_loss0.2559\n","Epoch 487, acc0.93, loss 0.2210, tol 0, val_acc0.90, val_loss0.2495\n","Epoch 488, acc0.94, loss 0.1825, tol 0, val_acc0.91, val_loss0.2513\n","Epoch 489, acc0.94, loss 0.1946, tol 0, val_acc0.90, val_loss0.2521\n","Epoch 490, acc0.94, loss 0.1882, tol 0, val_acc0.90, val_loss0.2506\n","Epoch 491, acc0.95, loss 0.1738, tol 0, val_acc0.90, val_loss0.2536\n","Epoch 492, acc0.95, loss 0.1735, tol 0, val_acc0.90, val_loss0.2591\n","Epoch 493, acc0.95, loss 0.1757, tol 0, val_acc0.90, val_loss0.2506\n","Epoch 494, acc0.93, loss 0.1977, tol 0, val_acc0.90, val_loss0.2581\n","Epoch 495, acc0.94, loss 0.1742, tol 0, val_acc0.89, val_loss0.2890\n","Epoch 496, acc0.93, loss 0.2061, tol 0, val_acc0.89, val_loss0.2674\n","Epoch 497, acc0.93, loss 0.1770, tol 0, val_acc0.90, val_loss0.2528\n","Epoch 498, acc0.94, loss 0.1841, tol 0, val_acc0.90, val_loss0.2521\n","Epoch 499, acc0.93, loss 0.2141, tol 0, val_acc0.90, val_loss0.2453\n","Epoch 500, acc0.94, loss 0.1796, tol 0, val_acc0.90, val_loss0.2484\n","Epoch 501, acc0.93, loss 0.1899, tol 1, val_acc0.90, val_loss0.2504\n","Reach Max Epoch Number\n","+-----------------+------------+\n","|     Modules     | Parameters |\n","+-----------------+------------+\n","|   conv1.weight  |    160     |\n","|    conv1.bias   |     32     |\n","|   conv2.weight  |    1024    |\n","|    conv2.bias   |     32     |\n","| classify.weight |     64     |\n","|  classify.bias  |     2      |\n","+-----------------+------------+\n","Total Trainable Params: 1314\n","Epoch 0, acc0.50, loss 0.7544, tol 0, val_acc0.50, val_loss0.7224 -- checkpoint saved\n","Epoch 1, acc0.49, loss 0.7034, tol 0, val_acc0.50, val_loss0.6980 -- checkpoint saved\n","Epoch 2, acc0.49, loss 0.6984, tol 0, val_acc0.49, val_loss0.6944\n","Epoch 3, acc0.51, loss 0.6948, tol 0, val_acc0.50, val_loss0.6959\n","Epoch 4, acc0.51, loss 0.6950, tol 0, val_acc0.49, val_loss0.6934\n","Epoch 5, acc0.51, loss 0.6957, tol 0, val_acc0.50, val_loss0.6928 -- checkpoint saved\n","Epoch 6, acc0.49, loss 0.6943, tol 0, val_acc0.50, val_loss0.6929\n","Epoch 7, acc0.52, loss 0.6929, tol 0, val_acc0.50, val_loss0.6943\n","Epoch 8, acc0.51, loss 0.6946, tol 0, val_acc0.50, val_loss0.6949\n","Epoch 9, acc0.51, loss 0.6927, tol 0, val_acc0.51, val_loss0.6923 -- checkpoint saved\n","Epoch 10, acc0.51, loss 0.6931, tol 0, val_acc0.52, val_loss0.6923\n","Epoch 11, acc0.51, loss 0.6929, tol 0, val_acc0.50, val_loss0.6937\n","Epoch 12, acc0.51, loss 0.6923, tol 0, val_acc0.50, val_loss0.6924\n","Epoch 13, acc0.51, loss 0.6935, tol 0, val_acc0.51, val_loss0.6924\n","Epoch 14, acc0.51, loss 0.6924, tol 0, val_acc0.54, val_loss0.6917 -- checkpoint saved\n","Epoch 15, acc0.52, loss 0.6914, tol 0, val_acc0.50, val_loss0.6917\n","Epoch 16, acc0.51, loss 0.6913, tol 0, val_acc0.56, val_loss0.6911 -- checkpoint saved\n","Epoch 17, acc0.51, loss 0.6919, tol 0, val_acc0.56, val_loss0.6910\n","Epoch 18, acc0.54, loss 0.6907, tol 0, val_acc0.50, val_loss0.6917\n","Epoch 19, acc0.52, loss 0.6906, tol 0, val_acc0.50, val_loss0.6934\n","Epoch 20, acc0.51, loss 0.6938, tol 0, val_acc0.54, val_loss0.6906\n","Epoch 21, acc0.51, loss 0.6922, tol 0, val_acc0.50, val_loss0.6930\n","Epoch 22, acc0.54, loss 0.6905, tol 0, val_acc0.57, val_loss0.6902 -- checkpoint saved\n","Epoch 23, acc0.51, loss 0.6911, tol 0, val_acc0.59, val_loss0.6900 -- checkpoint saved\n","Epoch 24, acc0.54, loss 0.6901, tol 0, val_acc0.58, val_loss0.6898\n","Epoch 25, acc0.57, loss 0.6899, tol 0, val_acc0.59, val_loss0.6896\n","Epoch 26, acc0.54, loss 0.6903, tol 0, val_acc0.52, val_loss0.6899\n","Epoch 27, acc0.55, loss 0.6899, tol 0, val_acc0.51, val_loss0.6907\n","Epoch 28, acc0.53, loss 0.6909, tol 0, val_acc0.50, val_loss0.6946\n","Epoch 29, acc0.53, loss 0.6921, tol 0, val_acc0.50, val_loss0.6898\n","Epoch 30, acc0.50, loss 0.6930, tol 0, val_acc0.50, val_loss0.6919\n","Epoch 31, acc0.52, loss 0.6921, tol 0, val_acc0.50, val_loss0.6903\n","Epoch 32, acc0.52, loss 0.6904, tol 0, val_acc0.50, val_loss0.6914\n","Epoch 33, acc0.52, loss 0.6894, tol 0, val_acc0.50, val_loss0.6911\n","Epoch 34, acc0.54, loss 0.6906, tol 0, val_acc0.50, val_loss0.6916\n","Epoch 35, acc0.53, loss 0.6904, tol 0, val_acc0.52, val_loss0.6889\n","Epoch 36, acc0.54, loss 0.6893, tol 0, val_acc0.51, val_loss0.6899\n","Epoch 37, acc0.51, loss 0.6915, tol 0, val_acc0.50, val_loss0.6903\n","Epoch 38, acc0.52, loss 0.6898, tol 0, val_acc0.51, val_loss0.6886\n","Epoch 39, acc0.54, loss 0.6895, tol 0, val_acc0.54, val_loss0.6881\n","Epoch 40, acc0.54, loss 0.6898, tol 0, val_acc0.50, val_loss0.6905\n","Epoch 41, acc0.51, loss 0.6915, tol 0, val_acc0.57, val_loss0.6877\n","Epoch 42, acc0.52, loss 0.6910, tol 0, val_acc0.51, val_loss0.6888\n","Epoch 43, acc0.52, loss 0.6896, tol 0, val_acc0.53, val_loss0.6880\n","Epoch 44, acc0.54, loss 0.6889, tol 0, val_acc0.51, val_loss0.6883\n","Epoch 45, acc0.54, loss 0.6882, tol 0, val_acc0.52, val_loss0.6879\n","Epoch 46, acc0.52, loss 0.6893, tol 0, val_acc0.50, val_loss0.6890\n","Epoch 47, acc0.52, loss 0.6891, tol 0, val_acc0.50, val_loss0.6888\n","Epoch 48, acc0.51, loss 0.6899, tol 0, val_acc0.54, val_loss0.6872\n","Epoch 49, acc0.55, loss 0.6883, tol 0, val_acc0.59, val_loss0.6865\n","Epoch 50, acc0.55, loss 0.6870, tol 0, val_acc0.60, val_loss0.6863 -- checkpoint saved\n","Epoch 51, acc0.57, loss 0.6872, tol 0, val_acc0.53, val_loss0.6870\n","Epoch 52, acc0.54, loss 0.6885, tol 0, val_acc0.60, val_loss0.6861\n","Epoch 53, acc0.57, loss 0.6873, tol 0, val_acc0.55, val_loss0.6864\n","Epoch 54, acc0.56, loss 0.6858, tol 0, val_acc0.52, val_loss0.6871\n","Epoch 55, acc0.53, loss 0.6865, tol 0, val_acc0.55, val_loss0.6863\n","Epoch 56, acc0.52, loss 0.6892, tol 0, val_acc0.58, val_loss0.6859\n","Epoch 57, acc0.51, loss 0.6909, tol 0, val_acc0.59, val_loss0.6857\n","Epoch 58, acc0.57, loss 0.6858, tol 0, val_acc0.55, val_loss0.6860\n","Epoch 59, acc0.53, loss 0.6884, tol 0, val_acc0.54, val_loss0.6868\n","Epoch 60, acc0.55, loss 0.6859, tol 0, val_acc0.52, val_loss0.6864\n","Epoch 61, acc0.53, loss 0.6874, tol 0, val_acc0.51, val_loss0.6878\n","Epoch 62, acc0.56, loss 0.6868, tol 0, val_acc0.60, val_loss0.6846\n","Epoch 63, acc0.55, loss 0.6874, tol 0, val_acc0.50, val_loss0.6896\n","Epoch 64, acc0.56, loss 0.6855, tol 0, val_acc0.59, val_loss0.6847\n","Epoch 65, acc0.57, loss 0.6830, tol 0, val_acc0.51, val_loss0.6887\n","Epoch 66, acc0.53, loss 0.6885, tol 0, val_acc0.58, val_loss0.6836\n","Epoch 67, acc0.56, loss 0.6864, tol 0, val_acc0.57, val_loss0.6840\n","Epoch 68, acc0.57, loss 0.6835, tol 0, val_acc0.58, val_loss0.6830\n","Epoch 69, acc0.56, loss 0.6846, tol 0, val_acc0.52, val_loss0.6865\n","Epoch 70, acc0.54, loss 0.6837, tol 0, val_acc0.58, val_loss0.6834\n","Epoch 71, acc0.55, loss 0.6863, tol 0, val_acc0.50, val_loss0.6899\n","Epoch 72, acc0.52, loss 0.6879, tol 0, val_acc0.60, val_loss0.6820 -- checkpoint saved\n","Epoch 73, acc0.55, loss 0.6829, tol 0, val_acc0.60, val_loss0.6817\n","Epoch 74, acc0.56, loss 0.6840, tol 0, val_acc0.58, val_loss0.6821\n","Epoch 75, acc0.58, loss 0.6809, tol 0, val_acc0.58, val_loss0.6814\n","Epoch 76, acc0.57, loss 0.6839, tol 0, val_acc0.60, val_loss0.6810\n","Epoch 77, acc0.56, loss 0.6825, tol 0, val_acc0.56, val_loss0.6834\n","Epoch 78, acc0.55, loss 0.6820, tol 0, val_acc0.58, val_loss0.6809\n","Epoch 79, acc0.56, loss 0.6842, tol 0, val_acc0.56, val_loss0.6811\n","Epoch 80, acc0.57, loss 0.6820, tol 0, val_acc0.60, val_loss0.6805\n","Epoch 81, acc0.62, loss 0.6803, tol 0, val_acc0.56, val_loss0.6812\n","Epoch 82, acc0.54, loss 0.6870, tol 0, val_acc0.57, val_loss0.6803\n","Epoch 83, acc0.56, loss 0.6839, tol 0, val_acc0.61, val_loss0.6792 -- checkpoint saved\n","Epoch 84, acc0.58, loss 0.6792, tol 0, val_acc0.61, val_loss0.6793\n","Epoch 85, acc0.55, loss 0.6815, tol 0, val_acc0.51, val_loss0.6874\n","Epoch 86, acc0.57, loss 0.6807, tol 0, val_acc0.61, val_loss0.6787\n","Epoch 87, acc0.58, loss 0.6819, tol 0, val_acc0.52, val_loss0.6867\n","Epoch 88, acc0.54, loss 0.6860, tol 0, val_acc0.59, val_loss0.6790\n","Epoch 89, acc0.57, loss 0.6779, tol 0, val_acc0.61, val_loss0.6782\n","Epoch 90, acc0.56, loss 0.6819, tol 0, val_acc0.60, val_loss0.6794\n","Epoch 91, acc0.55, loss 0.6810, tol 0, val_acc0.58, val_loss0.6780\n","Epoch 92, acc0.58, loss 0.6803, tol 0, val_acc0.56, val_loss0.6786\n","Epoch 93, acc0.57, loss 0.6810, tol 0, val_acc0.56, val_loss0.6811\n","Epoch 94, acc0.55, loss 0.6822, tol 0, val_acc0.61, val_loss0.6765\n","Epoch 95, acc0.56, loss 0.6807, tol 0, val_acc0.58, val_loss0.6777\n","Epoch 96, acc0.56, loss 0.6780, tol 0, val_acc0.60, val_loss0.6763\n","Epoch 97, acc0.57, loss 0.6807, tol 0, val_acc0.62, val_loss0.6761 -- checkpoint saved\n","Epoch 98, acc0.55, loss 0.6809, tol 0, val_acc0.58, val_loss0.6777\n","Epoch 99, acc0.58, loss 0.6801, tol 0, val_acc0.61, val_loss0.6760\n","Epoch 100, acc0.59, loss 0.6774, tol 0, val_acc0.61, val_loss0.6752\n","Epoch 101, acc0.62, loss 0.6747, tol 0, val_acc0.59, val_loss0.6761\n","Epoch 102, acc0.59, loss 0.6771, tol 0, val_acc0.61, val_loss0.6761\n","Epoch 103, acc0.59, loss 0.6752, tol 0, val_acc0.53, val_loss0.6818\n","Epoch 104, acc0.59, loss 0.6751, tol 0, val_acc0.63, val_loss0.6741 -- checkpoint saved\n","Epoch 105, acc0.60, loss 0.6756, tol 0, val_acc0.62, val_loss0.6734\n","Epoch 106, acc0.62, loss 0.6730, tol 0, val_acc0.60, val_loss0.6736\n","Epoch 107, acc0.60, loss 0.6752, tol 0, val_acc0.63, val_loss0.6733 -- checkpoint saved\n","Epoch 108, acc0.60, loss 0.6764, tol 0, val_acc0.60, val_loss0.6748\n","Epoch 109, acc0.60, loss 0.6737, tol 0, val_acc0.60, val_loss0.6728\n","Epoch 110, acc0.60, loss 0.6773, tol 0, val_acc0.62, val_loss0.6718\n","Epoch 111, acc0.60, loss 0.6777, tol 0, val_acc0.62, val_loss0.6715\n","Epoch 112, acc0.61, loss 0.6712, tol 0, val_acc0.61, val_loss0.6714\n","Epoch 113, acc0.57, loss 0.6781, tol 0, val_acc0.55, val_loss0.6776\n","Epoch 114, acc0.59, loss 0.6746, tol 0, val_acc0.59, val_loss0.6732\n","Epoch 115, acc0.60, loss 0.6745, tol 0, val_acc0.63, val_loss0.6705 -- checkpoint saved\n","Epoch 116, acc0.58, loss 0.6773, tol 0, val_acc0.64, val_loss0.6701 -- checkpoint saved\n","Epoch 117, acc0.57, loss 0.6781, tol 0, val_acc0.58, val_loss0.6729\n","Epoch 118, acc0.59, loss 0.6739, tol 0, val_acc0.58, val_loss0.6728\n","Epoch 119, acc0.60, loss 0.6692, tol 0, val_acc0.58, val_loss0.6726\n","Epoch 120, acc0.54, loss 0.6845, tol 0, val_acc0.60, val_loss0.6695\n","Epoch 121, acc0.57, loss 0.6744, tol 0, val_acc0.58, val_loss0.6733\n","Epoch 122, acc0.60, loss 0.6715, tol 0, val_acc0.63, val_loss0.6693\n","Epoch 123, acc0.59, loss 0.6723, tol 0, val_acc0.57, val_loss0.6732\n","Epoch 124, acc0.62, loss 0.6703, tol 0, val_acc0.64, val_loss0.6685\n","Epoch 125, acc0.61, loss 0.6712, tol 0, val_acc0.64, val_loss0.6679 -- checkpoint saved\n","Epoch 126, acc0.61, loss 0.6666, tol 0, val_acc0.62, val_loss0.6683\n","Epoch 127, acc0.61, loss 0.6672, tol 0, val_acc0.63, val_loss0.6685\n","Epoch 128, acc0.61, loss 0.6711, tol 0, val_acc0.63, val_loss0.6683\n","Epoch 129, acc0.59, loss 0.6706, tol 0, val_acc0.62, val_loss0.6672\n","Epoch 130, acc0.60, loss 0.6725, tol 0, val_acc0.62, val_loss0.6681\n","Epoch 131, acc0.57, loss 0.6774, tol 0, val_acc0.62, val_loss0.6681\n","Epoch 132, acc0.60, loss 0.6724, tol 0, val_acc0.55, val_loss0.6737\n","Epoch 133, acc0.59, loss 0.6728, tol 0, val_acc0.62, val_loss0.6657\n","Epoch 134, acc0.62, loss 0.6653, tol 0, val_acc0.61, val_loss0.6669\n","Epoch 135, acc0.60, loss 0.6700, tol 0, val_acc0.62, val_loss0.6669\n","Epoch 136, acc0.62, loss 0.6671, tol 0, val_acc0.61, val_loss0.6690\n","Epoch 137, acc0.64, loss 0.6657, tol 0, val_acc0.62, val_loss0.6671\n","Epoch 138, acc0.58, loss 0.6711, tol 0, val_acc0.54, val_loss0.6796\n","Epoch 139, acc0.56, loss 0.6775, tol 0, val_acc0.60, val_loss0.6655\n","Epoch 140, acc0.62, loss 0.6666, tol 0, val_acc0.56, val_loss0.6734\n","Epoch 141, acc0.59, loss 0.6707, tol 0, val_acc0.60, val_loss0.6699\n","Epoch 142, acc0.60, loss 0.6680, tol 0, val_acc0.59, val_loss0.6697\n","Epoch 143, acc0.57, loss 0.6749, tol 0, val_acc0.64, val_loss0.6634 -- checkpoint saved\n","Epoch 144, acc0.61, loss 0.6640, tol 0, val_acc0.58, val_loss0.6706\n","Epoch 145, acc0.60, loss 0.6697, tol 0, val_acc0.63, val_loss0.6637\n","Epoch 146, acc0.60, loss 0.6688, tol 0, val_acc0.61, val_loss0.6630\n","Epoch 147, acc0.58, loss 0.6723, tol 0, val_acc0.62, val_loss0.6650\n","Epoch 148, acc0.59, loss 0.6712, tol 0, val_acc0.57, val_loss0.6694\n","Epoch 149, acc0.61, loss 0.6653, tol 0, val_acc0.59, val_loss0.6682\n","Epoch 150, acc0.60, loss 0.6671, tol 0, val_acc0.62, val_loss0.6621\n","Epoch 151, acc0.58, loss 0.6715, tol 0, val_acc0.57, val_loss0.6687\n","Epoch 152, acc0.61, loss 0.6639, tol 0, val_acc0.63, val_loss0.6614\n","Epoch 153, acc0.62, loss 0.6653, tol 0, val_acc0.63, val_loss0.6615\n","Epoch 154, acc0.61, loss 0.6624, tol 0, val_acc0.62, val_loss0.6604\n","Epoch 155, acc0.62, loss 0.6678, tol 0, val_acc0.62, val_loss0.6603\n","Epoch 156, acc0.61, loss 0.6648, tol 0, val_acc0.55, val_loss0.6747\n","Epoch 157, acc0.62, loss 0.6549, tol 0, val_acc0.62, val_loss0.6595\n","Epoch 158, acc0.62, loss 0.6607, tol 0, val_acc0.63, val_loss0.6610\n","Epoch 159, acc0.62, loss 0.6593, tol 0, val_acc0.61, val_loss0.6641\n","Epoch 160, acc0.62, loss 0.6603, tol 0, val_acc0.62, val_loss0.6591\n","Epoch 161, acc0.61, loss 0.6633, tol 0, val_acc0.62, val_loss0.6588\n","Epoch 162, acc0.62, loss 0.6640, tol 0, val_acc0.62, val_loss0.6585\n","Epoch 163, acc0.63, loss 0.6608, tol 0, val_acc0.64, val_loss0.6585\n","Epoch 164, acc0.62, loss 0.6614, tol 0, val_acc0.62, val_loss0.6617\n","Epoch 165, acc0.63, loss 0.6563, tol 0, val_acc0.63, val_loss0.6576\n","Epoch 166, acc0.63, loss 0.6571, tol 0, val_acc0.63, val_loss0.6578\n","Epoch 167, acc0.63, loss 0.6572, tol 0, val_acc0.63, val_loss0.6568\n","Epoch 168, acc0.61, loss 0.6615, tol 0, val_acc0.62, val_loss0.6564\n","Epoch 169, acc0.61, loss 0.6609, tol 0, val_acc0.58, val_loss0.6683\n","Epoch 170, acc0.60, loss 0.6675, tol 0, val_acc0.63, val_loss0.6573\n","Epoch 171, acc0.61, loss 0.6623, tol 0, val_acc0.62, val_loss0.6567\n","Epoch 172, acc0.60, loss 0.6622, tol 0, val_acc0.60, val_loss0.6637\n","Epoch 173, acc0.62, loss 0.6602, tol 0, val_acc0.61, val_loss0.6573\n","Epoch 174, acc0.63, loss 0.6554, tol 0, val_acc0.62, val_loss0.6559\n","Epoch 175, acc0.63, loss 0.6580, tol 0, val_acc0.60, val_loss0.6622\n","Epoch 176, acc0.62, loss 0.6577, tol 0, val_acc0.62, val_loss0.6547\n","Epoch 177, acc0.62, loss 0.6578, tol 0, val_acc0.62, val_loss0.6571\n","Epoch 178, acc0.62, loss 0.6597, tol 0, val_acc0.62, val_loss0.6553\n","Epoch 179, acc0.62, loss 0.6573, tol 0, val_acc0.59, val_loss0.6633\n","Epoch 180, acc0.59, loss 0.6582, tol 0, val_acc0.62, val_loss0.6549\n","Epoch 181, acc0.59, loss 0.6617, tol 0, val_acc0.62, val_loss0.6563\n","Epoch 182, acc0.63, loss 0.6523, tol 0, val_acc0.63, val_loss0.6597\n","Epoch 183, acc0.62, loss 0.6603, tol 0, val_acc0.62, val_loss0.6545\n","Epoch 184, acc0.63, loss 0.6573, tol 0, val_acc0.61, val_loss0.6559\n","Epoch 185, acc0.65, loss 0.6485, tol 0, val_acc0.62, val_loss0.6531\n","Epoch 186, acc0.61, loss 0.6630, tol 0, val_acc0.63, val_loss0.6535\n","Epoch 187, acc0.64, loss 0.6543, tol 0, val_acc0.63, val_loss0.6533\n","Epoch 188, acc0.61, loss 0.6592, tol 0, val_acc0.62, val_loss0.6531\n","Epoch 189, acc0.63, loss 0.6590, tol 0, val_acc0.63, val_loss0.6524\n","Epoch 190, acc0.62, loss 0.6528, tol 0, val_acc0.62, val_loss0.6520\n","Epoch 191, acc0.61, loss 0.6615, tol 0, val_acc0.60, val_loss0.6577\n","Epoch 192, acc0.62, loss 0.6565, tol 0, val_acc0.62, val_loss0.6517\n","Epoch 193, acc0.63, loss 0.6523, tol 0, val_acc0.62, val_loss0.6547\n","Epoch 194, acc0.62, loss 0.6566, tol 0, val_acc0.62, val_loss0.6533\n","Epoch 195, acc0.64, loss 0.6482, tol 0, val_acc0.63, val_loss0.6543\n","Epoch 196, acc0.65, loss 0.6513, tol 0, val_acc0.61, val_loss0.6638\n","Epoch 197, acc0.61, loss 0.6590, tol 0, val_acc0.64, val_loss0.6572\n","Epoch 198, acc0.62, loss 0.6550, tol 0, val_acc0.62, val_loss0.6503\n","Epoch 199, acc0.62, loss 0.6579, tol 0, val_acc0.62, val_loss0.6544\n","Epoch 200, acc0.62, loss 0.6548, tol 0, val_acc0.61, val_loss0.6505\n","Epoch 201, acc0.61, loss 0.6597, tol 0, val_acc0.63, val_loss0.6584\n","Epoch 202, acc0.60, loss 0.6568, tol 0, val_acc0.62, val_loss0.6497\n","Epoch 203, acc0.62, loss 0.6532, tol 0, val_acc0.59, val_loss0.6564\n","Epoch 204, acc0.62, loss 0.6542, tol 0, val_acc0.63, val_loss0.6493\n","Epoch 205, acc0.61, loss 0.6608, tol 0, val_acc0.62, val_loss0.6506\n","Epoch 206, acc0.61, loss 0.6595, tol 0, val_acc0.62, val_loss0.6495\n","Epoch 207, acc0.60, loss 0.6562, tol 0, val_acc0.61, val_loss0.6534\n","Epoch 208, acc0.60, loss 0.6618, tol 0, val_acc0.59, val_loss0.6675\n","Epoch 209, acc0.57, loss 0.6758, tol 0, val_acc0.58, val_loss0.6588\n","Epoch 210, acc0.64, loss 0.6448, tol 0, val_acc0.63, val_loss0.6495\n","Epoch 211, acc0.61, loss 0.6580, tol 0, val_acc0.63, val_loss0.6571\n","Epoch 212, acc0.62, loss 0.6542, tol 0, val_acc0.59, val_loss0.6666\n","Epoch 213, acc0.61, loss 0.6655, tol 0, val_acc0.63, val_loss0.6532\n","Epoch 214, acc0.62, loss 0.6552, tol 0, val_acc0.57, val_loss0.6710\n","Epoch 215, acc0.57, loss 0.6716, tol 0, val_acc0.63, val_loss0.6480\n","Epoch 216, acc0.61, loss 0.6591, tol 0, val_acc0.55, val_loss0.6775\n","Epoch 217, acc0.61, loss 0.6571, tol 0, val_acc0.62, val_loss0.6502\n","Epoch 218, acc0.62, loss 0.6584, tol 0, val_acc0.62, val_loss0.6479\n","Epoch 219, acc0.64, loss 0.6478, tol 0, val_acc0.62, val_loss0.6497\n","Epoch 220, acc0.64, loss 0.6484, tol 0, val_acc0.62, val_loss0.6497\n","Epoch 221, acc0.65, loss 0.6439, tol 0, val_acc0.62, val_loss0.6477\n","Epoch 222, acc0.64, loss 0.6477, tol 0, val_acc0.63, val_loss0.6485\n","Epoch 223, acc0.65, loss 0.6469, tol 0, val_acc0.63, val_loss0.6474\n","Epoch 224, acc0.63, loss 0.6495, tol 0, val_acc0.63, val_loss0.6473\n","Epoch 225, acc0.62, loss 0.6549, tol 0, val_acc0.63, val_loss0.6474\n","Epoch 226, acc0.64, loss 0.6472, tol 0, val_acc0.63, val_loss0.6603\n","Epoch 227, acc0.64, loss 0.6426, tol 0, val_acc0.63, val_loss0.6469\n","Epoch 228, acc0.64, loss 0.6502, tol 0, val_acc0.63, val_loss0.6475\n","Epoch 229, acc0.65, loss 0.6433, tol 0, val_acc0.62, val_loss0.6485\n","Epoch 230, acc0.65, loss 0.6396, tol 0, val_acc0.63, val_loss0.6459\n","Epoch 231, acc0.63, loss 0.6437, tol 0, val_acc0.62, val_loss0.6498\n","Epoch 232, acc0.59, loss 0.6587, tol 0, val_acc0.59, val_loss0.6566\n","Epoch 233, acc0.64, loss 0.6465, tol 0, val_acc0.63, val_loss0.6455\n","Epoch 234, acc0.63, loss 0.6493, tol 0, val_acc0.62, val_loss0.6471\n","Epoch 235, acc0.64, loss 0.6487, tol 0, val_acc0.62, val_loss0.6455\n","Epoch 236, acc0.62, loss 0.6619, tol 0, val_acc0.60, val_loss0.6524\n","Epoch 237, acc0.61, loss 0.6672, tol 0, val_acc0.63, val_loss0.6452\n","Epoch 238, acc0.61, loss 0.6494, tol 0, val_acc0.62, val_loss0.6468\n","Epoch 239, acc0.62, loss 0.6460, tol 0, val_acc0.60, val_loss0.6643\n","Epoch 240, acc0.62, loss 0.6518, tol 0, val_acc0.62, val_loss0.6466\n","Epoch 241, acc0.63, loss 0.6509, tol 0, val_acc0.62, val_loss0.6444\n","Epoch 242, acc0.62, loss 0.6518, tol 0, val_acc0.60, val_loss0.6516\n","Epoch 243, acc0.64, loss 0.6412, tol 0, val_acc0.63, val_loss0.6442\n","Epoch 244, acc0.64, loss 0.6452, tol 0, val_acc0.62, val_loss0.6439\n","Epoch 245, acc0.63, loss 0.6493, tol 0, val_acc0.62, val_loss0.6437\n","Epoch 246, acc0.63, loss 0.6468, tol 0, val_acc0.62, val_loss0.6470\n","Epoch 247, acc0.63, loss 0.6487, tol 0, val_acc0.58, val_loss0.6700\n","Epoch 248, acc0.60, loss 0.6551, tol 0, val_acc0.63, val_loss0.6534\n","Epoch 249, acc0.61, loss 0.6591, tol 0, val_acc0.62, val_loss0.6448\n","Epoch 250, acc0.63, loss 0.6514, tol 0, val_acc0.63, val_loss0.6435\n","Epoch 251, acc0.64, loss 0.6404, tol 0, val_acc0.62, val_loss0.6432\n","Epoch 252, acc0.64, loss 0.6484, tol 0, val_acc0.63, val_loss0.6440\n","Epoch 253, acc0.65, loss 0.6401, tol 0, val_acc0.62, val_loss0.6459\n","Epoch 254, acc0.61, loss 0.6558, tol 0, val_acc0.63, val_loss0.6431\n","Epoch 255, acc0.65, loss 0.6436, tol 0, val_acc0.63, val_loss0.6466\n","Epoch 256, acc0.64, loss 0.6434, tol 0, val_acc0.63, val_loss0.6424\n","Epoch 257, acc0.65, loss 0.6440, tol 0, val_acc0.63, val_loss0.6424\n","Epoch 258, acc0.63, loss 0.6493, tol 0, val_acc0.63, val_loss0.6432\n","Epoch 259, acc0.64, loss 0.6512, tol 0, val_acc0.62, val_loss0.6469\n","Epoch 260, acc0.63, loss 0.6505, tol 0, val_acc0.63, val_loss0.6418\n","Epoch 261, acc0.64, loss 0.6414, tol 0, val_acc0.63, val_loss0.6498\n","Epoch 262, acc0.63, loss 0.6476, tol 0, val_acc0.62, val_loss0.6448\n","Epoch 263, acc0.65, loss 0.6405, tol 0, val_acc0.63, val_loss0.6417\n","Epoch 264, acc0.64, loss 0.6408, tol 0, val_acc0.62, val_loss0.6410\n","Epoch 265, acc0.60, loss 0.6636, tol 0, val_acc0.63, val_loss0.6452\n","Epoch 266, acc0.61, loss 0.6513, tol 0, val_acc0.64, val_loss0.6539\n","Epoch 267, acc0.62, loss 0.6479, tol 0, val_acc0.63, val_loss0.6562\n","Epoch 268, acc0.61, loss 0.6599, tol 0, val_acc0.63, val_loss0.6408\n","Epoch 269, acc0.64, loss 0.6383, tol 0, val_acc0.62, val_loss0.6445\n","Epoch 270, acc0.63, loss 0.6497, tol 0, val_acc0.60, val_loss0.6491\n","Epoch 271, acc0.63, loss 0.6460, tol 0, val_acc0.59, val_loss0.6542\n","Epoch 272, acc0.63, loss 0.6457, tol 0, val_acc0.63, val_loss0.6404\n","Epoch 273, acc0.62, loss 0.6578, tol 0, val_acc0.62, val_loss0.6418\n","Epoch 274, acc0.63, loss 0.6542, tol 0, val_acc0.63, val_loss0.6401\n","Epoch 275, acc0.65, loss 0.6418, tol 0, val_acc0.62, val_loss0.6451\n","Epoch 276, acc0.61, loss 0.6529, tol 0, val_acc0.60, val_loss0.6505\n","Epoch 277, acc0.65, loss 0.6445, tol 0, val_acc0.63, val_loss0.6423\n","Epoch 278, acc0.62, loss 0.6522, tol 0, val_acc0.62, val_loss0.6463\n","Epoch 279, acc0.61, loss 0.6553, tol 0, val_acc0.61, val_loss0.6469\n","Epoch 280, acc0.62, loss 0.6588, tol 0, val_acc0.61, val_loss0.6481\n","Epoch 281, acc0.61, loss 0.6562, tol 0, val_acc0.63, val_loss0.6421\n","Epoch 282, acc0.62, loss 0.6501, tol 0, val_acc0.63, val_loss0.6395\n","Epoch 283, acc0.64, loss 0.6413, tol 0, val_acc0.63, val_loss0.6390\n","Epoch 284, acc0.66, loss 0.6314, tol 0, val_acc0.63, val_loss0.6392\n","Epoch 285, acc0.65, loss 0.6402, tol 0, val_acc0.63, val_loss0.6387\n","Epoch 286, acc0.65, loss 0.6382, tol 0, val_acc0.63, val_loss0.6381\n","Epoch 287, acc0.62, loss 0.6512, tol 0, val_acc0.64, val_loss0.6388\n","Epoch 288, acc0.64, loss 0.6390, tol 0, val_acc0.63, val_loss0.6382\n","Epoch 289, acc0.64, loss 0.6406, tol 0, val_acc0.64, val_loss0.6407\n","Epoch 290, acc0.64, loss 0.6406, tol 0, val_acc0.63, val_loss0.6377\n","Epoch 291, acc0.65, loss 0.6408, tol 0, val_acc0.64, val_loss0.6382\n","Epoch 292, acc0.63, loss 0.6397, tol 0, val_acc0.63, val_loss0.6378\n","Epoch 293, acc0.64, loss 0.6383, tol 0, val_acc0.63, val_loss0.6488\n","Epoch 294, acc0.61, loss 0.6577, tol 0, val_acc0.63, val_loss0.6510\n","Epoch 295, acc0.63, loss 0.6459, tol 0, val_acc0.63, val_loss0.6372\n","Epoch 296, acc0.61, loss 0.6540, tol 0, val_acc0.63, val_loss0.6385\n","Epoch 297, acc0.58, loss 0.6660, tol 0, val_acc0.59, val_loss0.6529\n","Epoch 298, acc0.61, loss 0.6568, tol 0, val_acc0.62, val_loss0.6448\n","Epoch 299, acc0.62, loss 0.6424, tol 0, val_acc0.61, val_loss0.6468\n","Epoch 300, acc0.62, loss 0.6435, tol 0, val_acc0.63, val_loss0.6561\n","Epoch 301, acc0.62, loss 0.6453, tol 0, val_acc0.64, val_loss0.6501\n","Epoch 302, acc0.62, loss 0.6403, tol 0, val_acc0.64, val_loss0.6426\n","Epoch 303, acc0.66, loss 0.6457, tol 0, val_acc0.63, val_loss0.6401\n","Epoch 304, acc0.66, loss 0.6315, tol 0, val_acc0.62, val_loss0.6391\n","Epoch 305, acc0.66, loss 0.6366, tol 0, val_acc0.63, val_loss0.6410\n","Epoch 306, acc0.64, loss 0.6337, tol 0, val_acc0.62, val_loss0.6372\n","Epoch 307, acc0.66, loss 0.6340, tol 0, val_acc0.62, val_loss0.6368\n","Epoch 308, acc0.64, loss 0.6353, tol 0, val_acc0.62, val_loss0.6366\n","Epoch 309, acc0.65, loss 0.6420, tol 0, val_acc0.62, val_loss0.6365\n","Epoch 310, acc0.65, loss 0.6388, tol 0, val_acc0.63, val_loss0.6387\n","Epoch 311, acc0.64, loss 0.6369, tol 0, val_acc0.64, val_loss0.6407\n","Epoch 312, acc0.64, loss 0.6397, tol 0, val_acc0.64, val_loss0.6381\n","Epoch 313, acc0.64, loss 0.6479, tol 0, val_acc0.63, val_loss0.6392\n","Epoch 314, acc0.65, loss 0.6347, tol 0, val_acc0.62, val_loss0.6357\n","Epoch 315, acc0.65, loss 0.6368, tol 0, val_acc0.63, val_loss0.6347\n","Epoch 316, acc0.64, loss 0.6387, tol 0, val_acc0.64, val_loss0.6413\n","Epoch 317, acc0.64, loss 0.6376, tol 0, val_acc0.64, val_loss0.6336\n","Epoch 318, acc0.61, loss 0.6487, tol 0, val_acc0.60, val_loss0.6553\n","Epoch 319, acc0.61, loss 0.6529, tol 0, val_acc0.62, val_loss0.6423\n","Epoch 320, acc0.66, loss 0.6311, tol 0, val_acc0.62, val_loss0.6417\n","Epoch 321, acc0.63, loss 0.6447, tol 0, val_acc0.64, val_loss0.6343\n","Epoch 322, acc0.65, loss 0.6383, tol 0, val_acc0.63, val_loss0.6324\n","Epoch 323, acc0.65, loss 0.6335, tol 0, val_acc0.64, val_loss0.6335\n","Epoch 324, acc0.66, loss 0.6324, tol 0, val_acc0.63, val_loss0.6327\n","Epoch 325, acc0.63, loss 0.6430, tol 0, val_acc0.64, val_loss0.6460\n","Epoch 326, acc0.67, loss 0.6295, tol 0, val_acc0.63, val_loss0.6367\n","Epoch 327, acc0.64, loss 0.6332, tol 0, val_acc0.63, val_loss0.6316\n","Epoch 328, acc0.65, loss 0.6393, tol 0, val_acc0.62, val_loss0.6337\n","Epoch 329, acc0.65, loss 0.6392, tol 0, val_acc0.63, val_loss0.6529\n","Epoch 330, acc0.62, loss 0.6514, tol 0, val_acc0.65, val_loss0.6389\n","Epoch 331, acc0.67, loss 0.6343, tol 0, val_acc0.65, val_loss0.6338\n","Epoch 332, acc0.64, loss 0.6374, tol 0, val_acc0.61, val_loss0.6516\n","Epoch 333, acc0.65, loss 0.6301, tol 0, val_acc0.65, val_loss0.6314\n","Epoch 334, acc0.65, loss 0.6355, tol 0, val_acc0.63, val_loss0.6300\n","Epoch 335, acc0.65, loss 0.6349, tol 0, val_acc0.62, val_loss0.6318\n","Epoch 336, acc0.67, loss 0.6307, tol 0, val_acc0.63, val_loss0.6311\n","Epoch 337, acc0.64, loss 0.6374, tol 0, val_acc0.64, val_loss0.6295\n","Epoch 338, acc0.66, loss 0.6321, tol 0, val_acc0.64, val_loss0.6294\n","Epoch 339, acc0.66, loss 0.6282, tol 0, val_acc0.64, val_loss0.6292\n","Epoch 340, acc0.66, loss 0.6241, tol 0, val_acc0.63, val_loss0.6313\n","Epoch 341, acc0.65, loss 0.6304, tol 0, val_acc0.64, val_loss0.6292\n","Epoch 342, acc0.64, loss 0.6375, tol 0, val_acc0.64, val_loss0.6295\n","Epoch 343, acc0.66, loss 0.6315, tol 0, val_acc0.64, val_loss0.6291\n","Epoch 344, acc0.63, loss 0.6356, tol 0, val_acc0.64, val_loss0.6280\n","Epoch 345, acc0.64, loss 0.6453, tol 0, val_acc0.63, val_loss0.6278\n","Epoch 346, acc0.65, loss 0.6378, tol 0, val_acc0.63, val_loss0.6295\n","Epoch 347, acc0.64, loss 0.6325, tol 0, val_acc0.63, val_loss0.6307\n","Epoch 348, acc0.62, loss 0.6479, tol 0, val_acc0.63, val_loss0.6271\n","Epoch 349, acc0.65, loss 0.6247, tol 0, val_acc0.62, val_loss0.6380\n","Epoch 350, acc0.65, loss 0.6320, tol 0, val_acc0.64, val_loss0.6268\n","Epoch 351, acc0.63, loss 0.6335, tol 0, val_acc0.65, val_loss0.6423\n","Epoch 352, acc0.63, loss 0.6444, tol 0, val_acc0.64, val_loss0.6331\n","Epoch 353, acc0.67, loss 0.6225, tol 0, val_acc0.64, val_loss0.6275\n","Epoch 354, acc0.67, loss 0.6190, tol 0, val_acc0.64, val_loss0.6259\n","Epoch 355, acc0.65, loss 0.6338, tol 0, val_acc0.64, val_loss0.6265\n","Epoch 356, acc0.65, loss 0.6310, tol 0, val_acc0.65, val_loss0.6311\n","Epoch 357, acc0.66, loss 0.6334, tol 0, val_acc0.63, val_loss0.6269\n","Epoch 358, acc0.64, loss 0.6339, tol 0, val_acc0.63, val_loss0.6285\n","Epoch 359, acc0.66, loss 0.6217, tol 0, val_acc0.64, val_loss0.6250\n","Epoch 360, acc0.65, loss 0.6300, tol 0, val_acc0.64, val_loss0.6255\n","Epoch 361, acc0.67, loss 0.6119, tol 0, val_acc0.64, val_loss0.6268\n","Epoch 362, acc0.66, loss 0.6295, tol 0, val_acc0.64, val_loss0.6268\n","Epoch 363, acc0.66, loss 0.6268, tol 0, val_acc0.65, val_loss0.6273\n","Epoch 364, acc0.65, loss 0.6324, tol 0, val_acc0.65, val_loss0.6302\n","Epoch 365, acc0.64, loss 0.6328, tol 0, val_acc0.64, val_loss0.6305\n","Epoch 366, acc0.65, loss 0.6367, tol 0, val_acc0.65, val_loss0.6281\n","Epoch 367, acc0.67, loss 0.6240, tol 0, val_acc0.63, val_loss0.6226\n","Epoch 368, acc0.67, loss 0.6229, tol 0, val_acc0.66, val_loss0.6258\n","Epoch 369, acc0.67, loss 0.6211, tol 0, val_acc0.64, val_loss0.6219\n","Epoch 370, acc0.68, loss 0.6135, tol 0, val_acc0.65, val_loss0.6297\n","Epoch 371, acc0.64, loss 0.6363, tol 0, val_acc0.63, val_loss0.6213\n","Epoch 372, acc0.65, loss 0.6265, tol 0, val_acc0.65, val_loss0.6214\n","Epoch 373, acc0.67, loss 0.6253, tol 0, val_acc0.64, val_loss0.6221\n","Epoch 374, acc0.68, loss 0.6178, tol 0, val_acc0.64, val_loss0.6232\n","Epoch 375, acc0.66, loss 0.6225, tol 0, val_acc0.66, val_loss0.6316\n","Epoch 376, acc0.65, loss 0.6308, tol 0, val_acc0.64, val_loss0.6225\n","Epoch 377, acc0.66, loss 0.6202, tol 0, val_acc0.64, val_loss0.6233\n","Epoch 378, acc0.66, loss 0.6248, tol 0, val_acc0.65, val_loss0.6330\n","Epoch 379, acc0.67, loss 0.6242, tol 0, val_acc0.65, val_loss0.6233\n","Epoch 380, acc0.67, loss 0.6172, tol 0, val_acc0.65, val_loss0.6261\n","Epoch 381, acc0.65, loss 0.6279, tol 0, val_acc0.65, val_loss0.6247\n","Epoch 382, acc0.66, loss 0.6192, tol 0, val_acc0.66, val_loss0.6215\n","Epoch 383, acc0.66, loss 0.6153, tol 0, val_acc0.65, val_loss0.6214\n","Epoch 384, acc0.66, loss 0.6204, tol 0, val_acc0.65, val_loss0.6177\n","Epoch 385, acc0.65, loss 0.6261, tol 0, val_acc0.66, val_loss0.6172\n","Epoch 386, acc0.65, loss 0.6211, tol 0, val_acc0.65, val_loss0.6196\n","Epoch 387, acc0.65, loss 0.6253, tol 0, val_acc0.61, val_loss0.6631\n","Epoch 388, acc0.64, loss 0.6416, tol 0, val_acc0.63, val_loss0.6354\n","Epoch 389, acc0.63, loss 0.6444, tol 0, val_acc0.64, val_loss0.6259\n","Epoch 390, acc0.65, loss 0.6279, tol 0, val_acc0.65, val_loss0.6173\n","Epoch 391, acc0.67, loss 0.6280, tol 0, val_acc0.65, val_loss0.6164\n","Epoch 392, acc0.69, loss 0.6003, tol 0, val_acc0.65, val_loss0.6183\n","Epoch 393, acc0.69, loss 0.6027, tol 0, val_acc0.66, val_loss0.6154\n","Epoch 394, acc0.64, loss 0.6296, tol 0, val_acc0.65, val_loss0.6169\n","Epoch 395, acc0.66, loss 0.6120, tol 0, val_acc0.66, val_loss0.6163\n","Epoch 396, acc0.65, loss 0.6185, tol 0, val_acc0.65, val_loss0.6212\n","Epoch 397, acc0.66, loss 0.6190, tol 0, val_acc0.63, val_loss0.6325\n","Epoch 398, acc0.67, loss 0.6242, tol 0, val_acc0.65, val_loss0.6128\n","Epoch 399, acc0.67, loss 0.6123, tol 0, val_acc0.65, val_loss0.6158\n","Epoch 400, acc0.66, loss 0.6152, tol 0, val_acc0.64, val_loss0.6376\n","Epoch 401, acc0.66, loss 0.6215, tol 0, val_acc0.66, val_loss0.6222\n","Epoch 402, acc0.68, loss 0.6067, tol 0, val_acc0.66, val_loss0.6156\n","Epoch 403, acc0.67, loss 0.6159, tol 0, val_acc0.66, val_loss0.6164\n","Epoch 404, acc0.68, loss 0.6086, tol 0, val_acc0.66, val_loss0.6127\n","Epoch 405, acc0.66, loss 0.6108, tol 0, val_acc0.65, val_loss0.6096\n","Epoch 406, acc0.66, loss 0.6189, tol 0, val_acc0.66, val_loss0.6167\n","Epoch 407, acc0.66, loss 0.6156, tol 0, val_acc0.66, val_loss0.6122\n","Epoch 408, acc0.69, loss 0.6076, tol 0, val_acc0.66, val_loss0.6140\n","Epoch 409, acc0.67, loss 0.6095, tol 0, val_acc0.66, val_loss0.6191\n","Epoch 410, acc0.68, loss 0.6035, tol 0, val_acc0.66, val_loss0.6080\n","Epoch 411, acc0.66, loss 0.6205, tol 0, val_acc0.66, val_loss0.6108\n","Epoch 412, acc0.66, loss 0.6182, tol 0, val_acc0.66, val_loss0.6158\n","Epoch 413, acc0.68, loss 0.6044, tol 0, val_acc0.67, val_loss0.6088\n","Epoch 414, acc0.67, loss 0.6122, tol 0, val_acc0.66, val_loss0.6082\n","Epoch 415, acc0.68, loss 0.5981, tol 0, val_acc0.66, val_loss0.6125\n","Epoch 416, acc0.66, loss 0.6245, tol 0, val_acc0.67, val_loss0.6099\n","Epoch 417, acc0.67, loss 0.6182, tol 0, val_acc0.67, val_loss0.6047 -- checkpoint saved\n","Epoch 418, acc0.67, loss 0.6143, tol 0, val_acc0.66, val_loss0.6102\n","Epoch 419, acc0.67, loss 0.6221, tol 0, val_acc0.66, val_loss0.6041\n","Epoch 420, acc0.68, loss 0.6140, tol 0, val_acc0.67, val_loss0.6061\n","Epoch 421, acc0.68, loss 0.6096, tol 0, val_acc0.67, val_loss0.6029\n","Epoch 422, acc0.70, loss 0.6051, tol 0, val_acc0.66, val_loss0.6098\n","Epoch 423, acc0.67, loss 0.6130, tol 0, val_acc0.65, val_loss0.6282\n","Epoch 424, acc0.66, loss 0.6196, tol 0, val_acc0.64, val_loss0.6233\n","Epoch 425, acc0.66, loss 0.6202, tol 0, val_acc0.67, val_loss0.6081\n","Epoch 426, acc0.69, loss 0.6037, tol 0, val_acc0.67, val_loss0.6018\n","Epoch 427, acc0.66, loss 0.6189, tol 0, val_acc0.67, val_loss0.6014\n","Epoch 428, acc0.70, loss 0.5985, tol 0, val_acc0.68, val_loss0.6030\n","Epoch 429, acc0.69, loss 0.5935, tol 0, val_acc0.67, val_loss0.6034\n","Epoch 430, acc0.68, loss 0.6084, tol 0, val_acc0.66, val_loss0.6058\n","Epoch 431, acc0.68, loss 0.6012, tol 0, val_acc0.66, val_loss0.6070\n","Epoch 432, acc0.67, loss 0.6138, tol 0, val_acc0.67, val_loss0.6031\n","Epoch 433, acc0.67, loss 0.6105, tol 0, val_acc0.68, val_loss0.5981 -- checkpoint saved\n","Epoch 434, acc0.68, loss 0.6061, tol 0, val_acc0.67, val_loss0.5980\n","Epoch 435, acc0.69, loss 0.6034, tol 0, val_acc0.65, val_loss0.6155\n","Epoch 436, acc0.69, loss 0.5936, tol 0, val_acc0.67, val_loss0.5981\n","Epoch 437, acc0.67, loss 0.6034, tol 0, val_acc0.67, val_loss0.5975\n","Epoch 438, acc0.69, loss 0.6009, tol 0, val_acc0.68, val_loss0.5995\n","Epoch 439, acc0.68, loss 0.6037, tol 0, val_acc0.67, val_loss0.6004\n","Epoch 440, acc0.68, loss 0.6007, tol 0, val_acc0.68, val_loss0.5962 -- checkpoint saved\n","Epoch 441, acc0.68, loss 0.5972, tol 0, val_acc0.67, val_loss0.5963\n","Epoch 442, acc0.70, loss 0.5858, tol 0, val_acc0.67, val_loss0.6038\n","Epoch 443, acc0.69, loss 0.6030, tol 0, val_acc0.66, val_loss0.6017\n","Epoch 444, acc0.66, loss 0.6069, tol 0, val_acc0.68, val_loss0.5965\n","Epoch 445, acc0.69, loss 0.6038, tol 0, val_acc0.68, val_loss0.5929\n","Epoch 446, acc0.68, loss 0.6047, tol 0, val_acc0.68, val_loss0.5983\n","Epoch 447, acc0.66, loss 0.6199, tol 0, val_acc0.68, val_loss0.5966\n","Epoch 448, acc0.67, loss 0.6046, tol 0, val_acc0.68, val_loss0.5920 -- checkpoint saved\n","Epoch 449, acc0.69, loss 0.5960, tol 0, val_acc0.69, val_loss0.5918 -- checkpoint saved\n","Epoch 450, acc0.69, loss 0.6001, tol 0, val_acc0.68, val_loss0.5920\n","Epoch 451, acc0.70, loss 0.5904, tol 0, val_acc0.68, val_loss0.5910\n","Epoch 452, acc0.70, loss 0.5910, tol 0, val_acc0.68, val_loss0.5895\n","Epoch 453, acc0.69, loss 0.5931, tol 0, val_acc0.68, val_loss0.5898\n","Epoch 454, acc0.68, loss 0.6103, tol 0, val_acc0.68, val_loss0.5898\n","Epoch 455, acc0.70, loss 0.5879, tol 0, val_acc0.69, val_loss0.5880\n","Epoch 456, acc0.71, loss 0.5810, tol 0, val_acc0.68, val_loss0.5963\n","Epoch 457, acc0.68, loss 0.6022, tol 0, val_acc0.68, val_loss0.5982\n","Epoch 458, acc0.70, loss 0.5972, tol 0, val_acc0.69, val_loss0.5893\n","Epoch 459, acc0.68, loss 0.6032, tol 0, val_acc0.69, val_loss0.5893\n","Epoch 460, acc0.70, loss 0.5844, tol 0, val_acc0.69, val_loss0.5866 -- checkpoint saved\n","Epoch 461, acc0.71, loss 0.5900, tol 0, val_acc0.68, val_loss0.5869\n","Epoch 462, acc0.66, loss 0.6070, tol 0, val_acc0.68, val_loss0.6029\n","Epoch 463, acc0.69, loss 0.5943, tol 0, val_acc0.68, val_loss0.5857\n","Epoch 464, acc0.68, loss 0.5928, tol 0, val_acc0.69, val_loss0.5874\n","Epoch 465, acc0.71, loss 0.5860, tol 0, val_acc0.68, val_loss0.5851\n","Epoch 466, acc0.68, loss 0.5996, tol 0, val_acc0.69, val_loss0.5869\n","Epoch 467, acc0.70, loss 0.5785, tol 0, val_acc0.69, val_loss0.5825 -- checkpoint saved\n","Epoch 468, acc0.69, loss 0.5902, tol 0, val_acc0.69, val_loss0.5831\n","Epoch 469, acc0.70, loss 0.5859, tol 0, val_acc0.67, val_loss0.5923\n","Epoch 470, acc0.69, loss 0.5972, tol 0, val_acc0.67, val_loss0.5921\n","Epoch 471, acc0.72, loss 0.5718, tol 0, val_acc0.69, val_loss0.5815\n","Epoch 472, acc0.71, loss 0.5799, tol 0, val_acc0.69, val_loss0.5805\n","Epoch 473, acc0.71, loss 0.5866, tol 0, val_acc0.69, val_loss0.5814\n","Epoch 474, acc0.71, loss 0.5701, tol 0, val_acc0.70, val_loss0.5801 -- checkpoint saved\n","Epoch 475, acc0.71, loss 0.5736, tol 0, val_acc0.70, val_loss0.5793 -- checkpoint saved\n","Epoch 476, acc0.71, loss 0.5830, tol 0, val_acc0.70, val_loss0.5798\n","Epoch 477, acc0.69, loss 0.5844, tol 0, val_acc0.69, val_loss0.5824\n","Epoch 478, acc0.69, loss 0.5922, tol 0, val_acc0.70, val_loss0.5771\n","Epoch 479, acc0.70, loss 0.5950, tol 0, val_acc0.70, val_loss0.5762 -- checkpoint saved\n","Epoch 480, acc0.69, loss 0.5870, tol 0, val_acc0.70, val_loss0.5794\n","Epoch 481, acc0.71, loss 0.5779, tol 0, val_acc0.68, val_loss0.5822\n","Epoch 482, acc0.70, loss 0.5813, tol 0, val_acc0.69, val_loss0.5800\n","Epoch 483, acc0.70, loss 0.5784, tol 0, val_acc0.70, val_loss0.5747\n","Epoch 484, acc0.71, loss 0.5802, tol 0, val_acc0.71, val_loss0.5737 -- checkpoint saved\n","Epoch 485, acc0.73, loss 0.5597, tol 0, val_acc0.69, val_loss0.5841\n","Epoch 486, acc0.70, loss 0.5922, tol 0, val_acc0.68, val_loss0.5996\n","Epoch 487, acc0.68, loss 0.5880, tol 0, val_acc0.69, val_loss0.5881\n","Epoch 488, acc0.71, loss 0.5720, tol 0, val_acc0.70, val_loss0.5760\n","Epoch 489, acc0.70, loss 0.5817, tol 0, val_acc0.70, val_loss0.5713\n","Epoch 490, acc0.70, loss 0.5780, tol 0, val_acc0.70, val_loss0.5729\n","Epoch 491, acc0.70, loss 0.5769, tol 0, val_acc0.70, val_loss0.5731\n","Epoch 492, acc0.71, loss 0.5686, tol 0, val_acc0.71, val_loss0.5695\n","Epoch 493, acc0.73, loss 0.5644, tol 0, val_acc0.71, val_loss0.5705\n","Epoch 494, acc0.71, loss 0.5864, tol 0, val_acc0.70, val_loss0.5755\n","Epoch 495, acc0.70, loss 0.5820, tol 0, val_acc0.71, val_loss0.5692\n","Epoch 496, acc0.71, loss 0.5696, tol 0, val_acc0.71, val_loss0.5699\n","Epoch 497, acc0.69, loss 0.5872, tol 0, val_acc0.72, val_loss0.5681 -- checkpoint saved\n","Epoch 498, acc0.70, loss 0.5804, tol 0, val_acc0.72, val_loss0.5676 -- checkpoint saved\n","Epoch 499, acc0.71, loss 0.5731, tol 0, val_acc0.72, val_loss0.5668\n","Epoch 500, acc0.72, loss 0.5685, tol 0, val_acc0.71, val_loss0.5656\n","Epoch 501, acc0.73, loss 0.5637, tol 0, val_acc0.69, val_loss0.5729\n","Reach Max Epoch Number\n","+------------------------+------------+\n","|        Modules         | Parameters |\n","+------------------------+------------+\n","| layers.0.0.fc_Q.weight |     45     |\n","| layers.0.0.fc_K.weight |     45     |\n","| layers.0.0.fc_V.weight |     45     |\n","| layers.0.1.fc_Q.weight |     45     |\n","| layers.0.1.fc_K.weight |     45     |\n","| layers.0.1.fc_V.weight |     45     |\n","| layers.1.0.fc_Q.weight |    162     |\n","| layers.1.0.fc_K.weight |    162     |\n","| layers.1.0.fc_V.weight |    162     |\n","| layers.1.1.fc_Q.weight |    162     |\n","| layers.1.1.fc_K.weight |    162     |\n","| layers.1.1.fc_V.weight |    162     |\n","|    classify.weight     |     36     |\n","|     classify.bias      |     2      |\n","+------------------------+------------+\n","Total Trainable Params: 1280\n","Epoch 0, acc0.47, loss 0.7346, tol 0, val_acc0.49, val_loss0.7210 -- checkpoint saved\n","Epoch 1, acc0.50, loss 0.7123, tol 0, val_acc0.51, val_loss0.7137 -- checkpoint saved\n","Epoch 2, acc0.50, loss 0.7075, tol 0, val_acc0.52, val_loss0.7080 -- checkpoint saved\n","Epoch 3, acc0.49, loss 0.7119, tol 0, val_acc0.51, val_loss0.7020\n","Epoch 4, acc0.52, loss 0.6981, tol 0, val_acc0.54, val_loss0.6984 -- checkpoint saved\n","Epoch 5, acc0.53, loss 0.6947, tol 0, val_acc0.53, val_loss0.6960\n","Epoch 6, acc0.52, loss 0.6992, tol 0, val_acc0.52, val_loss0.6977\n","Epoch 7, acc0.51, loss 0.6979, tol 0, val_acc0.53, val_loss0.6929\n","Epoch 8, acc0.54, loss 0.6919, tol 0, val_acc0.54, val_loss0.6896 -- checkpoint saved\n","Epoch 9, acc0.54, loss 0.6901, tol 0, val_acc0.54, val_loss0.6885 -- checkpoint saved\n","Epoch 10, acc0.55, loss 0.6895, tol 0, val_acc0.53, val_loss0.6904\n","Epoch 11, acc0.54, loss 0.6882, tol 0, val_acc0.55, val_loss0.6865 -- checkpoint saved\n","Epoch 12, acc0.54, loss 0.6903, tol 0, val_acc0.54, val_loss0.6862\n","Epoch 13, acc0.56, loss 0.6846, tol 0, val_acc0.57, val_loss0.6836 -- checkpoint saved\n","Epoch 14, acc0.55, loss 0.6860, tol 0, val_acc0.56, val_loss0.6832\n","Epoch 15, acc0.56, loss 0.6842, tol 0, val_acc0.55, val_loss0.6817\n","Epoch 16, acc0.56, loss 0.6815, tol 0, val_acc0.57, val_loss0.6811\n","Epoch 17, acc0.56, loss 0.6807, tol 0, val_acc0.53, val_loss0.6853\n","Epoch 18, acc0.57, loss 0.6802, tol 0, val_acc0.55, val_loss0.6802\n","Epoch 19, acc0.57, loss 0.6789, tol 0, val_acc0.58, val_loss0.6788 -- checkpoint saved\n","Epoch 20, acc0.56, loss 0.6815, tol 0, val_acc0.56, val_loss0.6776\n","Epoch 21, acc0.58, loss 0.6741, tol 0, val_acc0.58, val_loss0.6770\n","Epoch 22, acc0.57, loss 0.6777, tol 0, val_acc0.58, val_loss0.6763 -- checkpoint saved\n","Epoch 23, acc0.56, loss 0.6777, tol 0, val_acc0.58, val_loss0.6758 -- checkpoint saved\n","Epoch 24, acc0.58, loss 0.6714, tol 0, val_acc0.57, val_loss0.6747\n","Epoch 25, acc0.58, loss 0.6732, tol 0, val_acc0.59, val_loss0.6753\n","Epoch 26, acc0.57, loss 0.6739, tol 0, val_acc0.58, val_loss0.6731\n","Epoch 27, acc0.57, loss 0.6735, tol 0, val_acc0.59, val_loss0.6715 -- checkpoint saved\n","Epoch 28, acc0.60, loss 0.6685, tol 0, val_acc0.58, val_loss0.6715\n","Epoch 29, acc0.57, loss 0.6746, tol 0, val_acc0.57, val_loss0.6756\n","Epoch 30, acc0.58, loss 0.6731, tol 0, val_acc0.60, val_loss0.6684 -- checkpoint saved\n","Epoch 31, acc0.58, loss 0.6706, tol 0, val_acc0.59, val_loss0.6679\n","Epoch 32, acc0.59, loss 0.6656, tol 0, val_acc0.57, val_loss0.6698\n","Epoch 33, acc0.57, loss 0.6729, tol 0, val_acc0.59, val_loss0.6719\n","Epoch 34, acc0.59, loss 0.6706, tol 0, val_acc0.61, val_loss0.6657 -- checkpoint saved\n","Epoch 35, acc0.60, loss 0.6622, tol 0, val_acc0.62, val_loss0.6635 -- checkpoint saved\n","Epoch 36, acc0.60, loss 0.6623, tol 0, val_acc0.59, val_loss0.6649\n","Epoch 37, acc0.61, loss 0.6657, tol 0, val_acc0.63, val_loss0.6622 -- checkpoint saved\n","Epoch 38, acc0.61, loss 0.6559, tol 0, val_acc0.58, val_loss0.6737\n","Epoch 39, acc0.59, loss 0.6719, tol 0, val_acc0.58, val_loss0.6676\n","Epoch 40, acc0.60, loss 0.6624, tol 0, val_acc0.61, val_loss0.6591\n","Epoch 41, acc0.61, loss 0.6547, tol 0, val_acc0.62, val_loss0.6603\n","Epoch 42, acc0.60, loss 0.6592, tol 0, val_acc0.61, val_loss0.6595\n","Epoch 43, acc0.61, loss 0.6536, tol 0, val_acc0.62, val_loss0.6554\n","Epoch 44, acc0.61, loss 0.6544, tol 0, val_acc0.60, val_loss0.6553\n","Epoch 45, acc0.62, loss 0.6512, tol 0, val_acc0.63, val_loss0.6531 -- checkpoint saved\n","Epoch 46, acc0.62, loss 0.6499, tol 0, val_acc0.63, val_loss0.6526\n","Epoch 47, acc0.61, loss 0.6484, tol 0, val_acc0.62, val_loss0.6548\n","Epoch 48, acc0.60, loss 0.6567, tol 0, val_acc0.63, val_loss0.6517\n","Epoch 49, acc0.62, loss 0.6452, tol 0, val_acc0.62, val_loss0.6542\n","Epoch 50, acc0.60, loss 0.6509, tol 0, val_acc0.63, val_loss0.6531\n","Epoch 51, acc0.64, loss 0.6460, tol 0, val_acc0.65, val_loss0.6477 -- checkpoint saved\n","Epoch 52, acc0.61, loss 0.6524, tol 0, val_acc0.63, val_loss0.6554\n","Epoch 53, acc0.61, loss 0.6565, tol 0, val_acc0.61, val_loss0.6625\n","Epoch 54, acc0.62, loss 0.6538, tol 0, val_acc0.61, val_loss0.6543\n","Epoch 55, acc0.64, loss 0.6311, tol 0, val_acc0.63, val_loss0.6507\n","Epoch 56, acc0.62, loss 0.6500, tol 0, val_acc0.64, val_loss0.6453\n","Epoch 57, acc0.62, loss 0.6451, tol 0, val_acc0.63, val_loss0.6465\n","Epoch 58, acc0.62, loss 0.6399, tol 0, val_acc0.63, val_loss0.6537\n","Epoch 59, acc0.64, loss 0.6392, tol 0, val_acc0.63, val_loss0.6428\n","Epoch 60, acc0.61, loss 0.6476, tol 0, val_acc0.61, val_loss0.6557\n","Epoch 61, acc0.61, loss 0.6504, tol 0, val_acc0.64, val_loss0.6420\n","Epoch 62, acc0.62, loss 0.6415, tol 0, val_acc0.63, val_loss0.6444\n","Epoch 63, acc0.65, loss 0.6319, tol 0, val_acc0.63, val_loss0.6423\n","Epoch 64, acc0.62, loss 0.6512, tol 0, val_acc0.65, val_loss0.6403 -- checkpoint saved\n","Epoch 65, acc0.64, loss 0.6398, tol 0, val_acc0.64, val_loss0.6401\n","Epoch 66, acc0.65, loss 0.6394, tol 0, val_acc0.64, val_loss0.6411\n","Epoch 67, acc0.65, loss 0.6364, tol 0, val_acc0.63, val_loss0.6413\n","Epoch 68, acc0.62, loss 0.6376, tol 0, val_acc0.64, val_loss0.6400\n","Epoch 69, acc0.62, loss 0.6502, tol 0, val_acc0.65, val_loss0.6409\n","Epoch 70, acc0.67, loss 0.6172, tol 0, val_acc0.64, val_loss0.6397\n","Epoch 71, acc0.60, loss 0.6469, tol 0, val_acc0.62, val_loss0.6416\n","Epoch 72, acc0.63, loss 0.6377, tol 0, val_acc0.63, val_loss0.6421\n","Epoch 73, acc0.63, loss 0.6404, tol 0, val_acc0.65, val_loss0.6399\n","Epoch 74, acc0.63, loss 0.6400, tol 0, val_acc0.62, val_loss0.6523\n","Epoch 75, acc0.65, loss 0.6324, tol 0, val_acc0.64, val_loss0.6414\n","Epoch 76, acc0.65, loss 0.6355, tol 0, val_acc0.65, val_loss0.6347 -- checkpoint saved\n","Epoch 77, acc0.66, loss 0.6296, tol 0, val_acc0.65, val_loss0.6341 -- checkpoint saved\n","Epoch 78, acc0.66, loss 0.6197, tol 0, val_acc0.65, val_loss0.6342\n","Epoch 79, acc0.67, loss 0.6194, tol 0, val_acc0.65, val_loss0.6329 -- checkpoint saved\n","Epoch 80, acc0.66, loss 0.6187, tol 0, val_acc0.65, val_loss0.6358\n","Epoch 81, acc0.66, loss 0.6129, tol 0, val_acc0.65, val_loss0.6345\n","Epoch 82, acc0.66, loss 0.6196, tol 0, val_acc0.65, val_loss0.6320 -- checkpoint saved\n","Epoch 83, acc0.64, loss 0.6315, tol 0, val_acc0.65, val_loss0.6311 -- checkpoint saved\n","Epoch 84, acc0.65, loss 0.6197, tol 0, val_acc0.65, val_loss0.6316\n","Epoch 85, acc0.66, loss 0.6183, tol 0, val_acc0.66, val_loss0.6298 -- checkpoint saved\n","Epoch 86, acc0.65, loss 0.6226, tol 0, val_acc0.65, val_loss0.6281\n","Epoch 87, acc0.64, loss 0.6249, tol 0, val_acc0.65, val_loss0.6305\n","Epoch 88, acc0.65, loss 0.6246, tol 0, val_acc0.66, val_loss0.6269 -- checkpoint saved\n","Epoch 89, acc0.67, loss 0.6168, tol 0, val_acc0.65, val_loss0.6302\n","Epoch 90, acc0.67, loss 0.6154, tol 0, val_acc0.66, val_loss0.6263\n","Epoch 91, acc0.68, loss 0.6168, tol 0, val_acc0.65, val_loss0.6247\n","Epoch 92, acc0.66, loss 0.6174, tol 0, val_acc0.66, val_loss0.6235 -- checkpoint saved\n","Epoch 93, acc0.65, loss 0.6197, tol 0, val_acc0.66, val_loss0.6290\n","Epoch 94, acc0.69, loss 0.6068, tol 0, val_acc0.65, val_loss0.6318\n","Epoch 95, acc0.67, loss 0.6114, tol 0, val_acc0.66, val_loss0.6230 -- checkpoint saved\n","Epoch 96, acc0.68, loss 0.5969, tol 0, val_acc0.66, val_loss0.6201\n","Epoch 97, acc0.69, loss 0.6091, tol 0, val_acc0.67, val_loss0.6196 -- checkpoint saved\n","Epoch 98, acc0.68, loss 0.6102, tol 0, val_acc0.67, val_loss0.6225\n","Epoch 99, acc0.69, loss 0.5996, tol 0, val_acc0.67, val_loss0.6162 -- checkpoint saved\n","Epoch 100, acc0.67, loss 0.6050, tol 0, val_acc0.68, val_loss0.6163\n","Epoch 101, acc0.67, loss 0.6026, tol 0, val_acc0.67, val_loss0.6146\n","Epoch 102, acc0.68, loss 0.5999, tol 0, val_acc0.67, val_loss0.6141\n","Epoch 103, acc0.67, loss 0.6075, tol 0, val_acc0.67, val_loss0.6211\n","Epoch 104, acc0.67, loss 0.6113, tol 0, val_acc0.64, val_loss0.6457\n","Epoch 105, acc0.68, loss 0.6022, tol 0, val_acc0.67, val_loss0.6110\n","Epoch 106, acc0.67, loss 0.6147, tol 0, val_acc0.67, val_loss0.6197\n","Epoch 107, acc0.69, loss 0.5949, tol 0, val_acc0.69, val_loss0.6086 -- checkpoint saved\n","Epoch 108, acc0.69, loss 0.5956, tol 0, val_acc0.68, val_loss0.6122\n","Epoch 109, acc0.67, loss 0.5986, tol 0, val_acc0.68, val_loss0.6065\n","Epoch 110, acc0.69, loss 0.6013, tol 0, val_acc0.69, val_loss0.6115\n","Epoch 111, acc0.69, loss 0.5972, tol 0, val_acc0.67, val_loss0.6192\n","Epoch 112, acc0.68, loss 0.5958, tol 0, val_acc0.69, val_loss0.6072\n","Epoch 113, acc0.69, loss 0.5914, tol 0, val_acc0.67, val_loss0.6009\n","Epoch 114, acc0.70, loss 0.5811, tol 0, val_acc0.68, val_loss0.6004\n","Epoch 115, acc0.72, loss 0.5779, tol 0, val_acc0.68, val_loss0.5993\n","Epoch 116, acc0.70, loss 0.5883, tol 0, val_acc0.71, val_loss0.5971 -- checkpoint saved\n","Epoch 117, acc0.70, loss 0.5866, tol 0, val_acc0.68, val_loss0.6060\n","Epoch 118, acc0.69, loss 0.5921, tol 0, val_acc0.71, val_loss0.5924 -- checkpoint saved\n","Epoch 119, acc0.71, loss 0.5744, tol 0, val_acc0.70, val_loss0.5916\n","Epoch 120, acc0.69, loss 0.5899, tol 0, val_acc0.71, val_loss0.5953\n","Epoch 121, acc0.71, loss 0.5716, tol 0, val_acc0.71, val_loss0.5918\n","Epoch 122, acc0.70, loss 0.5846, tol 0, val_acc0.69, val_loss0.6081\n","Epoch 123, acc0.72, loss 0.5775, tol 0, val_acc0.72, val_loss0.5865 -- checkpoint saved\n","Epoch 124, acc0.72, loss 0.5664, tol 0, val_acc0.72, val_loss0.5870\n","Epoch 125, acc0.70, loss 0.5718, tol 0, val_acc0.72, val_loss0.5834\n","Epoch 126, acc0.72, loss 0.5686, tol 0, val_acc0.70, val_loss0.5834\n","Epoch 127, acc0.72, loss 0.5596, tol 0, val_acc0.71, val_loss0.5803\n","Epoch 128, acc0.71, loss 0.5744, tol 0, val_acc0.71, val_loss0.5911\n","Epoch 129, acc0.73, loss 0.5587, tol 0, val_acc0.71, val_loss0.5973\n","Epoch 130, acc0.71, loss 0.5711, tol 0, val_acc0.72, val_loss0.5772\n","Epoch 131, acc0.72, loss 0.5703, tol 0, val_acc0.72, val_loss0.5734\n","Epoch 132, acc0.74, loss 0.5459, tol 0, val_acc0.73, val_loss0.5720 -- checkpoint saved\n","Epoch 133, acc0.73, loss 0.5636, tol 0, val_acc0.72, val_loss0.5770\n","Epoch 134, acc0.74, loss 0.5501, tol 0, val_acc0.72, val_loss0.5713\n","Epoch 135, acc0.73, loss 0.5537, tol 0, val_acc0.73, val_loss0.5660 -- checkpoint saved\n","Epoch 136, acc0.72, loss 0.5544, tol 0, val_acc0.73, val_loss0.5651\n","Epoch 137, acc0.76, loss 0.5348, tol 0, val_acc0.73, val_loss0.5699\n","Epoch 138, acc0.76, loss 0.5409, tol 0, val_acc0.73, val_loss0.5683\n","Epoch 139, acc0.75, loss 0.5422, tol 0, val_acc0.72, val_loss0.5595\n","Epoch 140, acc0.76, loss 0.5357, tol 0, val_acc0.72, val_loss0.5586\n","Epoch 141, acc0.74, loss 0.5432, tol 0, val_acc0.71, val_loss0.5615\n","Epoch 142, acc0.72, loss 0.5650, tol 0, val_acc0.74, val_loss0.5619\n","Epoch 143, acc0.74, loss 0.5509, tol 0, val_acc0.74, val_loss0.5637\n","Epoch 144, acc0.74, loss 0.5409, tol 0, val_acc0.75, val_loss0.5546 -- checkpoint saved\n","Epoch 145, acc0.75, loss 0.5313, tol 0, val_acc0.74, val_loss0.5629\n","Epoch 146, acc0.76, loss 0.5261, tol 0, val_acc0.75, val_loss0.5543\n","Epoch 147, acc0.75, loss 0.5289, tol 0, val_acc0.74, val_loss0.5512\n","Epoch 148, acc0.78, loss 0.5052, tol 0, val_acc0.73, val_loss0.5455\n","Epoch 149, acc0.76, loss 0.5226, tol 0, val_acc0.74, val_loss0.5473\n","Epoch 150, acc0.73, loss 0.5467, tol 0, val_acc0.67, val_loss0.6210\n","Epoch 151, acc0.74, loss 0.5475, tol 0, val_acc0.75, val_loss0.5549\n","Epoch 152, acc0.76, loss 0.5161, tol 0, val_acc0.75, val_loss0.5344\n","Epoch 153, acc0.75, loss 0.5296, tol 0, val_acc0.75, val_loss0.5367\n","Epoch 154, acc0.76, loss 0.5147, tol 0, val_acc0.75, val_loss0.5448\n","Epoch 155, acc0.75, loss 0.5357, tol 0, val_acc0.73, val_loss0.5365\n","Epoch 156, acc0.76, loss 0.5312, tol 0, val_acc0.75, val_loss0.5338\n","Epoch 157, acc0.76, loss 0.5206, tol 0, val_acc0.75, val_loss0.5522\n","Epoch 158, acc0.76, loss 0.5269, tol 0, val_acc0.74, val_loss0.5304\n","Epoch 159, acc0.77, loss 0.5032, tol 0, val_acc0.71, val_loss0.5575\n","Epoch 160, acc0.76, loss 0.5220, tol 0, val_acc0.74, val_loss0.5236\n","Epoch 161, acc0.77, loss 0.5045, tol 0, val_acc0.75, val_loss0.5437\n","Epoch 162, acc0.76, loss 0.5237, tol 0, val_acc0.75, val_loss0.5339\n","Epoch 163, acc0.77, loss 0.5207, tol 0, val_acc0.76, val_loss0.5206 -- checkpoint saved\n","Epoch 164, acc0.78, loss 0.5160, tol 0, val_acc0.73, val_loss0.5375\n","Epoch 165, acc0.78, loss 0.5046, tol 0, val_acc0.75, val_loss0.5359\n","Epoch 166, acc0.77, loss 0.5113, tol 0, val_acc0.74, val_loss0.5190\n","Epoch 167, acc0.78, loss 0.4967, tol 0, val_acc0.73, val_loss0.5230\n","Epoch 168, acc0.78, loss 0.4861, tol 0, val_acc0.75, val_loss0.5139\n","Epoch 169, acc0.79, loss 0.4900, tol 0, val_acc0.74, val_loss0.5197\n","Epoch 170, acc0.77, loss 0.5088, tol 0, val_acc0.74, val_loss0.5163\n","Epoch 171, acc0.76, loss 0.5051, tol 0, val_acc0.76, val_loss0.5130 -- checkpoint saved\n","Epoch 172, acc0.80, loss 0.4791, tol 0, val_acc0.77, val_loss0.5343\n","Epoch 173, acc0.78, loss 0.4867, tol 0, val_acc0.77, val_loss0.5256\n","Epoch 174, acc0.79, loss 0.4845, tol 0, val_acc0.75, val_loss0.5053\n","Epoch 175, acc0.79, loss 0.4794, tol 0, val_acc0.73, val_loss0.5119\n","Epoch 176, acc0.77, loss 0.4926, tol 0, val_acc0.75, val_loss0.5008\n","Epoch 177, acc0.80, loss 0.4851, tol 0, val_acc0.76, val_loss0.5038\n","Epoch 178, acc0.79, loss 0.4861, tol 0, val_acc0.73, val_loss0.5080\n","Epoch 179, acc0.81, loss 0.4560, tol 0, val_acc0.76, val_loss0.5048\n","Epoch 180, acc0.81, loss 0.4549, tol 0, val_acc0.74, val_loss0.4977\n","Epoch 181, acc0.79, loss 0.4818, tol 0, val_acc0.74, val_loss0.4990\n","Epoch 182, acc0.81, loss 0.4707, tol 0, val_acc0.75, val_loss0.4916\n","Epoch 183, acc0.81, loss 0.4570, tol 0, val_acc0.74, val_loss0.4929\n","Epoch 184, acc0.80, loss 0.4736, tol 0, val_acc0.75, val_loss0.4897\n","Epoch 185, acc0.80, loss 0.4734, tol 0, val_acc0.75, val_loss0.4871\n","Epoch 186, acc0.80, loss 0.4675, tol 0, val_acc0.76, val_loss0.4890\n","Epoch 187, acc0.79, loss 0.4706, tol 0, val_acc0.76, val_loss0.5237\n","Epoch 188, acc0.80, loss 0.4702, tol 0, val_acc0.75, val_loss0.4882\n","Epoch 189, acc0.80, loss 0.4596, tol 0, val_acc0.76, val_loss0.4823\n","Epoch 190, acc0.81, loss 0.4577, tol 0, val_acc0.77, val_loss0.4890\n","Epoch 191, acc0.82, loss 0.4499, tol 0, val_acc0.76, val_loss0.4774\n","Epoch 192, acc0.83, loss 0.4245, tol 0, val_acc0.77, val_loss0.4859\n","Epoch 193, acc0.79, loss 0.4758, tol 0, val_acc0.75, val_loss0.4963\n","Epoch 194, acc0.81, loss 0.4540, tol 0, val_acc0.76, val_loss0.4721\n","Epoch 195, acc0.81, loss 0.4588, tol 0, val_acc0.77, val_loss0.4765\n","Epoch 196, acc0.81, loss 0.4489, tol 0, val_acc0.75, val_loss0.4775\n","Epoch 197, acc0.82, loss 0.4411, tol 0, val_acc0.75, val_loss0.4753\n","Epoch 198, acc0.83, loss 0.4204, tol 0, val_acc0.77, val_loss0.4725\n","Epoch 199, acc0.82, loss 0.4283, tol 0, val_acc0.76, val_loss0.4736\n","Epoch 200, acc0.81, loss 0.4487, tol 0, val_acc0.75, val_loss0.4748\n","Epoch 201, acc0.82, loss 0.4544, tol 0, val_acc0.75, val_loss0.4743\n","Epoch 202, acc0.83, loss 0.4278, tol 0, val_acc0.78, val_loss0.4880\n","Epoch 203, acc0.81, loss 0.4438, tol 0, val_acc0.77, val_loss0.4693\n","Epoch 204, acc0.81, loss 0.4342, tol 0, val_acc0.76, val_loss0.4620\n","Epoch 205, acc0.82, loss 0.4309, tol 0, val_acc0.79, val_loss0.4621\n","Epoch 206, acc0.81, loss 0.4362, tol 0, val_acc0.77, val_loss0.4573\n","Epoch 207, acc0.81, loss 0.4457, tol 0, val_acc0.77, val_loss0.4671\n","Epoch 208, acc0.80, loss 0.4624, tol 0, val_acc0.78, val_loss0.4696\n","Epoch 209, acc0.83, loss 0.4224, tol 0, val_acc0.78, val_loss0.4637\n","Epoch 210, acc0.84, loss 0.4196, tol 0, val_acc0.77, val_loss0.4549\n","Epoch 211, acc0.83, loss 0.4219, tol 0, val_acc0.77, val_loss0.4543\n","Epoch 212, acc0.81, loss 0.4337, tol 0, val_acc0.78, val_loss0.4579\n","Epoch 213, acc0.82, loss 0.4236, tol 0, val_acc0.78, val_loss0.4514\n","Epoch 214, acc0.83, loss 0.4092, tol 0, val_acc0.77, val_loss0.4545\n","Epoch 215, acc0.83, loss 0.4015, tol 0, val_acc0.78, val_loss0.4501\n","Epoch 216, acc0.81, loss 0.4318, tol 0, val_acc0.77, val_loss0.4554\n","Epoch 217, acc0.83, loss 0.4180, tol 0, val_acc0.77, val_loss0.4486\n","Epoch 218, acc0.85, loss 0.4103, tol 0, val_acc0.78, val_loss0.4558\n","Epoch 219, acc0.83, loss 0.4188, tol 0, val_acc0.77, val_loss0.4460\n","Epoch 220, acc0.81, loss 0.4346, tol 0, val_acc0.77, val_loss0.4488\n","Epoch 221, acc0.82, loss 0.4150, tol 0, val_acc0.76, val_loss0.4562\n","Epoch 222, acc0.82, loss 0.4220, tol 0, val_acc0.78, val_loss0.4602\n","Epoch 223, acc0.81, loss 0.4420, tol 0, val_acc0.77, val_loss0.4408\n","Epoch 224, acc0.84, loss 0.4011, tol 0, val_acc0.77, val_loss0.4467\n","Epoch 225, acc0.82, loss 0.4182, tol 0, val_acc0.78, val_loss0.4503\n","Epoch 226, acc0.83, loss 0.4105, tol 0, val_acc0.77, val_loss0.4403\n","Epoch 227, acc0.83, loss 0.4042, tol 0, val_acc0.78, val_loss0.4387\n","Epoch 228, acc0.83, loss 0.4104, tol 0, val_acc0.78, val_loss0.4541\n","Epoch 229, acc0.84, loss 0.4123, tol 0, val_acc0.79, val_loss0.4445\n","Epoch 230, acc0.83, loss 0.3972, tol 0, val_acc0.78, val_loss0.4378\n","Epoch 231, acc0.83, loss 0.4008, tol 0, val_acc0.78, val_loss0.4402\n","Epoch 232, acc0.84, loss 0.3906, tol 0, val_acc0.78, val_loss0.4333\n","Epoch 233, acc0.83, loss 0.4038, tol 0, val_acc0.79, val_loss0.4376\n","Epoch 234, acc0.84, loss 0.3924, tol 0, val_acc0.78, val_loss0.4349\n","Epoch 235, acc0.82, loss 0.4098, tol 0, val_acc0.79, val_loss0.4543\n","Epoch 236, acc0.84, loss 0.3922, tol 0, val_acc0.79, val_loss0.4419\n","Epoch 237, acc0.82, loss 0.4049, tol 0, val_acc0.78, val_loss0.4375\n","Epoch 238, acc0.83, loss 0.3994, tol 0, val_acc0.79, val_loss0.4419\n","Epoch 239, acc0.85, loss 0.3834, tol 0, val_acc0.78, val_loss0.4377\n","Epoch 240, acc0.83, loss 0.3890, tol 0, val_acc0.79, val_loss0.4282 -- checkpoint saved\n","Epoch 241, acc0.83, loss 0.3862, tol 0, val_acc0.79, val_loss0.4295\n","Epoch 242, acc0.86, loss 0.3715, tol 0, val_acc0.79, val_loss0.4312\n","Epoch 243, acc0.84, loss 0.3891, tol 0, val_acc0.79, val_loss0.4324\n","Epoch 244, acc0.85, loss 0.3754, tol 0, val_acc0.80, val_loss0.4307\n","Epoch 245, acc0.85, loss 0.3755, tol 0, val_acc0.78, val_loss0.4248\n","Epoch 246, acc0.83, loss 0.3916, tol 0, val_acc0.78, val_loss0.4243\n","Epoch 247, acc0.84, loss 0.3879, tol 0, val_acc0.79, val_loss0.4226\n","Epoch 248, acc0.82, loss 0.3959, tol 0, val_acc0.79, val_loss0.4665\n","Epoch 249, acc0.81, loss 0.4166, tol 0, val_acc0.80, val_loss0.4164 -- checkpoint saved\n","Epoch 250, acc0.83, loss 0.4067, tol 0, val_acc0.80, val_loss0.4223\n","Epoch 251, acc0.84, loss 0.3855, tol 0, val_acc0.79, val_loss0.4347\n","Epoch 252, acc0.85, loss 0.3726, tol 0, val_acc0.79, val_loss0.4236\n","Epoch 253, acc0.85, loss 0.3798, tol 0, val_acc0.80, val_loss0.4191\n","Epoch 254, acc0.85, loss 0.3841, tol 0, val_acc0.79, val_loss0.4255\n","Epoch 255, acc0.85, loss 0.3729, tol 0, val_acc0.80, val_loss0.4380\n","Epoch 256, acc0.83, loss 0.4073, tol 0, val_acc0.80, val_loss0.4099 -- checkpoint saved\n","Epoch 257, acc0.84, loss 0.3822, tol 0, val_acc0.79, val_loss0.4214\n","Epoch 258, acc0.84, loss 0.3860, tol 0, val_acc0.80, val_loss0.4188\n","Epoch 259, acc0.85, loss 0.3739, tol 0, val_acc0.80, val_loss0.4098\n","Epoch 260, acc0.84, loss 0.3832, tol 0, val_acc0.80, val_loss0.4110\n","Epoch 261, acc0.83, loss 0.3762, tol 0, val_acc0.80, val_loss0.4147\n","Epoch 262, acc0.83, loss 0.3911, tol 0, val_acc0.80, val_loss0.4101\n","Epoch 263, acc0.86, loss 0.3718, tol 0, val_acc0.80, val_loss0.4113\n","Epoch 264, acc0.86, loss 0.3488, tol 0, val_acc0.81, val_loss0.4226\n","Epoch 265, acc0.85, loss 0.3860, tol 0, val_acc0.80, val_loss0.4209\n","Epoch 266, acc0.84, loss 0.3855, tol 0, val_acc0.80, val_loss0.4075\n","Epoch 267, acc0.86, loss 0.3714, tol 0, val_acc0.82, val_loss0.4154\n","Epoch 268, acc0.84, loss 0.3804, tol 0, val_acc0.80, val_loss0.4102\n","Epoch 269, acc0.86, loss 0.3489, tol 0, val_acc0.80, val_loss0.4121\n","Epoch 270, acc0.87, loss 0.3570, tol 0, val_acc0.80, val_loss0.4072\n","Epoch 271, acc0.84, loss 0.3710, tol 0, val_acc0.80, val_loss0.4032\n","Epoch 272, acc0.85, loss 0.3667, tol 0, val_acc0.81, val_loss0.4150\n","Epoch 273, acc0.83, loss 0.3765, tol 0, val_acc0.80, val_loss0.4121\n","Epoch 274, acc0.84, loss 0.3622, tol 0, val_acc0.81, val_loss0.4080\n","Epoch 275, acc0.85, loss 0.3558, tol 0, val_acc0.81, val_loss0.4076\n","Epoch 276, acc0.85, loss 0.3563, tol 0, val_acc0.80, val_loss0.4038\n","Epoch 277, acc0.85, loss 0.3593, tol 0, val_acc0.81, val_loss0.3981\n","Epoch 278, acc0.85, loss 0.3585, tol 0, val_acc0.81, val_loss0.4022\n","Epoch 279, acc0.85, loss 0.3731, tol 0, val_acc0.81, val_loss0.4037\n","Epoch 280, acc0.87, loss 0.3399, tol 0, val_acc0.82, val_loss0.3998\n","Epoch 281, acc0.84, loss 0.3664, tol 0, val_acc0.81, val_loss0.4196\n","Epoch 282, acc0.85, loss 0.3612, tol 0, val_acc0.82, val_loss0.3967 -- checkpoint saved\n","Epoch 283, acc0.84, loss 0.3776, tol 0, val_acc0.81, val_loss0.3879\n","Epoch 284, acc0.85, loss 0.3444, tol 0, val_acc0.81, val_loss0.4019\n","Epoch 285, acc0.85, loss 0.3559, tol 0, val_acc0.81, val_loss0.3982\n","Epoch 286, acc0.85, loss 0.3473, tol 0, val_acc0.82, val_loss0.3983\n","Epoch 287, acc0.85, loss 0.3718, tol 0, val_acc0.82, val_loss0.3917\n","Epoch 288, acc0.85, loss 0.3558, tol 0, val_acc0.80, val_loss0.4331\n","Epoch 289, acc0.85, loss 0.3587, tol 0, val_acc0.82, val_loss0.3883\n","Epoch 290, acc0.85, loss 0.3530, tol 0, val_acc0.82, val_loss0.4032\n","Epoch 291, acc0.86, loss 0.3543, tol 0, val_acc0.81, val_loss0.3888\n","Epoch 292, acc0.87, loss 0.3389, tol 0, val_acc0.81, val_loss0.4119\n","Epoch 293, acc0.87, loss 0.3436, tol 0, val_acc0.82, val_loss0.3940\n","Epoch 294, acc0.85, loss 0.3683, tol 0, val_acc0.80, val_loss0.4088\n","Epoch 295, acc0.82, loss 0.4006, tol 0, val_acc0.82, val_loss0.3847\n","Epoch 296, acc0.84, loss 0.3709, tol 0, val_acc0.81, val_loss0.3999\n","Epoch 297, acc0.85, loss 0.3486, tol 0, val_acc0.81, val_loss0.4106\n","Epoch 298, acc0.86, loss 0.3546, tol 0, val_acc0.83, val_loss0.3987\n","Epoch 299, acc0.86, loss 0.3389, tol 0, val_acc0.81, val_loss0.3787\n","Epoch 300, acc0.86, loss 0.3287, tol 0, val_acc0.83, val_loss0.3885\n","Epoch 301, acc0.85, loss 0.3630, tol 0, val_acc0.82, val_loss0.3831\n","Epoch 302, acc0.88, loss 0.3403, tol 0, val_acc0.82, val_loss0.4139\n","Epoch 303, acc0.86, loss 0.3493, tol 0, val_acc0.83, val_loss0.3910\n","Epoch 304, acc0.85, loss 0.3790, tol 0, val_acc0.81, val_loss0.3992\n","Epoch 305, acc0.84, loss 0.3559, tol 0, val_acc0.82, val_loss0.3887\n","Epoch 306, acc0.87, loss 0.3236, tol 0, val_acc0.82, val_loss0.3878\n","Epoch 307, acc0.86, loss 0.3409, tol 0, val_acc0.82, val_loss0.3871\n","Epoch 308, acc0.87, loss 0.3342, tol 0, val_acc0.82, val_loss0.3883\n","Epoch 309, acc0.86, loss 0.3458, tol 0, val_acc0.82, val_loss0.3907\n","Epoch 310, acc0.87, loss 0.3247, tol 0, val_acc0.83, val_loss0.3825\n","Epoch 311, acc0.86, loss 0.3481, tol 0, val_acc0.81, val_loss0.3864\n","Epoch 312, acc0.86, loss 0.3385, tol 0, val_acc0.82, val_loss0.3792\n","Epoch 313, acc0.86, loss 0.3327, tol 0, val_acc0.82, val_loss0.4027\n","Epoch 314, acc0.84, loss 0.3340, tol 0, val_acc0.80, val_loss0.4634\n","Epoch 315, acc0.86, loss 0.3615, tol 0, val_acc0.82, val_loss0.3848\n","Epoch 316, acc0.87, loss 0.3510, tol 0, val_acc0.82, val_loss0.3847\n","Epoch 317, acc0.86, loss 0.3410, tol 0, val_acc0.82, val_loss0.3828\n","Epoch 318, acc0.86, loss 0.3368, tol 0, val_acc0.83, val_loss0.3934\n","Epoch 319, acc0.86, loss 0.3518, tol 0, val_acc0.83, val_loss0.3993\n","Epoch 320, acc0.87, loss 0.3283, tol 0, val_acc0.82, val_loss0.3812\n","Epoch 321, acc0.87, loss 0.3264, tol 0, val_acc0.82, val_loss0.3854\n","Epoch 322, acc0.87, loss 0.3466, tol 0, val_acc0.82, val_loss0.3831\n","Epoch 323, acc0.88, loss 0.3227, tol 0, val_acc0.82, val_loss0.3828\n","Epoch 324, acc0.88, loss 0.3205, tol 0, val_acc0.82, val_loss0.3990\n","Epoch 325, acc0.86, loss 0.3367, tol 0, val_acc0.84, val_loss0.3841\n","Epoch 326, acc0.87, loss 0.3178, tol 0, val_acc0.82, val_loss0.3816\n","Epoch 327, acc0.87, loss 0.3243, tol 0, val_acc0.83, val_loss0.3932\n","Epoch 328, acc0.87, loss 0.3083, tol 0, val_acc0.83, val_loss0.3976\n","Epoch 329, acc0.87, loss 0.3192, tol 0, val_acc0.82, val_loss0.3845\n","Epoch 330, acc0.88, loss 0.3237, tol 0, val_acc0.81, val_loss0.4028\n","Epoch 331, acc0.87, loss 0.3365, tol 0, val_acc0.84, val_loss0.3772\n","Epoch 332, acc0.88, loss 0.3204, tol 0, val_acc0.82, val_loss0.4138\n","Epoch 333, acc0.86, loss 0.3544, tol 0, val_acc0.83, val_loss0.3783\n","Epoch 334, acc0.88, loss 0.3191, tol 0, val_acc0.83, val_loss0.3791\n","Epoch 335, acc0.86, loss 0.3316, tol 0, val_acc0.83, val_loss0.3830\n","Epoch 336, acc0.88, loss 0.3236, tol 0, val_acc0.83, val_loss0.4218\n","Epoch 337, acc0.86, loss 0.3242, tol 0, val_acc0.83, val_loss0.3792\n","Epoch 338, acc0.88, loss 0.2971, tol 0, val_acc0.82, val_loss0.3819\n","Epoch 339, acc0.86, loss 0.3385, tol 0, val_acc0.83, val_loss0.3819\n","Epoch 340, acc0.87, loss 0.3220, tol 0, val_acc0.82, val_loss0.3793\n","Epoch 341, acc0.86, loss 0.3435, tol 0, val_acc0.82, val_loss0.3811\n","Epoch 342, acc0.87, loss 0.3088, tol 0, val_acc0.84, val_loss0.3864\n","Epoch 343, acc0.86, loss 0.3404, tol 0, val_acc0.82, val_loss0.4424\n","Epoch 344, acc0.85, loss 0.3312, tol 0, val_acc0.84, val_loss0.3929\n","Epoch 345, acc0.89, loss 0.2892, tol 0, val_acc0.85, val_loss0.3725 -- checkpoint saved\n","Epoch 346, acc0.87, loss 0.3045, tol 0, val_acc0.83, val_loss0.3807\n","Epoch 347, acc0.88, loss 0.3152, tol 0, val_acc0.83, val_loss0.3725\n","Epoch 348, acc0.87, loss 0.3182, tol 0, val_acc0.82, val_loss0.3671\n","Epoch 349, acc0.87, loss 0.3304, tol 0, val_acc0.83, val_loss0.3696\n","Epoch 350, acc0.88, loss 0.3059, tol 0, val_acc0.83, val_loss0.3683\n","Epoch 351, acc0.89, loss 0.3113, tol 0, val_acc0.84, val_loss0.3684\n","Epoch 352, acc0.88, loss 0.3219, tol 0, val_acc0.83, val_loss0.3721\n","Epoch 353, acc0.86, loss 0.3365, tol 0, val_acc0.83, val_loss0.3682\n","Epoch 354, acc0.86, loss 0.3326, tol 0, val_acc0.84, val_loss0.3644\n","Epoch 355, acc0.87, loss 0.3277, tol 0, val_acc0.83, val_loss0.3886\n","Epoch 356, acc0.88, loss 0.3141, tol 0, val_acc0.84, val_loss0.3621\n","Epoch 357, acc0.87, loss 0.3184, tol 0, val_acc0.83, val_loss0.3575\n","Epoch 358, acc0.88, loss 0.2953, tol 0, val_acc0.84, val_loss0.3777\n","Epoch 359, acc0.88, loss 0.2989, tol 0, val_acc0.85, val_loss0.3710\n","Epoch 360, acc0.88, loss 0.3128, tol 0, val_acc0.84, val_loss0.3727\n","Epoch 361, acc0.88, loss 0.3103, tol 0, val_acc0.84, val_loss0.3681\n","Epoch 362, acc0.88, loss 0.2964, tol 0, val_acc0.82, val_loss0.3583\n","Epoch 363, acc0.87, loss 0.3111, tol 0, val_acc0.83, val_loss0.3592\n","Epoch 364, acc0.89, loss 0.3036, tol 0, val_acc0.85, val_loss0.3588\n","Epoch 365, acc0.91, loss 0.2778, tol 0, val_acc0.85, val_loss0.3522 -- checkpoint saved\n","Epoch 366, acc0.87, loss 0.3186, tol 0, val_acc0.83, val_loss0.3549\n","Epoch 367, acc0.86, loss 0.3470, tol 0, val_acc0.84, val_loss0.3600\n","Epoch 368, acc0.88, loss 0.3091, tol 0, val_acc0.84, val_loss0.3485\n","Epoch 369, acc0.90, loss 0.2717, tol 0, val_acc0.85, val_loss0.3470 -- checkpoint saved\n","Epoch 370, acc0.88, loss 0.3184, tol 0, val_acc0.85, val_loss0.3515\n","Epoch 371, acc0.89, loss 0.2824, tol 0, val_acc0.83, val_loss0.3843\n","Epoch 372, acc0.88, loss 0.3052, tol 0, val_acc0.85, val_loss0.3535\n","Epoch 373, acc0.88, loss 0.3026, tol 0, val_acc0.84, val_loss0.3524\n","Epoch 374, acc0.89, loss 0.2868, tol 0, val_acc0.85, val_loss0.3450 -- checkpoint saved\n","Epoch 375, acc0.88, loss 0.2967, tol 0, val_acc0.85, val_loss0.3386 -- checkpoint saved\n","Epoch 376, acc0.89, loss 0.2937, tol 0, val_acc0.83, val_loss0.3624\n","Epoch 377, acc0.87, loss 0.3213, tol 0, val_acc0.85, val_loss0.3390\n","Epoch 378, acc0.89, loss 0.2970, tol 0, val_acc0.83, val_loss0.3718\n","Epoch 379, acc0.89, loss 0.2984, tol 0, val_acc0.85, val_loss0.3447\n","Epoch 380, acc0.90, loss 0.2953, tol 0, val_acc0.85, val_loss0.3457\n","Epoch 381, acc0.89, loss 0.2830, tol 0, val_acc0.85, val_loss0.3418\n","Epoch 382, acc0.88, loss 0.3089, tol 0, val_acc0.84, val_loss0.3499\n","Epoch 383, acc0.88, loss 0.3009, tol 0, val_acc0.85, val_loss0.3380\n","Epoch 384, acc0.89, loss 0.2893, tol 0, val_acc0.85, val_loss0.3404\n","Epoch 385, acc0.88, loss 0.2935, tol 0, val_acc0.84, val_loss0.3553\n","Epoch 386, acc0.88, loss 0.3060, tol 0, val_acc0.85, val_loss0.3651\n","Epoch 387, acc0.89, loss 0.2941, tol 0, val_acc0.84, val_loss0.3474\n","Epoch 388, acc0.88, loss 0.3032, tol 0, val_acc0.85, val_loss0.3392\n","Epoch 389, acc0.89, loss 0.2954, tol 0, val_acc0.84, val_loss0.3383\n","Epoch 390, acc0.90, loss 0.2869, tol 0, val_acc0.86, val_loss0.3410\n","Epoch 391, acc0.88, loss 0.2842, tol 0, val_acc0.85, val_loss0.3396\n","Epoch 392, acc0.91, loss 0.2537, tol 0, val_acc0.86, val_loss0.3406\n","Epoch 393, acc0.89, loss 0.2781, tol 0, val_acc0.85, val_loss0.3513\n","Epoch 394, acc0.89, loss 0.2882, tol 0, val_acc0.84, val_loss0.3365\n","Epoch 395, acc0.89, loss 0.2706, tol 0, val_acc0.86, val_loss0.3372\n","Epoch 396, acc0.90, loss 0.2700, tol 0, val_acc0.85, val_loss0.3514\n","Epoch 397, acc0.88, loss 0.3103, tol 0, val_acc0.85, val_loss0.3490\n","Epoch 398, acc0.88, loss 0.2998, tol 0, val_acc0.85, val_loss0.4029\n","Epoch 399, acc0.88, loss 0.3026, tol 0, val_acc0.85, val_loss0.3428\n","Epoch 400, acc0.89, loss 0.2942, tol 0, val_acc0.85, val_loss0.3342\n","Epoch 401, acc0.89, loss 0.2713, tol 0, val_acc0.85, val_loss0.3678\n","Epoch 402, acc0.91, loss 0.2569, tol 0, val_acc0.85, val_loss0.3406\n","Epoch 403, acc0.89, loss 0.2773, tol 0, val_acc0.85, val_loss0.3373\n","Epoch 404, acc0.89, loss 0.2787, tol 0, val_acc0.85, val_loss0.3411\n","Epoch 405, acc0.89, loss 0.2699, tol 0, val_acc0.85, val_loss0.3503\n","Epoch 406, acc0.90, loss 0.2579, tol 0, val_acc0.86, val_loss0.3441\n","Epoch 407, acc0.89, loss 0.2783, tol 0, val_acc0.84, val_loss0.3435\n","Epoch 408, acc0.90, loss 0.2827, tol 0, val_acc0.85, val_loss0.3373\n","Epoch 409, acc0.90, loss 0.2596, tol 0, val_acc0.84, val_loss0.3541\n","Epoch 410, acc0.89, loss 0.2590, tol 0, val_acc0.85, val_loss0.3593\n","Epoch 411, acc0.90, loss 0.2637, tol 0, val_acc0.85, val_loss0.3556\n","Epoch 412, acc0.90, loss 0.2736, tol 0, val_acc0.85, val_loss0.3367\n","Epoch 413, acc0.91, loss 0.2663, tol 0, val_acc0.86, val_loss0.3337\n","Epoch 414, acc0.90, loss 0.2761, tol 0, val_acc0.85, val_loss0.3334\n","Epoch 415, acc0.88, loss 0.2876, tol 0, val_acc0.85, val_loss0.3343\n","Epoch 416, acc0.90, loss 0.2563, tol 0, val_acc0.85, val_loss0.3538\n","Epoch 417, acc0.90, loss 0.2603, tol 0, val_acc0.85, val_loss0.3385\n","Epoch 418, acc0.89, loss 0.2728, tol 0, val_acc0.85, val_loss0.3356\n","Epoch 419, acc0.90, loss 0.2658, tol 0, val_acc0.85, val_loss0.3345\n","Epoch 420, acc0.88, loss 0.2992, tol 0, val_acc0.85, val_loss0.3419\n","Epoch 421, acc0.91, loss 0.2532, tol 0, val_acc0.84, val_loss0.3390\n","Epoch 422, acc0.90, loss 0.2451, tol 0, val_acc0.85, val_loss0.3471\n","Epoch 423, acc0.89, loss 0.2801, tol 0, val_acc0.85, val_loss0.3532\n","Epoch 424, acc0.90, loss 0.2727, tol 0, val_acc0.86, val_loss0.3326 -- checkpoint saved\n","Epoch 425, acc0.90, loss 0.2672, tol 0, val_acc0.85, val_loss0.3578\n","Epoch 426, acc0.89, loss 0.2646, tol 0, val_acc0.85, val_loss0.3534\n","Epoch 427, acc0.90, loss 0.2660, tol 0, val_acc0.85, val_loss0.3335\n","Epoch 428, acc0.89, loss 0.2773, tol 0, val_acc0.84, val_loss0.3464\n","Epoch 429, acc0.91, loss 0.2484, tol 0, val_acc0.85, val_loss0.3374\n","Epoch 430, acc0.89, loss 0.2707, tol 0, val_acc0.85, val_loss0.3449\n","Epoch 431, acc0.89, loss 0.2535, tol 0, val_acc0.86, val_loss0.3571\n","Epoch 432, acc0.90, loss 0.2694, tol 0, val_acc0.85, val_loss0.3361\n","Epoch 433, acc0.89, loss 0.2832, tol 0, val_acc0.85, val_loss0.3300\n","Epoch 434, acc0.91, loss 0.2684, tol 0, val_acc0.86, val_loss0.3323\n","Epoch 435, acc0.89, loss 0.2648, tol 0, val_acc0.84, val_loss0.3859\n","Epoch 436, acc0.89, loss 0.2833, tol 0, val_acc0.85, val_loss0.3336\n","Epoch 437, acc0.91, loss 0.2503, tol 0, val_acc0.86, val_loss0.3320\n","Epoch 438, acc0.90, loss 0.2590, tol 0, val_acc0.86, val_loss0.3310\n","Epoch 439, acc0.91, loss 0.2380, tol 0, val_acc0.84, val_loss0.3423\n","Epoch 440, acc0.91, loss 0.2536, tol 0, val_acc0.85, val_loss0.3388\n","Epoch 441, acc0.91, loss 0.2473, tol 0, val_acc0.86, val_loss0.3364\n","Epoch 442, acc0.91, loss 0.2382, tol 0, val_acc0.86, val_loss0.3334\n","Epoch 443, acc0.91, loss 0.2330, tol 0, val_acc0.85, val_loss0.3477\n","Epoch 444, acc0.91, loss 0.2459, tol 0, val_acc0.85, val_loss0.3392\n","Epoch 445, acc0.91, loss 0.2447, tol 0, val_acc0.85, val_loss0.3361\n","Epoch 446, acc0.91, loss 0.2455, tol 0, val_acc0.86, val_loss0.3327\n","Epoch 447, acc0.91, loss 0.2399, tol 0, val_acc0.85, val_loss0.3449\n","Epoch 448, acc0.89, loss 0.2775, tol 0, val_acc0.85, val_loss0.3384\n","Epoch 449, acc0.90, loss 0.2607, tol 0, val_acc0.85, val_loss0.3524\n","Epoch 450, acc0.90, loss 0.2623, tol 0, val_acc0.85, val_loss0.3343\n","Epoch 451, acc0.90, loss 0.2579, tol 0, val_acc0.86, val_loss0.3360\n","Epoch 452, acc0.90, loss 0.2646, tol 0, val_acc0.86, val_loss0.3472\n","Epoch 453, acc0.91, loss 0.2399, tol 0, val_acc0.85, val_loss0.3420\n","Epoch 454, acc0.90, loss 0.2555, tol 0, val_acc0.86, val_loss0.3362\n","Epoch 455, acc0.91, loss 0.2335, tol 0, val_acc0.86, val_loss0.3405\n","Epoch 456, acc0.91, loss 0.2533, tol 0, val_acc0.86, val_loss0.3283\n","Epoch 457, acc0.90, loss 0.2561, tol 0, val_acc0.86, val_loss0.3256\n","Epoch 458, acc0.90, loss 0.2533, tol 0, val_acc0.86, val_loss0.3285\n","Epoch 459, acc0.89, loss 0.2618, tol 0, val_acc0.85, val_loss0.3322\n","Epoch 460, acc0.92, loss 0.2150, tol 0, val_acc0.86, val_loss0.3592\n","Epoch 461, acc0.92, loss 0.2386, tol 0, val_acc0.86, val_loss0.3362\n","Epoch 462, acc0.92, loss 0.2292, tol 0, val_acc0.86, val_loss0.3350\n","Epoch 463, acc0.91, loss 0.2350, tol 0, val_acc0.86, val_loss0.3398\n","Epoch 464, acc0.92, loss 0.2221, tol 0, val_acc0.85, val_loss0.3703\n","Epoch 465, acc0.90, loss 0.2489, tol 0, val_acc0.86, val_loss0.3382\n","Epoch 466, acc0.92, loss 0.2292, tol 0, val_acc0.87, val_loss0.3290\n","Epoch 467, acc0.91, loss 0.2502, tol 0, val_acc0.86, val_loss0.3282\n","Epoch 468, acc0.88, loss 0.2761, tol 0, val_acc0.86, val_loss0.3447\n","Epoch 469, acc0.91, loss 0.2337, tol 0, val_acc0.87, val_loss0.3230 -- checkpoint saved\n","Epoch 470, acc0.92, loss 0.2367, tol 0, val_acc0.86, val_loss0.3242\n","Epoch 471, acc0.91, loss 0.2484, tol 0, val_acc0.86, val_loss0.3256\n","Epoch 472, acc0.91, loss 0.2480, tol 0, val_acc0.86, val_loss0.3590\n","Epoch 473, acc0.89, loss 0.2716, tol 0, val_acc0.86, val_loss0.3268\n","Epoch 474, acc0.90, loss 0.2410, tol 0, val_acc0.86, val_loss0.3266\n","Epoch 475, acc0.90, loss 0.2645, tol 0, val_acc0.86, val_loss0.3231\n","Epoch 476, acc0.90, loss 0.2474, tol 0, val_acc0.86, val_loss0.3673\n","Epoch 477, acc0.92, loss 0.2346, tol 0, val_acc0.86, val_loss0.3569\n","Epoch 478, acc0.91, loss 0.2328, tol 0, val_acc0.85, val_loss0.3671\n","Epoch 479, acc0.90, loss 0.2664, tol 0, val_acc0.87, val_loss0.3360\n","Epoch 480, acc0.90, loss 0.2544, tol 0, val_acc0.87, val_loss0.3250\n","Epoch 481, acc0.91, loss 0.2417, tol 0, val_acc0.87, val_loss0.3220\n","Epoch 482, acc0.91, loss 0.2324, tol 0, val_acc0.86, val_loss0.3598\n","Epoch 483, acc0.91, loss 0.2304, tol 0, val_acc0.87, val_loss0.3267\n","Epoch 484, acc0.91, loss 0.2300, tol 0, val_acc0.86, val_loss0.3218\n","Epoch 485, acc0.92, loss 0.2264, tol 0, val_acc0.87, val_loss0.3191\n","Epoch 486, acc0.91, loss 0.2344, tol 0, val_acc0.87, val_loss0.3222\n","Epoch 487, acc0.92, loss 0.2283, tol 0, val_acc0.87, val_loss0.3203\n","Epoch 488, acc0.91, loss 0.2414, tol 0, val_acc0.87, val_loss0.3331\n","Epoch 489, acc0.92, loss 0.2376, tol 0, val_acc0.88, val_loss0.3287\n","Epoch 490, acc0.91, loss 0.2351, tol 0, val_acc0.87, val_loss0.3206\n","Epoch 491, acc0.92, loss 0.2422, tol 0, val_acc0.86, val_loss0.3225\n","Epoch 492, acc0.91, loss 0.2406, tol 0, val_acc0.87, val_loss0.3222\n","Epoch 493, acc0.92, loss 0.2141, tol 0, val_acc0.88, val_loss0.3238\n","Epoch 494, acc0.91, loss 0.2265, tol 0, val_acc0.85, val_loss0.3757\n","Epoch 495, acc0.90, loss 0.2479, tol 0, val_acc0.86, val_loss0.3257\n","Epoch 496, acc0.93, loss 0.2192, tol 0, val_acc0.87, val_loss0.3286\n","Epoch 497, acc0.90, loss 0.2629, tol 0, val_acc0.86, val_loss0.3238\n","Epoch 498, acc0.90, loss 0.2461, tol 0, val_acc0.86, val_loss0.3540\n","Epoch 499, acc0.91, loss 0.2325, tol 0, val_acc0.86, val_loss0.3539\n","Epoch 500, acc0.92, loss 0.2096, tol 0, val_acc0.87, val_loss0.3242\n","Epoch 501, acc0.90, loss 0.2442, tol 1, val_acc0.88, val_loss0.3339\n","Reach Max Epoch Number\n","+------------------------+------------+\n","|        Modules         | Parameters |\n","+------------------------+------------+\n","| layers.0.0.fc_Q.weight |     45     |\n","| layers.0.0.fc_K.weight |     45     |\n","| layers.0.0.fc_V.weight |     45     |\n","| layers.0.1.fc_Q.weight |     45     |\n","| layers.0.1.fc_K.weight |     45     |\n","| layers.0.1.fc_V.weight |     45     |\n","| layers.1.0.fc_Q.weight |    162     |\n","| layers.1.0.fc_K.weight |    162     |\n","| layers.1.0.fc_V.weight |    162     |\n","| layers.1.1.fc_Q.weight |    162     |\n","| layers.1.1.fc_K.weight |    162     |\n","| layers.1.1.fc_V.weight |    162     |\n","|    classify.weight     |     36     |\n","|     classify.bias      |     2      |\n","+------------------------+------------+\n","Total Trainable Params: 1280\n","Epoch 0, acc0.50, loss 0.8361, tol 0, val_acc0.51, val_loss0.7743 -- checkpoint saved\n","Epoch 1, acc0.48, loss 0.7637, tol 0, val_acc0.49, val_loss0.7221\n","Epoch 2, acc0.49, loss 0.7247, tol 0, val_acc0.50, val_loss0.7125\n","Epoch 3, acc0.50, loss 0.7118, tol 0, val_acc0.50, val_loss0.7108\n","Epoch 4, acc0.51, loss 0.7057, tol 0, val_acc0.49, val_loss0.7062\n","Epoch 5, acc0.49, loss 0.7068, tol 0, val_acc0.48, val_loss0.7046\n","Epoch 6, acc0.47, loss 0.7059, tol 0, val_acc0.49, val_loss0.7034\n","Epoch 7, acc0.49, loss 0.7038, tol 0, val_acc0.49, val_loss0.7025\n","Epoch 8, acc0.48, loss 0.7069, tol 0, val_acc0.50, val_loss0.7046\n","Epoch 9, acc0.47, loss 0.7015, tol 0, val_acc0.50, val_loss0.7000\n","Epoch 10, acc0.50, loss 0.6968, tol 0, val_acc0.49, val_loss0.6986\n","Epoch 11, acc0.46, loss 0.7028, tol 0, val_acc0.47, val_loss0.6981\n","Epoch 12, acc0.50, loss 0.6975, tol 0, val_acc0.49, val_loss0.6975\n","Epoch 13, acc0.47, loss 0.6986, tol 0, val_acc0.49, val_loss0.6968\n","Epoch 14, acc0.47, loss 0.7007, tol 0, val_acc0.48, val_loss0.6980\n","Epoch 15, acc0.50, loss 0.7036, tol 0, val_acc0.50, val_loss0.7079\n","Epoch 16, acc0.50, loss 0.7073, tol 0, val_acc0.50, val_loss0.7096\n","Epoch 17, acc0.51, loss 0.7004, tol 0, val_acc0.50, val_loss0.6962\n","Epoch 18, acc0.50, loss 0.6991, tol 0, val_acc0.49, val_loss0.6980\n","Epoch 19, acc0.52, loss 0.6959, tol 0, val_acc0.50, val_loss0.6951\n","Epoch 20, acc0.48, loss 0.6982, tol 0, val_acc0.51, val_loss0.6944 -- checkpoint saved\n","Epoch 21, acc0.52, loss 0.6931, tol 0, val_acc0.49, val_loss0.6972\n","Epoch 22, acc0.50, loss 0.6965, tol 0, val_acc0.51, val_loss0.6960\n","Epoch 23, acc0.49, loss 0.6964, tol 0, val_acc0.50, val_loss0.6938\n","Epoch 24, acc0.50, loss 0.6949, tol 0, val_acc0.50, val_loss0.6957\n","Epoch 25, acc0.49, loss 0.6969, tol 0, val_acc0.51, val_loss0.6936 -- checkpoint saved\n","Epoch 26, acc0.50, loss 0.6954, tol 0, val_acc0.50, val_loss0.6977\n","Epoch 27, acc0.50, loss 0.6990, tol 0, val_acc0.51, val_loss0.6975\n","Epoch 28, acc0.50, loss 0.6970, tol 0, val_acc0.50, val_loss0.6973\n","Epoch 29, acc0.51, loss 0.6951, tol 0, val_acc0.52, val_loss0.6927 -- checkpoint saved\n","Epoch 30, acc0.52, loss 0.6934, tol 0, val_acc0.53, val_loss0.6915 -- checkpoint saved\n","Epoch 31, acc0.51, loss 0.6904, tol 0, val_acc0.52, val_loss0.6915\n","Epoch 32, acc0.50, loss 0.6947, tol 0, val_acc0.52, val_loss0.6907\n","Epoch 33, acc0.50, loss 0.6968, tol 0, val_acc0.54, val_loss0.6904 -- checkpoint saved\n","Epoch 34, acc0.53, loss 0.6890, tol 0, val_acc0.53, val_loss0.6912\n","Epoch 35, acc0.51, loss 0.6943, tol 0, val_acc0.52, val_loss0.6905\n","Epoch 36, acc0.52, loss 0.6929, tol 0, val_acc0.53, val_loss0.6911\n","Epoch 37, acc0.51, loss 0.6949, tol 0, val_acc0.52, val_loss0.6909\n","Epoch 38, acc0.51, loss 0.6941, tol 0, val_acc0.52, val_loss0.6904\n","Epoch 39, acc0.53, loss 0.6891, tol 0, val_acc0.55, val_loss0.6878 -- checkpoint saved\n","Epoch 40, acc0.53, loss 0.6874, tol 0, val_acc0.54, val_loss0.6882\n","Epoch 41, acc0.52, loss 0.6907, tol 0, val_acc0.54, val_loss0.6872\n","Epoch 42, acc0.56, loss 0.6873, tol 0, val_acc0.53, val_loss0.6868\n","Epoch 43, acc0.51, loss 0.6905, tol 0, val_acc0.50, val_loss0.6926\n","Epoch 44, acc0.52, loss 0.6905, tol 0, val_acc0.52, val_loss0.6890\n","Epoch 45, acc0.54, loss 0.6869, tol 0, val_acc0.54, val_loss0.6855\n","Epoch 46, acc0.55, loss 0.6840, tol 0, val_acc0.51, val_loss0.6896\n","Epoch 47, acc0.54, loss 0.6865, tol 0, val_acc0.50, val_loss0.6930\n","Epoch 48, acc0.55, loss 0.6844, tol 0, val_acc0.53, val_loss0.6852\n","Epoch 49, acc0.53, loss 0.6868, tol 0, val_acc0.57, val_loss0.6806 -- checkpoint saved\n","Epoch 50, acc0.57, loss 0.6811, tol 0, val_acc0.59, val_loss0.6800 -- checkpoint saved\n","Epoch 51, acc0.55, loss 0.6833, tol 0, val_acc0.57, val_loss0.6782\n","Epoch 52, acc0.57, loss 0.6811, tol 0, val_acc0.59, val_loss0.6785\n","Epoch 53, acc0.55, loss 0.6853, tol 0, val_acc0.55, val_loss0.6806\n","Epoch 54, acc0.55, loss 0.6808, tol 0, val_acc0.50, val_loss0.6931\n","Epoch 55, acc0.56, loss 0.6819, tol 0, val_acc0.58, val_loss0.6760\n","Epoch 56, acc0.62, loss 0.6700, tol 0, val_acc0.58, val_loss0.6747\n","Epoch 57, acc0.57, loss 0.6719, tol 0, val_acc0.60, val_loss0.6702 -- checkpoint saved\n","Epoch 58, acc0.59, loss 0.6747, tol 0, val_acc0.63, val_loss0.6665 -- checkpoint saved\n","Epoch 59, acc0.62, loss 0.6655, tol 0, val_acc0.62, val_loss0.6633\n","Epoch 60, acc0.62, loss 0.6634, tol 0, val_acc0.64, val_loss0.6605 -- checkpoint saved\n","Epoch 61, acc0.62, loss 0.6626, tol 0, val_acc0.62, val_loss0.6579\n","Epoch 62, acc0.66, loss 0.6512, tol 0, val_acc0.65, val_loss0.6516 -- checkpoint saved\n","Epoch 63, acc0.65, loss 0.6477, tol 0, val_acc0.66, val_loss0.6467 -- checkpoint saved\n","Epoch 64, acc0.65, loss 0.6481, tol 0, val_acc0.66, val_loss0.6400 -- checkpoint saved\n","Epoch 65, acc0.67, loss 0.6347, tol 0, val_acc0.67, val_loss0.6349 -- checkpoint saved\n","Epoch 66, acc0.63, loss 0.6433, tol 0, val_acc0.66, val_loss0.6240\n","Epoch 67, acc0.68, loss 0.6190, tol 0, val_acc0.69, val_loss0.6214 -- checkpoint saved\n","Epoch 68, acc0.66, loss 0.6302, tol 0, val_acc0.70, val_loss0.6124 -- checkpoint saved\n","Epoch 69, acc0.67, loss 0.6148, tol 0, val_acc0.69, val_loss0.6081\n","Epoch 70, acc0.70, loss 0.6000, tol 0, val_acc0.71, val_loss0.6041 -- checkpoint saved\n","Epoch 71, acc0.72, loss 0.5909, tol 0, val_acc0.67, val_loss0.6138\n","Epoch 72, acc0.71, loss 0.5911, tol 0, val_acc0.71, val_loss0.5960 -- checkpoint saved\n","Epoch 73, acc0.70, loss 0.5946, tol 0, val_acc0.69, val_loss0.5998\n","Epoch 74, acc0.71, loss 0.5837, tol 0, val_acc0.73, val_loss0.5829 -- checkpoint saved\n","Epoch 75, acc0.73, loss 0.5698, tol 0, val_acc0.73, val_loss0.5759\n","Epoch 76, acc0.73, loss 0.5709, tol 0, val_acc0.70, val_loss0.5795\n","Epoch 77, acc0.72, loss 0.5748, tol 0, val_acc0.74, val_loss0.5671 -- checkpoint saved\n","Epoch 78, acc0.74, loss 0.5465, tol 0, val_acc0.76, val_loss0.5627 -- checkpoint saved\n","Epoch 79, acc0.75, loss 0.5392, tol 0, val_acc0.75, val_loss0.5620\n","Epoch 80, acc0.75, loss 0.5388, tol 0, val_acc0.76, val_loss0.5501 -- checkpoint saved\n","Epoch 81, acc0.76, loss 0.5338, tol 0, val_acc0.75, val_loss0.5437\n","Epoch 82, acc0.76, loss 0.5285, tol 0, val_acc0.75, val_loss0.5403\n","Epoch 83, acc0.78, loss 0.5136, tol 0, val_acc0.75, val_loss0.5405\n","Epoch 84, acc0.78, loss 0.5191, tol 0, val_acc0.77, val_loss0.5341 -- checkpoint saved\n","Epoch 85, acc0.77, loss 0.5162, tol 0, val_acc0.77, val_loss0.5187 -- checkpoint saved\n","Epoch 86, acc0.79, loss 0.4932, tol 0, val_acc0.78, val_loss0.5182 -- checkpoint saved\n","Epoch 87, acc0.79, loss 0.4994, tol 0, val_acc0.78, val_loss0.5076 -- checkpoint saved\n","Epoch 88, acc0.80, loss 0.4877, tol 0, val_acc0.77, val_loss0.5017\n","Epoch 89, acc0.81, loss 0.4737, tol 0, val_acc0.79, val_loss0.4997 -- checkpoint saved\n","Epoch 90, acc0.80, loss 0.4804, tol 0, val_acc0.78, val_loss0.4920\n","Epoch 91, acc0.78, loss 0.4816, tol 0, val_acc0.80, val_loss0.4870 -- checkpoint saved\n","Epoch 92, acc0.80, loss 0.4524, tol 0, val_acc0.81, val_loss0.4766 -- checkpoint saved\n","Epoch 93, acc0.81, loss 0.4573, tol 0, val_acc0.79, val_loss0.4834\n","Epoch 94, acc0.81, loss 0.4545, tol 0, val_acc0.82, val_loss0.4706 -- checkpoint saved\n","Epoch 95, acc0.83, loss 0.4260, tol 0, val_acc0.79, val_loss0.4797\n","Epoch 96, acc0.83, loss 0.4383, tol 0, val_acc0.79, val_loss0.4613\n","Epoch 97, acc0.84, loss 0.4214, tol 0, val_acc0.81, val_loss0.4530\n","Epoch 98, acc0.82, loss 0.4319, tol 0, val_acc0.82, val_loss0.4500 -- checkpoint saved\n","Epoch 99, acc0.81, loss 0.4338, tol 0, val_acc0.80, val_loss0.4649\n","Epoch 100, acc0.81, loss 0.4431, tol 0, val_acc0.83, val_loss0.4440 -- checkpoint saved\n","Epoch 101, acc0.84, loss 0.4107, tol 0, val_acc0.81, val_loss0.4361\n","Epoch 102, acc0.83, loss 0.4041, tol 0, val_acc0.82, val_loss0.4337\n","Epoch 103, acc0.84, loss 0.4105, tol 0, val_acc0.82, val_loss0.4321\n","Epoch 104, acc0.84, loss 0.4053, tol 0, val_acc0.81, val_loss0.4447\n","Epoch 105, acc0.85, loss 0.3791, tol 0, val_acc0.81, val_loss0.4340\n","Epoch 106, acc0.83, loss 0.4048, tol 0, val_acc0.81, val_loss0.4282\n","Epoch 107, acc0.84, loss 0.3935, tol 0, val_acc0.83, val_loss0.4162\n","Epoch 108, acc0.84, loss 0.3848, tol 0, val_acc0.83, val_loss0.4184\n","Epoch 109, acc0.86, loss 0.3790, tol 0, val_acc0.82, val_loss0.4326\n","Epoch 110, acc0.84, loss 0.4145, tol 0, val_acc0.81, val_loss0.4361\n","Epoch 111, acc0.84, loss 0.3877, tol 0, val_acc0.81, val_loss0.4413\n","Epoch 112, acc0.85, loss 0.3921, tol 0, val_acc0.82, val_loss0.4118\n","Epoch 113, acc0.84, loss 0.3931, tol 0, val_acc0.84, val_loss0.4009 -- checkpoint saved\n","Epoch 114, acc0.84, loss 0.3830, tol 0, val_acc0.84, val_loss0.3992\n","Epoch 115, acc0.84, loss 0.3914, tol 0, val_acc0.85, val_loss0.4008\n","Epoch 116, acc0.85, loss 0.3606, tol 0, val_acc0.84, val_loss0.4020\n","Epoch 117, acc0.85, loss 0.3712, tol 0, val_acc0.85, val_loss0.4051\n","Epoch 118, acc0.85, loss 0.3797, tol 0, val_acc0.83, val_loss0.4020\n","Epoch 119, acc0.86, loss 0.3480, tol 0, val_acc0.83, val_loss0.4068\n","Epoch 120, acc0.85, loss 0.3714, tol 0, val_acc0.82, val_loss0.4274\n","Epoch 121, acc0.85, loss 0.3788, tol 0, val_acc0.84, val_loss0.3934\n","Epoch 122, acc0.87, loss 0.3471, tol 0, val_acc0.84, val_loss0.3888\n","Epoch 123, acc0.85, loss 0.3591, tol 0, val_acc0.84, val_loss0.3956\n","Epoch 124, acc0.85, loss 0.3610, tol 0, val_acc0.82, val_loss0.4271\n","Epoch 125, acc0.86, loss 0.3626, tol 0, val_acc0.85, val_loss0.3814\n","Epoch 126, acc0.86, loss 0.3553, tol 0, val_acc0.85, val_loss0.3849\n","Epoch 127, acc0.84, loss 0.3802, tol 0, val_acc0.86, val_loss0.3765 -- checkpoint saved\n","Epoch 128, acc0.85, loss 0.3597, tol 0, val_acc0.85, val_loss0.3811\n","Epoch 129, acc0.85, loss 0.3780, tol 0, val_acc0.85, val_loss0.3750\n","Epoch 130, acc0.87, loss 0.3516, tol 0, val_acc0.86, val_loss0.3702 -- checkpoint saved\n","Epoch 131, acc0.88, loss 0.3272, tol 0, val_acc0.85, val_loss0.3793\n","Epoch 132, acc0.86, loss 0.3323, tol 0, val_acc0.85, val_loss0.3694\n","Epoch 133, acc0.88, loss 0.3417, tol 0, val_acc0.86, val_loss0.3769\n","Epoch 134, acc0.86, loss 0.3521, tol 0, val_acc0.86, val_loss0.3777\n","Epoch 135, acc0.87, loss 0.3443, tol 0, val_acc0.86, val_loss0.3717\n","Epoch 136, acc0.86, loss 0.3560, tol 0, val_acc0.85, val_loss0.3748\n","Epoch 137, acc0.86, loss 0.3464, tol 0, val_acc0.84, val_loss0.3946\n","Epoch 138, acc0.87, loss 0.3494, tol 0, val_acc0.85, val_loss0.3728\n","Epoch 139, acc0.87, loss 0.3267, tol 0, val_acc0.84, val_loss0.3817\n","Epoch 140, acc0.87, loss 0.3335, tol 0, val_acc0.86, val_loss0.3697\n","Epoch 141, acc0.87, loss 0.3286, tol 0, val_acc0.85, val_loss0.3692\n","Epoch 142, acc0.87, loss 0.3251, tol 0, val_acc0.83, val_loss0.3902\n","Epoch 143, acc0.86, loss 0.3469, tol 0, val_acc0.83, val_loss0.3995\n","Epoch 144, acc0.87, loss 0.3356, tol 0, val_acc0.86, val_loss0.3604\n","Epoch 145, acc0.88, loss 0.3149, tol 0, val_acc0.86, val_loss0.3531\n","Epoch 146, acc0.86, loss 0.3684, tol 0, val_acc0.85, val_loss0.3607\n","Epoch 147, acc0.87, loss 0.3446, tol 0, val_acc0.85, val_loss0.3732\n","Epoch 148, acc0.88, loss 0.3092, tol 0, val_acc0.85, val_loss0.3830\n","Epoch 149, acc0.86, loss 0.3470, tol 0, val_acc0.87, val_loss0.3503 -- checkpoint saved\n","Epoch 150, acc0.87, loss 0.3386, tol 0, val_acc0.86, val_loss0.3535\n","Epoch 151, acc0.88, loss 0.3075, tol 0, val_acc0.85, val_loss0.3725\n","Epoch 152, acc0.87, loss 0.3214, tol 0, val_acc0.86, val_loss0.3514\n","Epoch 153, acc0.87, loss 0.3152, tol 0, val_acc0.86, val_loss0.3566\n","Epoch 154, acc0.88, loss 0.3250, tol 0, val_acc0.87, val_loss0.3462\n","Epoch 155, acc0.86, loss 0.3441, tol 0, val_acc0.87, val_loss0.3446\n","Epoch 156, acc0.88, loss 0.3222, tol 0, val_acc0.85, val_loss0.3559\n","Epoch 157, acc0.89, loss 0.3020, tol 0, val_acc0.85, val_loss0.3775\n","Epoch 158, acc0.88, loss 0.3267, tol 0, val_acc0.85, val_loss0.3592\n","Epoch 159, acc0.87, loss 0.3248, tol 0, val_acc0.86, val_loss0.3468\n","Epoch 160, acc0.87, loss 0.3135, tol 0, val_acc0.86, val_loss0.3448\n","Epoch 161, acc0.86, loss 0.3532, tol 0, val_acc0.87, val_loss0.3373\n","Epoch 162, acc0.88, loss 0.3074, tol 0, val_acc0.86, val_loss0.3382\n","Epoch 163, acc0.85, loss 0.3523, tol 0, val_acc0.87, val_loss0.3371\n","Epoch 164, acc0.88, loss 0.3014, tol 0, val_acc0.87, val_loss0.3347\n","Epoch 165, acc0.89, loss 0.2993, tol 0, val_acc0.87, val_loss0.3449\n","Epoch 166, acc0.87, loss 0.3209, tol 0, val_acc0.84, val_loss0.3721\n","Epoch 167, acc0.88, loss 0.3229, tol 0, val_acc0.86, val_loss0.3402\n","Epoch 168, acc0.89, loss 0.2864, tol 0, val_acc0.88, val_loss0.3348\n","Epoch 169, acc0.87, loss 0.3287, tol 0, val_acc0.87, val_loss0.3326\n","Epoch 170, acc0.89, loss 0.3048, tol 0, val_acc0.86, val_loss0.3483\n","Epoch 171, acc0.88, loss 0.2988, tol 0, val_acc0.87, val_loss0.3383\n","Epoch 172, acc0.87, loss 0.3273, tol 0, val_acc0.87, val_loss0.3307\n","Epoch 173, acc0.88, loss 0.3175, tol 0, val_acc0.88, val_loss0.3306 -- checkpoint saved\n","Epoch 174, acc0.87, loss 0.3277, tol 0, val_acc0.87, val_loss0.3314\n","Epoch 175, acc0.88, loss 0.3305, tol 0, val_acc0.88, val_loss0.3279 -- checkpoint saved\n","Epoch 176, acc0.89, loss 0.2884, tol 0, val_acc0.87, val_loss0.3236\n","Epoch 177, acc0.89, loss 0.2972, tol 0, val_acc0.87, val_loss0.3377\n","Epoch 178, acc0.88, loss 0.2908, tol 0, val_acc0.86, val_loss0.3454\n","Epoch 179, acc0.88, loss 0.3031, tol 0, val_acc0.87, val_loss0.3412\n","Epoch 180, acc0.87, loss 0.3106, tol 0, val_acc0.87, val_loss0.3355\n","Epoch 181, acc0.87, loss 0.3265, tol 0, val_acc0.86, val_loss0.3355\n","Epoch 182, acc0.89, loss 0.2846, tol 0, val_acc0.87, val_loss0.3260\n","Epoch 183, acc0.89, loss 0.2972, tol 0, val_acc0.86, val_loss0.3430\n","Epoch 184, acc0.88, loss 0.3107, tol 0, val_acc0.87, val_loss0.3353\n","Epoch 185, acc0.87, loss 0.3216, tol 0, val_acc0.87, val_loss0.3356\n","Epoch 186, acc0.88, loss 0.3006, tol 0, val_acc0.87, val_loss0.3360\n","Epoch 187, acc0.88, loss 0.3050, tol 0, val_acc0.87, val_loss0.3231\n","Epoch 188, acc0.88, loss 0.3158, tol 0, val_acc0.87, val_loss0.3305\n","Epoch 189, acc0.88, loss 0.2988, tol 0, val_acc0.88, val_loss0.3268\n","Epoch 190, acc0.89, loss 0.2980, tol 0, val_acc0.88, val_loss0.3222 -- checkpoint saved\n","Epoch 191, acc0.86, loss 0.3495, tol 0, val_acc0.87, val_loss0.3305\n","Epoch 192, acc0.89, loss 0.2904, tol 0, val_acc0.87, val_loss0.3300\n","Epoch 193, acc0.89, loss 0.2890, tol 0, val_acc0.88, val_loss0.3253\n","Epoch 194, acc0.89, loss 0.2974, tol 0, val_acc0.87, val_loss0.3208\n","Epoch 195, acc0.89, loss 0.2819, tol 0, val_acc0.88, val_loss0.3275\n","Epoch 196, acc0.87, loss 0.3035, tol 0, val_acc0.88, val_loss0.3288\n","Epoch 197, acc0.88, loss 0.3246, tol 0, val_acc0.87, val_loss0.3249\n","Epoch 198, acc0.88, loss 0.3122, tol 0, val_acc0.87, val_loss0.3374\n","Epoch 199, acc0.89, loss 0.3064, tol 0, val_acc0.87, val_loss0.3369\n","Epoch 200, acc0.88, loss 0.3013, tol 0, val_acc0.88, val_loss0.3294\n","Epoch 201, acc0.88, loss 0.2951, tol 0, val_acc0.87, val_loss0.3419\n","Epoch 202, acc0.89, loss 0.2879, tol 0, val_acc0.88, val_loss0.3257\n","Epoch 203, acc0.88, loss 0.3027, tol 0, val_acc0.87, val_loss0.3369\n","Epoch 204, acc0.88, loss 0.3035, tol 0, val_acc0.87, val_loss0.3344\n","Epoch 205, acc0.89, loss 0.2878, tol 0, val_acc0.87, val_loss0.3246\n","Epoch 206, acc0.89, loss 0.2851, tol 0, val_acc0.88, val_loss0.3253\n","Epoch 207, acc0.89, loss 0.2841, tol 0, val_acc0.88, val_loss0.3162\n","Epoch 208, acc0.87, loss 0.3014, tol 0, val_acc0.87, val_loss0.3268\n","Epoch 209, acc0.89, loss 0.2848, tol 0, val_acc0.88, val_loss0.3165\n","Epoch 210, acc0.88, loss 0.2956, tol 0, val_acc0.87, val_loss0.3271\n","Epoch 211, acc0.89, loss 0.2930, tol 0, val_acc0.87, val_loss0.3227\n","Epoch 212, acc0.89, loss 0.2835, tol 0, val_acc0.87, val_loss0.3157\n","Epoch 213, acc0.88, loss 0.3096, tol 0, val_acc0.87, val_loss0.3115\n","Epoch 214, acc0.89, loss 0.2893, tol 0, val_acc0.87, val_loss0.3309\n","Epoch 215, acc0.89, loss 0.3049, tol 0, val_acc0.87, val_loss0.3207\n","Epoch 216, acc0.90, loss 0.2865, tol 0, val_acc0.88, val_loss0.3138\n","Epoch 217, acc0.89, loss 0.2976, tol 0, val_acc0.88, val_loss0.3178\n","Epoch 218, acc0.88, loss 0.2967, tol 0, val_acc0.88, val_loss0.3160\n","Epoch 219, acc0.91, loss 0.2777, tol 0, val_acc0.88, val_loss0.3175\n","Epoch 220, acc0.89, loss 0.2850, tol 0, val_acc0.88, val_loss0.3153\n","Epoch 221, acc0.90, loss 0.2761, tol 0, val_acc0.87, val_loss0.3218\n","Epoch 222, acc0.91, loss 0.2599, tol 0, val_acc0.87, val_loss0.3266\n","Epoch 223, acc0.90, loss 0.2645, tol 0, val_acc0.88, val_loss0.3142\n","Epoch 224, acc0.89, loss 0.2805, tol 0, val_acc0.88, val_loss0.3101 -- checkpoint saved\n","Epoch 225, acc0.89, loss 0.3015, tol 0, val_acc0.88, val_loss0.3087 -- checkpoint saved\n","Epoch 226, acc0.88, loss 0.2954, tol 0, val_acc0.87, val_loss0.3143\n","Epoch 227, acc0.90, loss 0.2731, tol 0, val_acc0.87, val_loss0.3202\n","Epoch 228, acc0.89, loss 0.2904, tol 0, val_acc0.87, val_loss0.3141\n","Epoch 229, acc0.89, loss 0.2835, tol 0, val_acc0.88, val_loss0.3259\n","Epoch 230, acc0.87, loss 0.3100, tol 0, val_acc0.88, val_loss0.3039\n","Epoch 231, acc0.89, loss 0.2938, tol 0, val_acc0.88, val_loss0.3071\n","Epoch 232, acc0.89, loss 0.2899, tol 0, val_acc0.88, val_loss0.3033\n","Epoch 233, acc0.90, loss 0.2740, tol 0, val_acc0.88, val_loss0.3083\n","Epoch 234, acc0.88, loss 0.3072, tol 0, val_acc0.87, val_loss0.3174\n","Epoch 235, acc0.89, loss 0.2961, tol 0, val_acc0.87, val_loss0.3176\n","Epoch 236, acc0.90, loss 0.2772, tol 0, val_acc0.88, val_loss0.3090\n","Epoch 237, acc0.90, loss 0.2815, tol 0, val_acc0.87, val_loss0.3080\n","Epoch 238, acc0.88, loss 0.2910, tol 0, val_acc0.88, val_loss0.3142\n","Epoch 239, acc0.89, loss 0.2873, tol 0, val_acc0.88, val_loss0.3208\n","Epoch 240, acc0.90, loss 0.2729, tol 0, val_acc0.88, val_loss0.3143\n","Epoch 241, acc0.88, loss 0.2751, tol 0, val_acc0.88, val_loss0.3172\n","Epoch 242, acc0.90, loss 0.2605, tol 0, val_acc0.88, val_loss0.3173\n","Epoch 243, acc0.89, loss 0.2677, tol 0, val_acc0.88, val_loss0.3137\n","Epoch 244, acc0.89, loss 0.2788, tol 0, val_acc0.87, val_loss0.3154\n","Epoch 245, acc0.89, loss 0.2940, tol 0, val_acc0.88, val_loss0.3044\n","Epoch 246, acc0.90, loss 0.2670, tol 0, val_acc0.86, val_loss0.3328\n","Epoch 247, acc0.88, loss 0.3005, tol 0, val_acc0.88, val_loss0.3165\n","Epoch 248, acc0.89, loss 0.2633, tol 0, val_acc0.88, val_loss0.3068\n","Epoch 249, acc0.89, loss 0.2816, tol 0, val_acc0.88, val_loss0.3139\n","Epoch 250, acc0.90, loss 0.2557, tol 0, val_acc0.88, val_loss0.3133\n","Epoch 251, acc0.89, loss 0.2752, tol 0, val_acc0.88, val_loss0.3158\n","Epoch 252, acc0.90, loss 0.2623, tol 0, val_acc0.88, val_loss0.3112\n","Epoch 253, acc0.89, loss 0.2778, tol 0, val_acc0.89, val_loss0.3060\n","Epoch 254, acc0.90, loss 0.2558, tol 0, val_acc0.88, val_loss0.3131\n","Epoch 255, acc0.90, loss 0.2779, tol 0, val_acc0.88, val_loss0.3051\n","Epoch 256, acc0.91, loss 0.2599, tol 0, val_acc0.88, val_loss0.3059\n","Epoch 257, acc0.89, loss 0.2836, tol 0, val_acc0.88, val_loss0.3043\n","Epoch 258, acc0.89, loss 0.2782, tol 0, val_acc0.88, val_loss0.2991\n","Epoch 259, acc0.88, loss 0.2779, tol 0, val_acc0.89, val_loss0.3046\n","Epoch 260, acc0.89, loss 0.3020, tol 0, val_acc0.88, val_loss0.3078\n","Epoch 261, acc0.89, loss 0.2740, tol 0, val_acc0.89, val_loss0.2977 -- checkpoint saved\n","Epoch 262, acc0.89, loss 0.2691, tol 0, val_acc0.88, val_loss0.2999\n","Epoch 263, acc0.90, loss 0.2599, tol 0, val_acc0.88, val_loss0.3043\n","Epoch 264, acc0.89, loss 0.2789, tol 0, val_acc0.89, val_loss0.2990\n","Epoch 265, acc0.91, loss 0.2449, tol 0, val_acc0.88, val_loss0.2973\n","Epoch 266, acc0.91, loss 0.2410, tol 0, val_acc0.88, val_loss0.3040\n","Epoch 267, acc0.89, loss 0.2820, tol 0, val_acc0.89, val_loss0.3041\n","Epoch 268, acc0.90, loss 0.2653, tol 0, val_acc0.88, val_loss0.3026\n","Epoch 269, acc0.89, loss 0.2925, tol 0, val_acc0.88, val_loss0.3085\n","Epoch 270, acc0.89, loss 0.2828, tol 0, val_acc0.89, val_loss0.2951 -- checkpoint saved\n","Epoch 271, acc0.90, loss 0.2648, tol 0, val_acc0.89, val_loss0.3010\n","Epoch 272, acc0.90, loss 0.2605, tol 0, val_acc0.89, val_loss0.2976\n","Epoch 273, acc0.90, loss 0.2780, tol 0, val_acc0.88, val_loss0.3129\n","Epoch 274, acc0.90, loss 0.2675, tol 0, val_acc0.88, val_loss0.3027\n","Epoch 275, acc0.90, loss 0.2585, tol 0, val_acc0.88, val_loss0.3050\n","Epoch 276, acc0.89, loss 0.2703, tol 0, val_acc0.89, val_loss0.3040\n","Epoch 277, acc0.90, loss 0.2703, tol 0, val_acc0.88, val_loss0.3152\n","Epoch 278, acc0.90, loss 0.2576, tol 0, val_acc0.88, val_loss0.3044\n","Epoch 279, acc0.89, loss 0.2672, tol 0, val_acc0.89, val_loss0.3020\n","Epoch 280, acc0.90, loss 0.2607, tol 0, val_acc0.89, val_loss0.3049\n","Epoch 281, acc0.90, loss 0.2483, tol 0, val_acc0.88, val_loss0.2991\n","Epoch 282, acc0.88, loss 0.2878, tol 0, val_acc0.88, val_loss0.2988\n","Epoch 283, acc0.91, loss 0.2627, tol 0, val_acc0.88, val_loss0.2993\n","Epoch 284, acc0.91, loss 0.2288, tol 0, val_acc0.89, val_loss0.3087\n","Epoch 285, acc0.89, loss 0.2743, tol 0, val_acc0.88, val_loss0.2988\n","Epoch 286, acc0.91, loss 0.2390, tol 0, val_acc0.89, val_loss0.3046\n","Epoch 287, acc0.90, loss 0.2442, tol 0, val_acc0.89, val_loss0.3017\n","Epoch 288, acc0.90, loss 0.2800, tol 0, val_acc0.89, val_loss0.3021\n","Epoch 289, acc0.89, loss 0.2750, tol 0, val_acc0.89, val_loss0.2921\n","Epoch 290, acc0.90, loss 0.2627, tol 0, val_acc0.88, val_loss0.3070\n","Epoch 291, acc0.88, loss 0.2797, tol 0, val_acc0.89, val_loss0.2906\n","Epoch 292, acc0.90, loss 0.2699, tol 0, val_acc0.89, val_loss0.2938\n","Epoch 293, acc0.89, loss 0.2602, tol 0, val_acc0.88, val_loss0.3187\n","Epoch 294, acc0.89, loss 0.2751, tol 0, val_acc0.89, val_loss0.2946\n","Epoch 295, acc0.90, loss 0.2528, tol 0, val_acc0.89, val_loss0.2959\n","Epoch 296, acc0.89, loss 0.2707, tol 0, val_acc0.89, val_loss0.2980\n","Epoch 297, acc0.90, loss 0.2684, tol 0, val_acc0.89, val_loss0.2948\n","Epoch 298, acc0.90, loss 0.2609, tol 0, val_acc0.89, val_loss0.2973\n","Epoch 299, acc0.90, loss 0.2491, tol 0, val_acc0.89, val_loss0.2949\n","Epoch 300, acc0.89, loss 0.2592, tol 0, val_acc0.89, val_loss0.2970\n","Epoch 301, acc0.89, loss 0.2755, tol 0, val_acc0.89, val_loss0.2971\n","Epoch 302, acc0.90, loss 0.2621, tol 0, val_acc0.89, val_loss0.2956\n","Epoch 303, acc0.91, loss 0.2492, tol 0, val_acc0.88, val_loss0.3125\n","Epoch 304, acc0.89, loss 0.2542, tol 0, val_acc0.89, val_loss0.2987\n","Epoch 305, acc0.90, loss 0.2435, tol 0, val_acc0.89, val_loss0.2969\n","Epoch 306, acc0.91, loss 0.2545, tol 0, val_acc0.89, val_loss0.2990\n","Epoch 307, acc0.89, loss 0.2646, tol 0, val_acc0.89, val_loss0.3056\n","Epoch 308, acc0.89, loss 0.2714, tol 0, val_acc0.88, val_loss0.2943\n","Epoch 309, acc0.90, loss 0.2644, tol 0, val_acc0.89, val_loss0.2977\n","Epoch 310, acc0.90, loss 0.2548, tol 0, val_acc0.88, val_loss0.3139\n","Epoch 311, acc0.90, loss 0.2619, tol 0, val_acc0.89, val_loss0.2970\n","Epoch 312, acc0.89, loss 0.2748, tol 0, val_acc0.88, val_loss0.2949\n","Epoch 313, acc0.90, loss 0.2500, tol 0, val_acc0.88, val_loss0.3019\n","Epoch 314, acc0.89, loss 0.2778, tol 0, val_acc0.88, val_loss0.2984\n","Epoch 315, acc0.90, loss 0.2569, tol 0, val_acc0.88, val_loss0.2999\n","Epoch 316, acc0.90, loss 0.2583, tol 0, val_acc0.89, val_loss0.2980\n","Epoch 317, acc0.89, loss 0.2725, tol 0, val_acc0.89, val_loss0.2948\n","Epoch 318, acc0.89, loss 0.2682, tol 0, val_acc0.89, val_loss0.3011\n","Epoch 319, acc0.89, loss 0.2680, tol 0, val_acc0.90, val_loss0.2935\n","Epoch 320, acc0.91, loss 0.2336, tol 0, val_acc0.89, val_loss0.2953\n","Epoch 321, acc0.89, loss 0.2717, tol 0, val_acc0.90, val_loss0.2976\n","Epoch 322, acc0.91, loss 0.2522, tol 0, val_acc0.89, val_loss0.2976\n","Epoch 323, acc0.90, loss 0.2635, tol 0, val_acc0.89, val_loss0.2943\n","Epoch 324, acc0.90, loss 0.2714, tol 0, val_acc0.89, val_loss0.2985\n","Epoch 325, acc0.90, loss 0.2581, tol 0, val_acc0.89, val_loss0.2903\n","Epoch 326, acc0.90, loss 0.2605, tol 0, val_acc0.89, val_loss0.3049\n","Epoch 327, acc0.90, loss 0.2563, tol 0, val_acc0.89, val_loss0.3017\n","Epoch 328, acc0.92, loss 0.2385, tol 0, val_acc0.89, val_loss0.2966\n","Epoch 329, acc0.89, loss 0.2822, tol 0, val_acc0.90, val_loss0.2936\n","Epoch 330, acc0.91, loss 0.2483, tol 0, val_acc0.89, val_loss0.3011\n","Epoch 331, acc0.91, loss 0.2316, tol 0, val_acc0.89, val_loss0.3002\n","Epoch 332, acc0.89, loss 0.2605, tol 0, val_acc0.89, val_loss0.3072\n","Epoch 333, acc0.89, loss 0.2663, tol 0, val_acc0.89, val_loss0.2943\n","Epoch 334, acc0.90, loss 0.2340, tol 0, val_acc0.89, val_loss0.2952\n","Epoch 335, acc0.91, loss 0.2378, tol 0, val_acc0.89, val_loss0.3056\n","Epoch 336, acc0.90, loss 0.2486, tol 0, val_acc0.88, val_loss0.3128\n","Epoch 337, acc0.90, loss 0.2621, tol 0, val_acc0.89, val_loss0.2936\n","Epoch 338, acc0.90, loss 0.2692, tol 0, val_acc0.89, val_loss0.2930\n","Epoch 339, acc0.91, loss 0.2444, tol 0, val_acc0.89, val_loss0.2972\n","Epoch 340, acc0.90, loss 0.2512, tol 0, val_acc0.89, val_loss0.2910\n","Epoch 341, acc0.91, loss 0.2381, tol 0, val_acc0.89, val_loss0.2923\n","Epoch 342, acc0.91, loss 0.2611, tol 0, val_acc0.89, val_loss0.2935\n","Epoch 343, acc0.91, loss 0.2440, tol 0, val_acc0.89, val_loss0.2905\n","Epoch 344, acc0.89, loss 0.2766, tol 0, val_acc0.89, val_loss0.2902\n","Epoch 345, acc0.92, loss 0.2392, tol 0, val_acc0.89, val_loss0.2952\n","Epoch 346, acc0.91, loss 0.2400, tol 0, val_acc0.89, val_loss0.2965\n","Epoch 347, acc0.90, loss 0.2482, tol 0, val_acc0.89, val_loss0.3012\n","Epoch 348, acc0.90, loss 0.2406, tol 0, val_acc0.89, val_loss0.2915\n","Epoch 349, acc0.91, loss 0.2481, tol 0, val_acc0.89, val_loss0.2946\n","Epoch 350, acc0.90, loss 0.2495, tol 0, val_acc0.89, val_loss0.2976\n","Epoch 351, acc0.91, loss 0.2287, tol 0, val_acc0.88, val_loss0.3078\n","Epoch 352, acc0.91, loss 0.2567, tol 0, val_acc0.89, val_loss0.2905\n","Epoch 353, acc0.90, loss 0.2657, tol 0, val_acc0.89, val_loss0.2921\n","Epoch 354, acc0.91, loss 0.2427, tol 0, val_acc0.89, val_loss0.2907\n","Epoch 355, acc0.89, loss 0.2750, tol 0, val_acc0.89, val_loss0.2872\n","Epoch 356, acc0.91, loss 0.2309, tol 0, val_acc0.89, val_loss0.2915\n","Epoch 357, acc0.92, loss 0.2398, tol 0, val_acc0.88, val_loss0.2934\n","Epoch 358, acc0.90, loss 0.2417, tol 0, val_acc0.89, val_loss0.2931\n","Epoch 359, acc0.91, loss 0.2474, tol 0, val_acc0.89, val_loss0.2970\n","Epoch 360, acc0.90, loss 0.2511, tol 0, val_acc0.90, val_loss0.2972\n","Epoch 361, acc0.90, loss 0.2397, tol 0, val_acc0.89, val_loss0.2905\n","Epoch 362, acc0.92, loss 0.2276, tol 0, val_acc0.88, val_loss0.2973\n","Epoch 363, acc0.89, loss 0.2649, tol 0, val_acc0.88, val_loss0.3315\n","Epoch 364, acc0.91, loss 0.2422, tol 0, val_acc0.89, val_loss0.2999\n","Epoch 365, acc0.89, loss 0.2764, tol 0, val_acc0.89, val_loss0.2913\n","Epoch 366, acc0.91, loss 0.2412, tol 0, val_acc0.90, val_loss0.2964\n","Epoch 367, acc0.90, loss 0.2640, tol 0, val_acc0.89, val_loss0.2980\n","Epoch 368, acc0.90, loss 0.2352, tol 0, val_acc0.89, val_loss0.2941\n","Epoch 369, acc0.90, loss 0.2501, tol 0, val_acc0.89, val_loss0.2973\n","Epoch 370, acc0.89, loss 0.2512, tol 0, val_acc0.88, val_loss0.2976\n","Epoch 371, acc0.90, loss 0.2388, tol 0, val_acc0.89, val_loss0.2909\n","Epoch 372, acc0.91, loss 0.2539, tol 0, val_acc0.88, val_loss0.3032\n","Epoch 373, acc0.90, loss 0.2400, tol 0, val_acc0.89, val_loss0.2931\n","Epoch 374, acc0.92, loss 0.2375, tol 0, val_acc0.89, val_loss0.2932\n","Epoch 375, acc0.91, loss 0.2279, tol 0, val_acc0.89, val_loss0.2911\n","Epoch 376, acc0.90, loss 0.2698, tol 0, val_acc0.90, val_loss0.2882\n","Epoch 377, acc0.91, loss 0.2365, tol 0, val_acc0.89, val_loss0.2930\n","Epoch 378, acc0.90, loss 0.2541, tol 0, val_acc0.89, val_loss0.2910\n","Epoch 379, acc0.92, loss 0.2250, tol 0, val_acc0.89, val_loss0.3020\n","Epoch 380, acc0.89, loss 0.2623, tol 0, val_acc0.89, val_loss0.2971\n","Epoch 381, acc0.91, loss 0.2369, tol 0, val_acc0.89, val_loss0.2977\n","Epoch 382, acc0.92, loss 0.2326, tol 0, val_acc0.89, val_loss0.2985\n","Epoch 383, acc0.91, loss 0.2452, tol 0, val_acc0.90, val_loss0.2958\n","Epoch 384, acc0.91, loss 0.2337, tol 0, val_acc0.89, val_loss0.2926\n","Epoch 385, acc0.89, loss 0.2595, tol 0, val_acc0.88, val_loss0.2929\n","Epoch 386, acc0.91, loss 0.2280, tol 0, val_acc0.88, val_loss0.2968\n","Epoch 387, acc0.91, loss 0.2545, tol 0, val_acc0.89, val_loss0.2975\n","Epoch 388, acc0.91, loss 0.2353, tol 0, val_acc0.89, val_loss0.2999\n","Epoch 389, acc0.90, loss 0.2501, tol 0, val_acc0.88, val_loss0.2974\n","Epoch 390, acc0.91, loss 0.2230, tol 0, val_acc0.89, val_loss0.2913\n","Epoch 391, acc0.90, loss 0.2524, tol 0, val_acc0.89, val_loss0.2902\n","Epoch 392, acc0.91, loss 0.2562, tol 0, val_acc0.89, val_loss0.2932\n","Epoch 393, acc0.90, loss 0.2699, tol 0, val_acc0.89, val_loss0.2906\n","Epoch 394, acc0.91, loss 0.2460, tol 0, val_acc0.89, val_loss0.2904\n","Epoch 395, acc0.91, loss 0.2347, tol 0, val_acc0.89, val_loss0.2935\n","Epoch 396, acc0.91, loss 0.2516, tol 0, val_acc0.89, val_loss0.2945\n","Epoch 397, acc0.90, loss 0.2470, tol 0, val_acc0.89, val_loss0.2936\n","Epoch 398, acc0.91, loss 0.2455, tol 0, val_acc0.89, val_loss0.2930\n","Epoch 399, acc0.92, loss 0.2354, tol 0, val_acc0.89, val_loss0.2975\n","Epoch 400, acc0.92, loss 0.2206, tol 0, val_acc0.89, val_loss0.2909\n","Epoch 401, acc0.89, loss 0.2702, tol 0, val_acc0.89, val_loss0.2915\n","Epoch 402, acc0.92, loss 0.2193, tol 0, val_acc0.89, val_loss0.3009\n","Epoch 403, acc0.91, loss 0.2310, tol 0, val_acc0.90, val_loss0.2921\n","Epoch 404, acc0.91, loss 0.2455, tol 0, val_acc0.89, val_loss0.2920\n","Epoch 405, acc0.91, loss 0.2441, tol 0, val_acc0.89, val_loss0.2975\n","Epoch 406, acc0.90, loss 0.2684, tol 0, val_acc0.88, val_loss0.3102\n","Epoch 407, acc0.90, loss 0.2326, tol 0, val_acc0.89, val_loss0.2919\n","Epoch 408, acc0.91, loss 0.2193, tol 0, val_acc0.89, val_loss0.2911\n","Epoch 409, acc0.90, loss 0.2220, tol 0, val_acc0.89, val_loss0.3003\n","Epoch 410, acc0.92, loss 0.2208, tol 0, val_acc0.88, val_loss0.3218\n","Epoch 411, acc0.91, loss 0.2475, tol 0, val_acc0.89, val_loss0.3007\n","Epoch 412, acc0.91, loss 0.2287, tol 0, val_acc0.90, val_loss0.2868\n","Epoch 413, acc0.91, loss 0.2306, tol 0, val_acc0.89, val_loss0.2923\n","Epoch 414, acc0.91, loss 0.2210, tol 0, val_acc0.89, val_loss0.2927\n","Epoch 415, acc0.91, loss 0.2337, tol 0, val_acc0.88, val_loss0.2937\n","Epoch 416, acc0.92, loss 0.2367, tol 0, val_acc0.89, val_loss0.2972\n","Epoch 417, acc0.91, loss 0.2271, tol 0, val_acc0.89, val_loss0.2937\n","Epoch 418, acc0.90, loss 0.2694, tol 0, val_acc0.89, val_loss0.2952\n","Epoch 419, acc0.89, loss 0.2959, tol 0, val_acc0.89, val_loss0.2899\n","Epoch 420, acc0.90, loss 0.2489, tol 0, val_acc0.89, val_loss0.2901\n","Epoch 421, acc0.90, loss 0.2622, tol 0, val_acc0.88, val_loss0.2955\n","Epoch 422, acc0.92, loss 0.2276, tol 0, val_acc0.89, val_loss0.2976\n","Epoch 423, acc0.91, loss 0.2269, tol 0, val_acc0.89, val_loss0.2968\n","Epoch 424, acc0.91, loss 0.2334, tol 0, val_acc0.88, val_loss0.3046\n","Epoch 425, acc0.91, loss 0.2279, tol 0, val_acc0.88, val_loss0.2955\n","Epoch 426, acc0.90, loss 0.2423, tol 0, val_acc0.89, val_loss0.2876\n","Epoch 427, acc0.91, loss 0.2218, tol 0, val_acc0.89, val_loss0.2969\n","Epoch 428, acc0.91, loss 0.2469, tol 0, val_acc0.89, val_loss0.2909\n","Epoch 429, acc0.92, loss 0.2237, tol 0, val_acc0.88, val_loss0.3050\n","Epoch 430, acc0.91, loss 0.2330, tol 0, val_acc0.88, val_loss0.2903\n","Epoch 431, acc0.91, loss 0.2438, tol 0, val_acc0.89, val_loss0.2919\n","Epoch 432, acc0.90, loss 0.2643, tol 0, val_acc0.88, val_loss0.2884\n","Epoch 433, acc0.91, loss 0.2356, tol 0, val_acc0.88, val_loss0.2936\n","Epoch 434, acc0.92, loss 0.2001, tol 0, val_acc0.89, val_loss0.2980\n","Epoch 435, acc0.91, loss 0.2214, tol 0, val_acc0.89, val_loss0.2932\n","Epoch 436, acc0.92, loss 0.2287, tol 0, val_acc0.88, val_loss0.3110\n","Epoch 437, acc0.90, loss 0.2404, tol 0, val_acc0.89, val_loss0.2960\n","Epoch 438, acc0.90, loss 0.2383, tol 0, val_acc0.88, val_loss0.3253\n","Epoch 439, acc0.91, loss 0.2487, tol 0, val_acc0.88, val_loss0.3116\n","Epoch 440, acc0.89, loss 0.2586, tol 0, val_acc0.88, val_loss0.2971\n","Epoch 441, acc0.90, loss 0.2655, tol 0, val_acc0.89, val_loss0.2872\n","Epoch 442, acc0.91, loss 0.2281, tol 0, val_acc0.89, val_loss0.3108\n","Epoch 443, acc0.91, loss 0.2157, tol 0, val_acc0.89, val_loss0.3044\n","Epoch 444, acc0.92, loss 0.2281, tol 0, val_acc0.88, val_loss0.2919\n","Epoch 445, acc0.92, loss 0.2152, tol 0, val_acc0.88, val_loss0.3096\n","Epoch 446, acc0.91, loss 0.2339, tol 0, val_acc0.89, val_loss0.2954\n","Epoch 447, acc0.91, loss 0.2446, tol 0, val_acc0.89, val_loss0.2934\n","Epoch 448, acc0.92, loss 0.2094, tol 0, val_acc0.88, val_loss0.2934\n","Epoch 449, acc0.92, loss 0.2361, tol 0, val_acc0.89, val_loss0.2918\n","Epoch 450, acc0.90, loss 0.2476, tol 0, val_acc0.89, val_loss0.2861\n","Epoch 451, acc0.91, loss 0.2482, tol 0, val_acc0.89, val_loss0.2915\n","Epoch 452, acc0.90, loss 0.2377, tol 0, val_acc0.89, val_loss0.2896\n","Epoch 453, acc0.91, loss 0.2329, tol 0, val_acc0.89, val_loss0.2944\n","Epoch 454, acc0.92, loss 0.2152, tol 0, val_acc0.89, val_loss0.2950\n","Epoch 455, acc0.91, loss 0.2405, tol 0, val_acc0.89, val_loss0.3040\n","Epoch 456, acc0.91, loss 0.2367, tol 0, val_acc0.89, val_loss0.2944\n","Epoch 457, acc0.90, loss 0.2442, tol 0, val_acc0.88, val_loss0.3137\n","Epoch 458, acc0.90, loss 0.2454, tol 0, val_acc0.89, val_loss0.2998\n","Epoch 459, acc0.91, loss 0.2174, tol 0, val_acc0.89, val_loss0.2977\n","Epoch 460, acc0.92, loss 0.2336, tol 0, val_acc0.89, val_loss0.2983\n","Epoch 461, acc0.91, loss 0.2528, tol 0, val_acc0.88, val_loss0.3329\n","Epoch 462, acc0.91, loss 0.2479, tol 0, val_acc0.88, val_loss0.3177\n","Epoch 463, acc0.90, loss 0.2535, tol 0, val_acc0.89, val_loss0.2998\n","Epoch 464, acc0.90, loss 0.2513, tol 0, val_acc0.88, val_loss0.3019\n","Epoch 465, acc0.92, loss 0.2273, tol 0, val_acc0.88, val_loss0.3013\n","Epoch 466, acc0.92, loss 0.2278, tol 0, val_acc0.90, val_loss0.2957\n","Epoch 467, acc0.92, loss 0.2324, tol 0, val_acc0.90, val_loss0.2913\n","Epoch 468, acc0.91, loss 0.2297, tol 0, val_acc0.89, val_loss0.2943\n","Epoch 469, acc0.92, loss 0.2254, tol 0, val_acc0.88, val_loss0.2916\n","Epoch 470, acc0.92, loss 0.2165, tol 0, val_acc0.88, val_loss0.3080\n","Epoch 471, acc0.91, loss 0.2245, tol 0, val_acc0.88, val_loss0.3027\n","Epoch 472, acc0.92, loss 0.2205, tol 0, val_acc0.89, val_loss0.2993\n","Epoch 473, acc0.92, loss 0.2328, tol 0, val_acc0.88, val_loss0.2989\n","Epoch 474, acc0.91, loss 0.2425, tol 0, val_acc0.89, val_loss0.2971\n","Epoch 475, acc0.92, loss 0.2171, tol 0, val_acc0.89, val_loss0.3018\n","Epoch 476, acc0.92, loss 0.2279, tol 0, val_acc0.88, val_loss0.3003\n","Epoch 477, acc0.91, loss 0.2390, tol 0, val_acc0.88, val_loss0.2990\n","Epoch 478, acc0.92, loss 0.2054, tol 0, val_acc0.89, val_loss0.3070\n","Epoch 479, acc0.92, loss 0.2246, tol 0, val_acc0.88, val_loss0.2994\n","Epoch 480, acc0.91, loss 0.2270, tol 0, val_acc0.88, val_loss0.2983\n","Epoch 481, acc0.92, loss 0.2235, tol 0, val_acc0.88, val_loss0.3113\n","Epoch 482, acc0.92, loss 0.2083, tol 0, val_acc0.89, val_loss0.3022\n","Epoch 483, acc0.92, loss 0.2375, tol 0, val_acc0.88, val_loss0.2952\n","Epoch 484, acc0.92, loss 0.2328, tol 0, val_acc0.89, val_loss0.2981\n","Epoch 485, acc0.93, loss 0.2124, tol 0, val_acc0.89, val_loss0.3099\n","Epoch 486, acc0.89, loss 0.2798, tol 0, val_acc0.88, val_loss0.2857\n","Epoch 487, acc0.90, loss 0.2563, tol 0, val_acc0.88, val_loss0.3030\n","Epoch 488, acc0.89, loss 0.2504, tol 0, val_acc0.89, val_loss0.2934\n","Epoch 489, acc0.91, loss 0.2319, tol 0, val_acc0.89, val_loss0.2916\n","Epoch 490, acc0.91, loss 0.2293, tol 0, val_acc0.89, val_loss0.2969\n","Epoch 491, acc0.92, loss 0.2152, tol 0, val_acc0.89, val_loss0.2997\n","Epoch 492, acc0.89, loss 0.2576, tol 0, val_acc0.89, val_loss0.2916\n","Epoch 493, acc0.91, loss 0.2367, tol 0, val_acc0.89, val_loss0.2925\n","Epoch 494, acc0.93, loss 0.1990, tol 0, val_acc0.89, val_loss0.2909\n","Epoch 495, acc0.92, loss 0.2169, tol 0, val_acc0.89, val_loss0.2963\n","Epoch 496, acc0.92, loss 0.2133, tol 0, val_acc0.89, val_loss0.3030\n","Epoch 497, acc0.92, loss 0.2377, tol 0, val_acc0.89, val_loss0.2954\n","Epoch 498, acc0.92, loss 0.2365, tol 0, val_acc0.89, val_loss0.2928\n","Epoch 499, acc0.92, loss 0.2242, tol 0, val_acc0.89, val_loss0.2891\n","Epoch 500, acc0.92, loss 0.2292, tol 0, val_acc0.89, val_loss0.2952\n","Epoch 501, acc0.92, loss 0.2220, tol 1, val_acc0.88, val_loss0.3087\n","Reach Max Epoch Number\n","+-----------------+------------+\n","|     Modules     | Parameters |\n","+-----------------+------------+\n","|   conv1.weight  |    160     |\n","|    conv1.bias   |     32     |\n","|   conv2.weight  |    1024    |\n","|    conv2.bias   |     32     |\n","| classify.weight |     64     |\n","|  classify.bias  |     2      |\n","+-----------------+------------+\n","Total Trainable Params: 1314\n","Epoch 0, acc0.51, loss 0.7507, tol 0, val_acc0.50, val_loss0.7053 -- checkpoint saved\n","Epoch 1, acc0.49, loss 0.6982, tol 0, val_acc0.50, val_loss0.6949 -- checkpoint saved\n","Epoch 2, acc0.47, loss 0.6965, tol 0, val_acc0.50, val_loss0.6934\n","Epoch 3, acc0.52, loss 0.6927, tol 0, val_acc0.50, val_loss0.6937\n","Epoch 4, acc0.50, loss 0.6938, tol 0, val_acc0.50, val_loss0.6931 -- checkpoint saved\n","Epoch 5, acc0.51, loss 0.6931, tol 0, val_acc0.50, val_loss0.6923 -- checkpoint saved\n","Epoch 6, acc0.52, loss 0.6927, tol 0, val_acc0.51, val_loss0.6923 -- checkpoint saved\n","Epoch 7, acc0.49, loss 0.6933, tol 0, val_acc0.51, val_loss0.6919\n","Epoch 8, acc0.55, loss 0.6923, tol 0, val_acc0.57, val_loss0.6918 -- checkpoint saved\n","Epoch 9, acc0.52, loss 0.6921, tol 0, val_acc0.52, val_loss0.6915\n","Epoch 10, acc0.53, loss 0.6918, tol 0, val_acc0.56, val_loss0.6912\n","Epoch 11, acc0.55, loss 0.6914, tol 0, val_acc0.56, val_loss0.6909\n","Epoch 12, acc0.53, loss 0.6922, tol 0, val_acc0.51, val_loss0.6915\n","Epoch 13, acc0.49, loss 0.6927, tol 0, val_acc0.54, val_loss0.6907\n","Epoch 14, acc0.54, loss 0.6914, tol 0, val_acc0.51, val_loss0.6910\n","Epoch 15, acc0.51, loss 0.6910, tol 0, val_acc0.52, val_loss0.6908\n","Epoch 16, acc0.55, loss 0.6905, tol 0, val_acc0.55, val_loss0.6904\n","Epoch 17, acc0.55, loss 0.6899, tol 0, val_acc0.50, val_loss0.6940\n","Epoch 18, acc0.50, loss 0.6944, tol 0, val_acc0.55, val_loss0.6899\n","Epoch 19, acc0.51, loss 0.6917, tol 0, val_acc0.56, val_loss0.6900\n","Epoch 20, acc0.55, loss 0.6895, tol 0, val_acc0.54, val_loss0.6900\n","Epoch 21, acc0.55, loss 0.6903, tol 0, val_acc0.51, val_loss0.6911\n","Epoch 22, acc0.52, loss 0.6901, tol 0, val_acc0.52, val_loss0.6902\n","Epoch 23, acc0.55, loss 0.6886, tol 0, val_acc0.51, val_loss0.6919\n","Epoch 24, acc0.52, loss 0.6909, tol 0, val_acc0.50, val_loss0.6923\n","Epoch 25, acc0.51, loss 0.6921, tol 0, val_acc0.55, val_loss0.6895\n","Epoch 26, acc0.55, loss 0.6901, tol 0, val_acc0.53, val_loss0.6888\n","Epoch 27, acc0.54, loss 0.6898, tol 0, val_acc0.56, val_loss0.6889\n","Epoch 28, acc0.54, loss 0.6877, tol 0, val_acc0.53, val_loss0.6885\n","Epoch 29, acc0.54, loss 0.6897, tol 0, val_acc0.53, val_loss0.6883\n","Epoch 30, acc0.54, loss 0.6884, tol 0, val_acc0.54, val_loss0.6889\n","Epoch 31, acc0.54, loss 0.6888, tol 0, val_acc0.55, val_loss0.6880\n","Epoch 32, acc0.56, loss 0.6876, tol 0, val_acc0.54, val_loss0.6878\n","Epoch 33, acc0.54, loss 0.6881, tol 0, val_acc0.52, val_loss0.6930\n","Epoch 34, acc0.54, loss 0.6896, tol 0, val_acc0.54, val_loss0.6889\n","Epoch 35, acc0.53, loss 0.6901, tol 0, val_acc0.53, val_loss0.6877\n","Epoch 36, acc0.55, loss 0.6875, tol 0, val_acc0.55, val_loss0.6874\n","Epoch 37, acc0.54, loss 0.6894, tol 0, val_acc0.53, val_loss0.6872\n","Epoch 38, acc0.54, loss 0.6887, tol 0, val_acc0.54, val_loss0.6876\n","Epoch 39, acc0.53, loss 0.6917, tol 0, val_acc0.54, val_loss0.6879\n","Epoch 40, acc0.52, loss 0.6904, tol 0, val_acc0.54, val_loss0.6871\n","Epoch 41, acc0.54, loss 0.6876, tol 0, val_acc0.55, val_loss0.6868\n","Epoch 42, acc0.56, loss 0.6861, tol 0, val_acc0.55, val_loss0.6866\n","Epoch 43, acc0.53, loss 0.6881, tol 0, val_acc0.54, val_loss0.6866\n","Epoch 44, acc0.54, loss 0.6902, tol 0, val_acc0.55, val_loss0.6865\n","Epoch 45, acc0.57, loss 0.6846, tol 0, val_acc0.55, val_loss0.6873\n","Epoch 46, acc0.55, loss 0.6865, tol 0, val_acc0.56, val_loss0.6880\n","Epoch 47, acc0.55, loss 0.6873, tol 0, val_acc0.54, val_loss0.6859\n","Epoch 48, acc0.55, loss 0.6879, tol 0, val_acc0.55, val_loss0.6856\n","Epoch 49, acc0.56, loss 0.6879, tol 0, val_acc0.57, val_loss0.6880\n","Epoch 50, acc0.55, loss 0.6852, tol 0, val_acc0.53, val_loss0.6878\n","Epoch 51, acc0.54, loss 0.6880, tol 0, val_acc0.56, val_loss0.6870\n","Epoch 52, acc0.56, loss 0.6874, tol 0, val_acc0.56, val_loss0.6853\n","Epoch 53, acc0.54, loss 0.6876, tol 0, val_acc0.55, val_loss0.6853\n","Epoch 54, acc0.55, loss 0.6841, tol 0, val_acc0.55, val_loss0.6846\n","Epoch 55, acc0.55, loss 0.6857, tol 0, val_acc0.55, val_loss0.6850\n","Epoch 56, acc0.54, loss 0.6888, tol 0, val_acc0.54, val_loss0.6842\n","Epoch 57, acc0.54, loss 0.6867, tol 0, val_acc0.55, val_loss0.6842\n","Epoch 58, acc0.56, loss 0.6855, tol 0, val_acc0.56, val_loss0.6868\n","Epoch 59, acc0.56, loss 0.6833, tol 0, val_acc0.55, val_loss0.6836\n","Epoch 60, acc0.56, loss 0.6848, tol 0, val_acc0.55, val_loss0.6882\n","Epoch 61, acc0.56, loss 0.6842, tol 0, val_acc0.52, val_loss0.6883\n","Epoch 62, acc0.53, loss 0.6882, tol 0, val_acc0.56, val_loss0.6860\n","Epoch 63, acc0.53, loss 0.6871, tol 0, val_acc0.54, val_loss0.6828\n","Epoch 64, acc0.57, loss 0.6845, tol 0, val_acc0.56, val_loss0.6826\n","Epoch 65, acc0.56, loss 0.6877, tol 0, val_acc0.56, val_loss0.6825\n","Epoch 66, acc0.56, loss 0.6844, tol 0, val_acc0.56, val_loss0.6822\n","Epoch 67, acc0.55, loss 0.6852, tol 0, val_acc0.57, val_loss0.6829\n","Epoch 68, acc0.56, loss 0.6845, tol 0, val_acc0.58, val_loss0.6820 -- checkpoint saved\n","Epoch 69, acc0.56, loss 0.6847, tol 0, val_acc0.55, val_loss0.6850\n","Epoch 70, acc0.55, loss 0.6866, tol 0, val_acc0.58, val_loss0.6815 -- checkpoint saved\n","Epoch 71, acc0.58, loss 0.6838, tol 0, val_acc0.59, val_loss0.6820\n","Epoch 72, acc0.57, loss 0.6844, tol 0, val_acc0.58, val_loss0.6811\n","Epoch 73, acc0.56, loss 0.6868, tol 0, val_acc0.59, val_loss0.6811\n","Epoch 74, acc0.59, loss 0.6829, tol 0, val_acc0.58, val_loss0.6804\n","Epoch 75, acc0.56, loss 0.6837, tol 0, val_acc0.58, val_loss0.6803\n","Epoch 76, acc0.56, loss 0.6822, tol 0, val_acc0.57, val_loss0.6811\n","Epoch 77, acc0.57, loss 0.6826, tol 0, val_acc0.59, val_loss0.6809\n","Epoch 78, acc0.59, loss 0.6808, tol 0, val_acc0.59, val_loss0.6800\n","Epoch 79, acc0.61, loss 0.6773, tol 0, val_acc0.57, val_loss0.6790\n","Epoch 80, acc0.58, loss 0.6819, tol 0, val_acc0.58, val_loss0.6787\n","Epoch 81, acc0.57, loss 0.6820, tol 0, val_acc0.59, val_loss0.6783\n","Epoch 82, acc0.58, loss 0.6819, tol 0, val_acc0.55, val_loss0.6815\n","Epoch 83, acc0.57, loss 0.6813, tol 0, val_acc0.58, val_loss0.6794\n","Epoch 84, acc0.57, loss 0.6825, tol 0, val_acc0.60, val_loss0.6778 -- checkpoint saved\n","Epoch 85, acc0.58, loss 0.6798, tol 0, val_acc0.55, val_loss0.6812\n","Epoch 86, acc0.57, loss 0.6833, tol 0, val_acc0.59, val_loss0.6772\n","Epoch 87, acc0.61, loss 0.6763, tol 0, val_acc0.59, val_loss0.6769\n","Epoch 88, acc0.56, loss 0.6827, tol 0, val_acc0.59, val_loss0.6771\n","Epoch 89, acc0.56, loss 0.6812, tol 0, val_acc0.54, val_loss0.6817\n","Epoch 90, acc0.55, loss 0.6849, tol 0, val_acc0.59, val_loss0.6767\n","Epoch 91, acc0.60, loss 0.6757, tol 0, val_acc0.58, val_loss0.6794\n","Epoch 92, acc0.59, loss 0.6771, tol 0, val_acc0.57, val_loss0.6782\n","Epoch 93, acc0.58, loss 0.6735, tol 0, val_acc0.59, val_loss0.6752\n","Epoch 94, acc0.58, loss 0.6790, tol 0, val_acc0.60, val_loss0.6752\n","Epoch 95, acc0.59, loss 0.6774, tol 0, val_acc0.60, val_loss0.6750\n","Epoch 96, acc0.61, loss 0.6745, tol 0, val_acc0.61, val_loss0.6742 -- checkpoint saved\n","Epoch 97, acc0.58, loss 0.6773, tol 0, val_acc0.61, val_loss0.6743\n","Epoch 98, acc0.59, loss 0.6735, tol 0, val_acc0.55, val_loss0.6814\n","Epoch 99, acc0.57, loss 0.6803, tol 0, val_acc0.60, val_loss0.6734\n","Epoch 100, acc0.59, loss 0.6782, tol 0, val_acc0.61, val_loss0.6733\n","Epoch 101, acc0.58, loss 0.6787, tol 0, val_acc0.60, val_loss0.6732\n","Epoch 102, acc0.60, loss 0.6782, tol 0, val_acc0.60, val_loss0.6720\n","Epoch 103, acc0.57, loss 0.6816, tol 0, val_acc0.59, val_loss0.6760\n","Epoch 104, acc0.59, loss 0.6779, tol 0, val_acc0.60, val_loss0.6721\n","Epoch 105, acc0.59, loss 0.6757, tol 0, val_acc0.59, val_loss0.6737\n","Epoch 106, acc0.61, loss 0.6727, tol 0, val_acc0.61, val_loss0.6719 -- checkpoint saved\n","Epoch 107, acc0.61, loss 0.6717, tol 0, val_acc0.61, val_loss0.6717\n","Epoch 108, acc0.60, loss 0.6726, tol 0, val_acc0.60, val_loss0.6716\n","Epoch 109, acc0.58, loss 0.6744, tol 0, val_acc0.58, val_loss0.6740\n","Epoch 110, acc0.59, loss 0.6744, tol 0, val_acc0.59, val_loss0.6719\n","Epoch 111, acc0.55, loss 0.6794, tol 0, val_acc0.60, val_loss0.6699\n","Epoch 112, acc0.60, loss 0.6716, tol 0, val_acc0.61, val_loss0.6702\n","Epoch 113, acc0.61, loss 0.6673, tol 0, val_acc0.59, val_loss0.6721\n","Epoch 114, acc0.60, loss 0.6725, tol 0, val_acc0.60, val_loss0.6715\n","Epoch 115, acc0.62, loss 0.6661, tol 0, val_acc0.61, val_loss0.6684\n","Epoch 116, acc0.61, loss 0.6708, tol 0, val_acc0.59, val_loss0.6702\n","Epoch 117, acc0.61, loss 0.6687, tol 0, val_acc0.61, val_loss0.6682 -- checkpoint saved\n","Epoch 118, acc0.61, loss 0.6715, tol 0, val_acc0.62, val_loss0.6677 -- checkpoint saved\n","Epoch 119, acc0.60, loss 0.6699, tol 0, val_acc0.61, val_loss0.6669\n","Epoch 120, acc0.60, loss 0.6705, tol 0, val_acc0.61, val_loss0.6669\n","Epoch 121, acc0.60, loss 0.6682, tol 0, val_acc0.60, val_loss0.6684\n","Epoch 122, acc0.57, loss 0.6763, tol 0, val_acc0.61, val_loss0.6677\n","Epoch 123, acc0.59, loss 0.6676, tol 0, val_acc0.53, val_loss0.6826\n","Epoch 124, acc0.60, loss 0.6680, tol 0, val_acc0.61, val_loss0.6652\n","Epoch 125, acc0.59, loss 0.6685, tol 0, val_acc0.62, val_loss0.6653\n","Epoch 126, acc0.62, loss 0.6635, tol 0, val_acc0.61, val_loss0.6664\n","Epoch 127, acc0.62, loss 0.6643, tol 0, val_acc0.57, val_loss0.6759\n","Epoch 128, acc0.59, loss 0.6676, tol 0, val_acc0.61, val_loss0.6646\n","Epoch 129, acc0.59, loss 0.6692, tol 0, val_acc0.60, val_loss0.6660\n","Epoch 130, acc0.57, loss 0.6709, tol 0, val_acc0.61, val_loss0.6646\n","Epoch 131, acc0.57, loss 0.6777, tol 0, val_acc0.61, val_loss0.6647\n","Epoch 132, acc0.61, loss 0.6676, tol 0, val_acc0.62, val_loss0.6646\n","Epoch 133, acc0.58, loss 0.6703, tol 0, val_acc0.60, val_loss0.6685\n","Epoch 134, acc0.57, loss 0.6745, tol 0, val_acc0.60, val_loss0.6665\n","Epoch 135, acc0.59, loss 0.6749, tol 0, val_acc0.62, val_loss0.6620 -- checkpoint saved\n","Epoch 136, acc0.60, loss 0.6714, tol 0, val_acc0.58, val_loss0.6701\n","Epoch 137, acc0.59, loss 0.6717, tol 0, val_acc0.58, val_loss0.6651\n","Epoch 138, acc0.61, loss 0.6639, tol 0, val_acc0.61, val_loss0.6637\n","Epoch 139, acc0.58, loss 0.6734, tol 0, val_acc0.63, val_loss0.6612 -- checkpoint saved\n","Epoch 140, acc0.58, loss 0.6741, tol 0, val_acc0.57, val_loss0.6728\n","Epoch 141, acc0.60, loss 0.6637, tol 0, val_acc0.60, val_loss0.6672\n","Epoch 142, acc0.57, loss 0.6730, tol 0, val_acc0.62, val_loss0.6611\n","Epoch 143, acc0.63, loss 0.6632, tol 0, val_acc0.63, val_loss0.6616\n","Epoch 144, acc0.62, loss 0.6628, tol 0, val_acc0.62, val_loss0.6600\n","Epoch 145, acc0.63, loss 0.6600, tol 0, val_acc0.62, val_loss0.6610\n","Epoch 146, acc0.61, loss 0.6608, tol 0, val_acc0.61, val_loss0.6596\n","Epoch 147, acc0.61, loss 0.6665, tol 0, val_acc0.62, val_loss0.6599\n","Epoch 148, acc0.59, loss 0.6651, tol 0, val_acc0.60, val_loss0.6634\n","Epoch 149, acc0.60, loss 0.6635, tol 0, val_acc0.62, val_loss0.6589\n","Epoch 150, acc0.58, loss 0.6733, tol 0, val_acc0.58, val_loss0.6636\n","Epoch 151, acc0.59, loss 0.6658, tol 0, val_acc0.58, val_loss0.6634\n","Epoch 152, acc0.59, loss 0.6657, tol 0, val_acc0.64, val_loss0.6579 -- checkpoint saved\n","Epoch 153, acc0.63, loss 0.6578, tol 0, val_acc0.62, val_loss0.6579\n","Epoch 154, acc0.60, loss 0.6637, tol 0, val_acc0.58, val_loss0.6703\n","Epoch 155, acc0.59, loss 0.6660, tol 0, val_acc0.61, val_loss0.6607\n","Epoch 156, acc0.61, loss 0.6628, tol 0, val_acc0.63, val_loss0.6584\n","Epoch 157, acc0.61, loss 0.6626, tol 0, val_acc0.64, val_loss0.6572 -- checkpoint saved\n","Epoch 158, acc0.59, loss 0.6693, tol 0, val_acc0.62, val_loss0.6570\n","Epoch 159, acc0.61, loss 0.6610, tol 0, val_acc0.62, val_loss0.6561\n","Epoch 160, acc0.62, loss 0.6662, tol 0, val_acc0.59, val_loss0.6615\n","Epoch 161, acc0.60, loss 0.6659, tol 0, val_acc0.62, val_loss0.6557\n","Epoch 162, acc0.63, loss 0.6612, tol 0, val_acc0.58, val_loss0.6674\n","Epoch 163, acc0.63, loss 0.6617, tol 0, val_acc0.64, val_loss0.6568\n","Epoch 164, acc0.64, loss 0.6539, tol 0, val_acc0.61, val_loss0.6592\n","Epoch 165, acc0.59, loss 0.6652, tol 0, val_acc0.56, val_loss0.6725\n","Epoch 166, acc0.59, loss 0.6659, tol 0, val_acc0.54, val_loss0.6816\n","Epoch 167, acc0.59, loss 0.6671, tol 0, val_acc0.62, val_loss0.6561\n","Epoch 168, acc0.62, loss 0.6549, tol 0, val_acc0.62, val_loss0.6552\n","Epoch 169, acc0.59, loss 0.6647, tol 0, val_acc0.63, val_loss0.6537\n","Epoch 170, acc0.60, loss 0.6646, tol 0, val_acc0.62, val_loss0.6573\n","Epoch 171, acc0.60, loss 0.6655, tol 0, val_acc0.63, val_loss0.6549\n","Epoch 172, acc0.61, loss 0.6622, tol 0, val_acc0.63, val_loss0.6533\n","Epoch 173, acc0.62, loss 0.6600, tol 0, val_acc0.62, val_loss0.6543\n","Epoch 174, acc0.63, loss 0.6549, tol 0, val_acc0.63, val_loss0.6530\n","Epoch 175, acc0.62, loss 0.6569, tol 0, val_acc0.62, val_loss0.6580\n","Epoch 176, acc0.60, loss 0.6623, tol 0, val_acc0.62, val_loss0.6516\n","Epoch 177, acc0.62, loss 0.6530, tol 0, val_acc0.58, val_loss0.6688\n","Epoch 178, acc0.58, loss 0.6730, tol 0, val_acc0.61, val_loss0.6535\n","Epoch 179, acc0.59, loss 0.6647, tol 0, val_acc0.62, val_loss0.6553\n","Epoch 180, acc0.64, loss 0.6521, tol 0, val_acc0.62, val_loss0.6510\n","Epoch 181, acc0.62, loss 0.6562, tol 0, val_acc0.62, val_loss0.6528\n","Epoch 182, acc0.64, loss 0.6515, tol 0, val_acc0.63, val_loss0.6522\n","Epoch 183, acc0.63, loss 0.6559, tol 0, val_acc0.62, val_loss0.6578\n","Epoch 184, acc0.60, loss 0.6618, tol 0, val_acc0.63, val_loss0.6499\n","Epoch 185, acc0.65, loss 0.6497, tol 0, val_acc0.63, val_loss0.6508\n","Epoch 186, acc0.67, loss 0.6461, tol 0, val_acc0.62, val_loss0.6498\n","Epoch 187, acc0.61, loss 0.6588, tol 0, val_acc0.63, val_loss0.6494\n","Epoch 188, acc0.63, loss 0.6532, tol 0, val_acc0.63, val_loss0.6497\n","Epoch 189, acc0.62, loss 0.6555, tol 0, val_acc0.63, val_loss0.6485\n","Epoch 190, acc0.63, loss 0.6586, tol 0, val_acc0.63, val_loss0.6485\n","Epoch 191, acc0.61, loss 0.6620, tol 0, val_acc0.63, val_loss0.6500\n","Epoch 192, acc0.65, loss 0.6468, tol 0, val_acc0.62, val_loss0.6539\n","Epoch 193, acc0.61, loss 0.6531, tol 0, val_acc0.62, val_loss0.6521\n","Epoch 194, acc0.60, loss 0.6624, tol 0, val_acc0.64, val_loss0.6476\n","Epoch 195, acc0.61, loss 0.6620, tol 0, val_acc0.63, val_loss0.6468\n","Epoch 196, acc0.63, loss 0.6521, tol 0, val_acc0.63, val_loss0.6471\n","Epoch 197, acc0.61, loss 0.6608, tol 0, val_acc0.61, val_loss0.6572\n","Epoch 198, acc0.62, loss 0.6475, tol 0, val_acc0.61, val_loss0.6632\n","Epoch 199, acc0.61, loss 0.6551, tol 0, val_acc0.62, val_loss0.6473\n","Epoch 200, acc0.63, loss 0.6527, tol 0, val_acc0.62, val_loss0.6505\n","Epoch 201, acc0.64, loss 0.6465, tol 0, val_acc0.62, val_loss0.6462\n","Epoch 202, acc0.63, loss 0.6501, tol 0, val_acc0.62, val_loss0.6479\n","Epoch 203, acc0.61, loss 0.6529, tol 0, val_acc0.62, val_loss0.6555\n","Epoch 204, acc0.62, loss 0.6541, tol 0, val_acc0.62, val_loss0.6483\n","Epoch 205, acc0.63, loss 0.6470, tol 0, val_acc0.63, val_loss0.6462\n","Epoch 206, acc0.62, loss 0.6556, tol 0, val_acc0.64, val_loss0.6442\n","Epoch 207, acc0.63, loss 0.6486, tol 0, val_acc0.63, val_loss0.6439\n","Epoch 208, acc0.64, loss 0.6457, tol 0, val_acc0.63, val_loss0.6437\n","Epoch 209, acc0.62, loss 0.6565, tol 0, val_acc0.63, val_loss0.6501\n","Epoch 210, acc0.63, loss 0.6531, tol 0, val_acc0.63, val_loss0.6439\n","Epoch 211, acc0.64, loss 0.6495, tol 0, val_acc0.62, val_loss0.6467\n","Epoch 212, acc0.66, loss 0.6387, tol 0, val_acc0.63, val_loss0.6449\n","Epoch 213, acc0.64, loss 0.6513, tol 0, val_acc0.62, val_loss0.6427\n","Epoch 214, acc0.64, loss 0.6512, tol 0, val_acc0.63, val_loss0.6493\n","Epoch 215, acc0.61, loss 0.6619, tol 0, val_acc0.63, val_loss0.6447\n","Epoch 216, acc0.61, loss 0.6546, tol 0, val_acc0.62, val_loss0.6552\n","Epoch 217, acc0.62, loss 0.6533, tol 0, val_acc0.63, val_loss0.6494\n","Epoch 218, acc0.64, loss 0.6493, tol 0, val_acc0.61, val_loss0.6578\n","Epoch 219, acc0.61, loss 0.6533, tol 0, val_acc0.63, val_loss0.6432\n","Epoch 220, acc0.62, loss 0.6499, tol 0, val_acc0.63, val_loss0.6502\n","Epoch 221, acc0.62, loss 0.6559, tol 0, val_acc0.63, val_loss0.6501\n","Epoch 222, acc0.63, loss 0.6456, tol 0, val_acc0.63, val_loss0.6409\n","Epoch 223, acc0.63, loss 0.6507, tol 0, val_acc0.62, val_loss0.6434\n","Epoch 224, acc0.64, loss 0.6441, tol 0, val_acc0.63, val_loss0.6423\n","Epoch 225, acc0.64, loss 0.6461, tol 0, val_acc0.64, val_loss0.6405\n","Epoch 226, acc0.64, loss 0.6446, tol 0, val_acc0.64, val_loss0.6443\n","Epoch 227, acc0.63, loss 0.6508, tol 0, val_acc0.65, val_loss0.6440\n","Epoch 228, acc0.61, loss 0.6501, tol 0, val_acc0.63, val_loss0.6414\n","Epoch 229, acc0.63, loss 0.6449, tol 0, val_acc0.63, val_loss0.6408\n","Epoch 230, acc0.65, loss 0.6418, tol 0, val_acc0.64, val_loss0.6385\n","Epoch 231, acc0.66, loss 0.6427, tol 0, val_acc0.63, val_loss0.6381\n","Epoch 232, acc0.65, loss 0.6415, tol 0, val_acc0.62, val_loss0.6380\n","Epoch 233, acc0.64, loss 0.6462, tol 0, val_acc0.63, val_loss0.6386\n","Epoch 234, acc0.65, loss 0.6421, tol 0, val_acc0.64, val_loss0.6417\n","Epoch 235, acc0.65, loss 0.6443, tol 0, val_acc0.64, val_loss0.6454\n","Epoch 236, acc0.63, loss 0.6455, tol 0, val_acc0.64, val_loss0.6368\n","Epoch 237, acc0.64, loss 0.6459, tol 0, val_acc0.63, val_loss0.6397\n","Epoch 238, acc0.64, loss 0.6482, tol 0, val_acc0.63, val_loss0.6375\n","Epoch 239, acc0.65, loss 0.6439, tol 0, val_acc0.63, val_loss0.6480\n","Epoch 240, acc0.61, loss 0.6590, tol 0, val_acc0.62, val_loss0.6364\n","Epoch 241, acc0.63, loss 0.6452, tol 0, val_acc0.65, val_loss0.6416\n","Epoch 242, acc0.66, loss 0.6290, tol 0, val_acc0.64, val_loss0.6357\n","Epoch 243, acc0.65, loss 0.6403, tol 0, val_acc0.63, val_loss0.6446\n","Epoch 244, acc0.65, loss 0.6377, tol 0, val_acc0.63, val_loss0.6370\n","Epoch 245, acc0.65, loss 0.6389, tol 0, val_acc0.64, val_loss0.6347\n","Epoch 246, acc0.66, loss 0.6375, tol 0, val_acc0.60, val_loss0.6594\n","Epoch 247, acc0.64, loss 0.6427, tol 0, val_acc0.62, val_loss0.6452\n","Epoch 248, acc0.65, loss 0.6397, tol 0, val_acc0.65, val_loss0.6401\n","Epoch 249, acc0.66, loss 0.6372, tol 0, val_acc0.64, val_loss0.6379\n","Epoch 250, acc0.63, loss 0.6408, tol 0, val_acc0.64, val_loss0.6381\n","Epoch 251, acc0.64, loss 0.6466, tol 0, val_acc0.66, val_loss0.6408\n","Epoch 252, acc0.63, loss 0.6483, tol 0, val_acc0.62, val_loss0.6509\n","Epoch 253, acc0.64, loss 0.6472, tol 0, val_acc0.63, val_loss0.6343\n","Epoch 254, acc0.64, loss 0.6391, tol 0, val_acc0.64, val_loss0.6410\n","Epoch 255, acc0.61, loss 0.6526, tol 0, val_acc0.65, val_loss0.6379\n","Epoch 256, acc0.62, loss 0.6413, tol 0, val_acc0.65, val_loss0.6320\n","Epoch 257, acc0.67, loss 0.6302, tol 0, val_acc0.65, val_loss0.6313\n","Epoch 258, acc0.65, loss 0.6477, tol 0, val_acc0.65, val_loss0.6314\n","Epoch 259, acc0.66, loss 0.6344, tol 0, val_acc0.65, val_loss0.6327\n","Epoch 260, acc0.67, loss 0.6371, tol 0, val_acc0.63, val_loss0.6317\n","Epoch 261, acc0.63, loss 0.6434, tol 0, val_acc0.64, val_loss0.6391\n","Epoch 262, acc0.66, loss 0.6293, tol 0, val_acc0.65, val_loss0.6307\n","Epoch 263, acc0.66, loss 0.6431, tol 0, val_acc0.64, val_loss0.6299\n","Epoch 264, acc0.64, loss 0.6382, tol 0, val_acc0.64, val_loss0.6322\n","Epoch 265, acc0.64, loss 0.6418, tol 0, val_acc0.64, val_loss0.6310\n","Epoch 266, acc0.62, loss 0.6498, tol 0, val_acc0.64, val_loss0.6313\n","Epoch 267, acc0.66, loss 0.6344, tol 0, val_acc0.64, val_loss0.6348\n","Epoch 268, acc0.67, loss 0.6312, tol 0, val_acc0.65, val_loss0.6328\n","Epoch 269, acc0.66, loss 0.6313, tol 0, val_acc0.64, val_loss0.6281\n","Epoch 270, acc0.66, loss 0.6340, tol 0, val_acc0.64, val_loss0.6278\n","Epoch 271, acc0.66, loss 0.6335, tol 0, val_acc0.65, val_loss0.6370\n","Epoch 272, acc0.65, loss 0.6389, tol 0, val_acc0.63, val_loss0.6447\n","Epoch 273, acc0.65, loss 0.6401, tol 0, val_acc0.65, val_loss0.6276\n","Epoch 274, acc0.64, loss 0.6370, tol 0, val_acc0.63, val_loss0.6439\n","Epoch 275, acc0.65, loss 0.6339, tol 0, val_acc0.66, val_loss0.6336\n","Epoch 276, acc0.65, loss 0.6407, tol 0, val_acc0.65, val_loss0.6272\n","Epoch 277, acc0.67, loss 0.6331, tol 0, val_acc0.65, val_loss0.6286\n","Epoch 278, acc0.67, loss 0.6318, tol 0, val_acc0.65, val_loss0.6255\n","Epoch 279, acc0.64, loss 0.6374, tol 0, val_acc0.65, val_loss0.6283\n","Epoch 280, acc0.63, loss 0.6380, tol 0, val_acc0.62, val_loss0.6478\n","Epoch 281, acc0.62, loss 0.6505, tol 0, val_acc0.65, val_loss0.6283\n","Epoch 282, acc0.66, loss 0.6274, tol 0, val_acc0.66, val_loss0.6261\n","Epoch 283, acc0.67, loss 0.6295, tol 0, val_acc0.66, val_loss0.6248\n","Epoch 284, acc0.70, loss 0.6253, tol 0, val_acc0.65, val_loss0.6256\n","Epoch 285, acc0.68, loss 0.6211, tol 0, val_acc0.65, val_loss0.6283\n","Epoch 286, acc0.65, loss 0.6285, tol 0, val_acc0.65, val_loss0.6276\n","Epoch 287, acc0.68, loss 0.6234, tol 0, val_acc0.65, val_loss0.6283\n","Epoch 288, acc0.65, loss 0.6338, tol 0, val_acc0.66, val_loss0.6251\n","Epoch 289, acc0.67, loss 0.6248, tol 0, val_acc0.66, val_loss0.6230 -- checkpoint saved\n","Epoch 290, acc0.67, loss 0.6243, tol 0, val_acc0.66, val_loss0.6297\n","Epoch 291, acc0.65, loss 0.6343, tol 0, val_acc0.66, val_loss0.6225\n","Epoch 292, acc0.68, loss 0.6235, tol 0, val_acc0.66, val_loss0.6232\n","Epoch 293, acc0.67, loss 0.6286, tol 0, val_acc0.66, val_loss0.6218\n","Epoch 294, acc0.68, loss 0.6274, tol 0, val_acc0.65, val_loss0.6297\n","Epoch 295, acc0.66, loss 0.6229, tol 0, val_acc0.67, val_loss0.6288\n","Epoch 296, acc0.66, loss 0.6278, tol 0, val_acc0.67, val_loss0.6265\n","Epoch 297, acc0.68, loss 0.6242, tol 0, val_acc0.66, val_loss0.6202\n","Epoch 298, acc0.70, loss 0.6229, tol 0, val_acc0.66, val_loss0.6209\n","Epoch 299, acc0.68, loss 0.6241, tol 0, val_acc0.66, val_loss0.6268\n","Epoch 300, acc0.63, loss 0.6423, tol 0, val_acc0.65, val_loss0.6192\n","Epoch 301, acc0.67, loss 0.6241, tol 0, val_acc0.65, val_loss0.6197\n","Epoch 302, acc0.67, loss 0.6272, tol 0, val_acc0.65, val_loss0.6191\n","Epoch 303, acc0.66, loss 0.6316, tol 0, val_acc0.66, val_loss0.6180\n","Epoch 304, acc0.67, loss 0.6267, tol 0, val_acc0.62, val_loss0.6427\n","Epoch 305, acc0.64, loss 0.6337, tol 0, val_acc0.66, val_loss0.6179\n","Epoch 306, acc0.69, loss 0.6138, tol 0, val_acc0.66, val_loss0.6174\n","Epoch 307, acc0.68, loss 0.6219, tol 0, val_acc0.66, val_loss0.6220\n","Epoch 308, acc0.64, loss 0.6337, tol 0, val_acc0.66, val_loss0.6256\n","Epoch 309, acc0.67, loss 0.6284, tol 0, val_acc0.67, val_loss0.6163 -- checkpoint saved\n","Epoch 310, acc0.68, loss 0.6203, tol 0, val_acc0.67, val_loss0.6192\n","Epoch 311, acc0.68, loss 0.6209, tol 0, val_acc0.67, val_loss0.6156\n","Epoch 312, acc0.67, loss 0.6198, tol 0, val_acc0.66, val_loss0.6248\n","Epoch 313, acc0.67, loss 0.6258, tol 0, val_acc0.64, val_loss0.6313\n","Epoch 314, acc0.69, loss 0.6198, tol 0, val_acc0.67, val_loss0.6176\n","Epoch 315, acc0.65, loss 0.6264, tol 0, val_acc0.67, val_loss0.6137\n","Epoch 316, acc0.67, loss 0.6253, tol 0, val_acc0.66, val_loss0.6149\n","Epoch 317, acc0.68, loss 0.6179, tol 0, val_acc0.66, val_loss0.6156\n","Epoch 318, acc0.69, loss 0.6112, tol 0, val_acc0.67, val_loss0.6133\n","Epoch 319, acc0.70, loss 0.6106, tol 0, val_acc0.65, val_loss0.6267\n","Epoch 320, acc0.66, loss 0.6264, tol 0, val_acc0.66, val_loss0.6168\n","Epoch 321, acc0.67, loss 0.6157, tol 0, val_acc0.66, val_loss0.6125\n","Epoch 322, acc0.65, loss 0.6243, tol 0, val_acc0.64, val_loss0.6309\n","Epoch 323, acc0.66, loss 0.6165, tol 0, val_acc0.66, val_loss0.6261\n","Epoch 324, acc0.67, loss 0.6154, tol 0, val_acc0.67, val_loss0.6121\n","Epoch 325, acc0.68, loss 0.6168, tol 0, val_acc0.66, val_loss0.6242\n","Epoch 326, acc0.65, loss 0.6261, tol 0, val_acc0.67, val_loss0.6114\n","Epoch 327, acc0.68, loss 0.6209, tol 0, val_acc0.67, val_loss0.6124\n","Epoch 328, acc0.67, loss 0.6198, tol 0, val_acc0.68, val_loss0.6132\n","Epoch 329, acc0.67, loss 0.6123, tol 0, val_acc0.66, val_loss0.6111\n","Epoch 330, acc0.67, loss 0.6214, tol 0, val_acc0.68, val_loss0.6120\n","Epoch 331, acc0.68, loss 0.6189, tol 0, val_acc0.67, val_loss0.6088\n","Epoch 332, acc0.69, loss 0.6162, tol 0, val_acc0.67, val_loss0.6143\n","Epoch 333, acc0.68, loss 0.6115, tol 0, val_acc0.67, val_loss0.6207\n","Epoch 334, acc0.67, loss 0.6079, tol 0, val_acc0.68, val_loss0.6128\n","Epoch 335, acc0.68, loss 0.6172, tol 0, val_acc0.67, val_loss0.6133\n","Epoch 336, acc0.68, loss 0.6172, tol 0, val_acc0.70, val_loss0.6063 -- checkpoint saved\n","Epoch 337, acc0.70, loss 0.6116, tol 0, val_acc0.68, val_loss0.6089\n","Epoch 338, acc0.69, loss 0.6109, tol 0, val_acc0.68, val_loss0.6077\n","Epoch 339, acc0.70, loss 0.6075, tol 0, val_acc0.68, val_loss0.6058\n","Epoch 340, acc0.69, loss 0.6072, tol 0, val_acc0.66, val_loss0.6156\n","Epoch 341, acc0.68, loss 0.6148, tol 0, val_acc0.65, val_loss0.6234\n","Epoch 342, acc0.68, loss 0.6103, tol 0, val_acc0.67, val_loss0.6118\n","Epoch 343, acc0.69, loss 0.6054, tol 0, val_acc0.68, val_loss0.6051\n","Epoch 344, acc0.68, loss 0.6063, tol 0, val_acc0.68, val_loss0.6065\n","Epoch 345, acc0.68, loss 0.6153, tol 0, val_acc0.67, val_loss0.6116\n","Epoch 346, acc0.67, loss 0.6164, tol 0, val_acc0.68, val_loss0.6067\n","Epoch 347, acc0.71, loss 0.5980, tol 0, val_acc0.69, val_loss0.6039\n","Epoch 348, acc0.65, loss 0.6224, tol 0, val_acc0.67, val_loss0.6028\n","Epoch 349, acc0.70, loss 0.6008, tol 0, val_acc0.70, val_loss0.6056\n","Epoch 350, acc0.70, loss 0.6153, tol 0, val_acc0.68, val_loss0.6027\n","Epoch 351, acc0.70, loss 0.6063, tol 0, val_acc0.68, val_loss0.6059\n","Epoch 352, acc0.67, loss 0.6052, tol 0, val_acc0.69, val_loss0.6007\n","Epoch 353, acc0.67, loss 0.6233, tol 0, val_acc0.66, val_loss0.6162\n","Epoch 354, acc0.66, loss 0.6198, tol 0, val_acc0.67, val_loss0.6088\n","Epoch 355, acc0.70, loss 0.6083, tol 0, val_acc0.67, val_loss0.6001\n","Epoch 356, acc0.70, loss 0.6059, tol 0, val_acc0.68, val_loss0.6047\n","Epoch 357, acc0.68, loss 0.6098, tol 0, val_acc0.69, val_loss0.6026\n","Epoch 358, acc0.69, loss 0.6055, tol 0, val_acc0.69, val_loss0.6024\n","Epoch 359, acc0.68, loss 0.6166, tol 0, val_acc0.69, val_loss0.5983\n","Epoch 360, acc0.70, loss 0.6023, tol 0, val_acc0.70, val_loss0.6005\n","Epoch 361, acc0.71, loss 0.6067, tol 0, val_acc0.69, val_loss0.5983\n","Epoch 362, acc0.72, loss 0.5924, tol 0, val_acc0.70, val_loss0.5996\n","Epoch 363, acc0.70, loss 0.6012, tol 0, val_acc0.69, val_loss0.5962\n","Epoch 364, acc0.70, loss 0.5982, tol 0, val_acc0.69, val_loss0.5987\n","Epoch 365, acc0.72, loss 0.5921, tol 0, val_acc0.69, val_loss0.5960\n","Epoch 366, acc0.71, loss 0.6009, tol 0, val_acc0.69, val_loss0.5965\n","Epoch 367, acc0.71, loss 0.5959, tol 0, val_acc0.69, val_loss0.6009\n","Epoch 368, acc0.72, loss 0.5993, tol 0, val_acc0.69, val_loss0.5986\n","Epoch 369, acc0.71, loss 0.5979, tol 0, val_acc0.68, val_loss0.5940\n","Epoch 370, acc0.72, loss 0.5984, tol 0, val_acc0.69, val_loss0.5978\n","Epoch 371, acc0.68, loss 0.6040, tol 0, val_acc0.64, val_loss0.6278\n","Epoch 372, acc0.66, loss 0.6188, tol 0, val_acc0.64, val_loss0.6211\n","Epoch 373, acc0.69, loss 0.6023, tol 0, val_acc0.64, val_loss0.6234\n","Epoch 374, acc0.68, loss 0.6070, tol 0, val_acc0.70, val_loss0.5960\n","Epoch 375, acc0.70, loss 0.6031, tol 0, val_acc0.71, val_loss0.5912 -- checkpoint saved\n","Epoch 376, acc0.71, loss 0.5988, tol 0, val_acc0.70, val_loss0.5911\n","Epoch 377, acc0.71, loss 0.5990, tol 0, val_acc0.70, val_loss0.5946\n","Epoch 378, acc0.72, loss 0.5952, tol 0, val_acc0.69, val_loss0.5902\n","Epoch 379, acc0.71, loss 0.5931, tol 0, val_acc0.69, val_loss0.5900\n","Epoch 380, acc0.70, loss 0.5971, tol 0, val_acc0.70, val_loss0.5915\n","Epoch 381, acc0.70, loss 0.6092, tol 0, val_acc0.70, val_loss0.5948\n","Epoch 382, acc0.69, loss 0.5941, tol 0, val_acc0.71, val_loss0.5904\n","Epoch 383, acc0.71, loss 0.5936, tol 0, val_acc0.70, val_loss0.5947\n","Epoch 384, acc0.71, loss 0.5961, tol 0, val_acc0.68, val_loss0.5962\n","Epoch 385, acc0.71, loss 0.5933, tol 0, val_acc0.71, val_loss0.5876\n","Epoch 386, acc0.72, loss 0.5918, tol 0, val_acc0.71, val_loss0.5870 -- checkpoint saved\n","Epoch 387, acc0.71, loss 0.5935, tol 0, val_acc0.70, val_loss0.5892\n","Epoch 388, acc0.72, loss 0.5882, tol 0, val_acc0.70, val_loss0.5864\n","Epoch 389, acc0.71, loss 0.5946, tol 0, val_acc0.70, val_loss0.5858\n","Epoch 390, acc0.71, loss 0.5989, tol 0, val_acc0.69, val_loss0.5919\n","Epoch 391, acc0.71, loss 0.5886, tol 0, val_acc0.71, val_loss0.5863\n","Epoch 392, acc0.69, loss 0.6014, tol 0, val_acc0.68, val_loss0.5924\n","Epoch 393, acc0.69, loss 0.6030, tol 0, val_acc0.71, val_loss0.5878\n","Epoch 394, acc0.72, loss 0.5852, tol 0, val_acc0.71, val_loss0.5855\n","Epoch 395, acc0.71, loss 0.5975, tol 0, val_acc0.71, val_loss0.5853 -- checkpoint saved\n","Epoch 396, acc0.73, loss 0.5899, tol 0, val_acc0.71, val_loss0.5820 -- checkpoint saved\n","Epoch 397, acc0.71, loss 0.5935, tol 0, val_acc0.71, val_loss0.5824\n","Epoch 398, acc0.74, loss 0.5752, tol 0, val_acc0.72, val_loss0.5824\n","Epoch 399, acc0.70, loss 0.5900, tol 0, val_acc0.71, val_loss0.5860\n","Epoch 400, acc0.69, loss 0.6011, tol 0, val_acc0.71, val_loss0.5804\n","Epoch 401, acc0.70, loss 0.5949, tol 0, val_acc0.71, val_loss0.5854\n","Epoch 402, acc0.72, loss 0.5807, tol 0, val_acc0.71, val_loss0.5824\n","Epoch 403, acc0.72, loss 0.5853, tol 0, val_acc0.71, val_loss0.5798\n","Epoch 404, acc0.75, loss 0.5715, tol 0, val_acc0.71, val_loss0.5840\n","Epoch 405, acc0.70, loss 0.5911, tol 0, val_acc0.71, val_loss0.5794\n","Epoch 406, acc0.70, loss 0.5927, tol 0, val_acc0.71, val_loss0.5811\n","Epoch 407, acc0.71, loss 0.5846, tol 0, val_acc0.72, val_loss0.5792 -- checkpoint saved\n","Epoch 408, acc0.74, loss 0.5849, tol 0, val_acc0.71, val_loss0.5768\n","Epoch 409, acc0.72, loss 0.5794, tol 0, val_acc0.71, val_loss0.5777\n","Epoch 410, acc0.72, loss 0.5866, tol 0, val_acc0.71, val_loss0.5809\n","Epoch 411, acc0.73, loss 0.5785, tol 0, val_acc0.71, val_loss0.5764\n","Epoch 412, acc0.71, loss 0.5857, tol 0, val_acc0.72, val_loss0.5759\n","Epoch 413, acc0.71, loss 0.5871, tol 0, val_acc0.72, val_loss0.5755 -- checkpoint saved\n","Epoch 414, acc0.71, loss 0.5816, tol 0, val_acc0.72, val_loss0.5770\n","Epoch 415, acc0.72, loss 0.5767, tol 0, val_acc0.71, val_loss0.5788\n","Epoch 416, acc0.73, loss 0.5826, tol 0, val_acc0.70, val_loss0.5841\n","Epoch 417, acc0.69, loss 0.5849, tol 0, val_acc0.71, val_loss0.5768\n","Epoch 418, acc0.72, loss 0.5817, tol 0, val_acc0.72, val_loss0.5736\n","Epoch 419, acc0.71, loss 0.5867, tol 0, val_acc0.72, val_loss0.5720\n","Epoch 420, acc0.72, loss 0.5798, tol 0, val_acc0.71, val_loss0.5748\n","Epoch 421, acc0.71, loss 0.5835, tol 0, val_acc0.71, val_loss0.5765\n","Epoch 422, acc0.70, loss 0.5953, tol 0, val_acc0.73, val_loss0.5707 -- checkpoint saved\n","Epoch 423, acc0.75, loss 0.5775, tol 0, val_acc0.72, val_loss0.5715\n","Epoch 424, acc0.73, loss 0.5787, tol 0, val_acc0.73, val_loss0.5711\n","Epoch 425, acc0.74, loss 0.5805, tol 0, val_acc0.71, val_loss0.5773\n","Epoch 426, acc0.72, loss 0.5771, tol 0, val_acc0.74, val_loss0.5696 -- checkpoint saved\n","Epoch 427, acc0.73, loss 0.5693, tol 0, val_acc0.72, val_loss0.5689\n","Epoch 428, acc0.73, loss 0.5793, tol 0, val_acc0.72, val_loss0.5700\n","Epoch 429, acc0.73, loss 0.5751, tol 0, val_acc0.72, val_loss0.5707\n","Epoch 430, acc0.72, loss 0.5807, tol 0, val_acc0.73, val_loss0.5690\n","Epoch 431, acc0.72, loss 0.5786, tol 0, val_acc0.68, val_loss0.5847\n","Epoch 432, acc0.70, loss 0.5878, tol 0, val_acc0.71, val_loss0.5732\n","Epoch 433, acc0.73, loss 0.5776, tol 0, val_acc0.73, val_loss0.5659\n","Epoch 434, acc0.71, loss 0.5829, tol 0, val_acc0.74, val_loss0.5641\n","Epoch 435, acc0.71, loss 0.5815, tol 0, val_acc0.73, val_loss0.5700\n","Epoch 436, acc0.72, loss 0.5764, tol 0, val_acc0.66, val_loss0.5994\n","Epoch 437, acc0.73, loss 0.5796, tol 0, val_acc0.73, val_loss0.5688\n","Epoch 438, acc0.74, loss 0.5723, tol 0, val_acc0.74, val_loss0.5663\n","Epoch 439, acc0.71, loss 0.5798, tol 0, val_acc0.73, val_loss0.5701\n","Epoch 440, acc0.71, loss 0.5818, tol 0, val_acc0.74, val_loss0.5660\n","Epoch 441, acc0.73, loss 0.5691, tol 0, val_acc0.74, val_loss0.5657\n","Epoch 442, acc0.75, loss 0.5659, tol 0, val_acc0.72, val_loss0.5653\n","Epoch 443, acc0.74, loss 0.5586, tol 0, val_acc0.72, val_loss0.5640\n","Epoch 444, acc0.75, loss 0.5572, tol 0, val_acc0.73, val_loss0.5636\n","Epoch 445, acc0.75, loss 0.5558, tol 0, val_acc0.74, val_loss0.5599\n","Epoch 446, acc0.75, loss 0.5584, tol 0, val_acc0.73, val_loss0.5619\n","Epoch 447, acc0.73, loss 0.5694, tol 0, val_acc0.74, val_loss0.5603\n","Epoch 448, acc0.73, loss 0.5721, tol 0, val_acc0.73, val_loss0.5636\n","Epoch 449, acc0.74, loss 0.5680, tol 0, val_acc0.74, val_loss0.5583 -- checkpoint saved\n","Epoch 450, acc0.75, loss 0.5550, tol 0, val_acc0.73, val_loss0.5604\n","Epoch 451, acc0.75, loss 0.5619, tol 0, val_acc0.73, val_loss0.5595\n","Epoch 452, acc0.74, loss 0.5627, tol 0, val_acc0.73, val_loss0.5577\n","Epoch 453, acc0.75, loss 0.5576, tol 0, val_acc0.72, val_loss0.5598\n","Epoch 454, acc0.74, loss 0.5673, tol 0, val_acc0.73, val_loss0.5573\n","Epoch 455, acc0.74, loss 0.5709, tol 0, val_acc0.72, val_loss0.5597\n","Epoch 456, acc0.72, loss 0.5689, tol 0, val_acc0.70, val_loss0.5678\n","Epoch 457, acc0.72, loss 0.5734, tol 0, val_acc0.74, val_loss0.5533\n","Epoch 458, acc0.74, loss 0.5667, tol 0, val_acc0.73, val_loss0.5533\n","Epoch 459, acc0.73, loss 0.5628, tol 0, val_acc0.73, val_loss0.5527\n","Epoch 460, acc0.74, loss 0.5544, tol 0, val_acc0.74, val_loss0.5513 -- checkpoint saved\n","Epoch 461, acc0.74, loss 0.5596, tol 0, val_acc0.75, val_loss0.5508 -- checkpoint saved\n","Epoch 462, acc0.73, loss 0.5698, tol 0, val_acc0.74, val_loss0.5551\n","Epoch 463, acc0.74, loss 0.5657, tol 0, val_acc0.75, val_loss0.5495 -- checkpoint saved\n","Epoch 464, acc0.75, loss 0.5610, tol 0, val_acc0.73, val_loss0.5541\n","Epoch 465, acc0.77, loss 0.5425, tol 0, val_acc0.75, val_loss0.5480\n","Epoch 466, acc0.76, loss 0.5494, tol 0, val_acc0.73, val_loss0.5537\n","Epoch 467, acc0.72, loss 0.5681, tol 0, val_acc0.72, val_loss0.5615\n","Epoch 468, acc0.71, loss 0.5811, tol 0, val_acc0.75, val_loss0.5465 -- checkpoint saved\n","Epoch 469, acc0.77, loss 0.5505, tol 0, val_acc0.75, val_loss0.5463\n","Epoch 470, acc0.77, loss 0.5521, tol 0, val_acc0.75, val_loss0.5471\n","Epoch 471, acc0.76, loss 0.5439, tol 0, val_acc0.75, val_loss0.5472\n","Epoch 472, acc0.75, loss 0.5559, tol 0, val_acc0.74, val_loss0.5471\n","Epoch 473, acc0.74, loss 0.5560, tol 0, val_acc0.75, val_loss0.5444\n","Epoch 474, acc0.75, loss 0.5539, tol 0, val_acc0.76, val_loss0.5442 -- checkpoint saved\n","Epoch 475, acc0.76, loss 0.5529, tol 0, val_acc0.76, val_loss0.5455\n","Epoch 476, acc0.75, loss 0.5581, tol 0, val_acc0.73, val_loss0.5520\n","Epoch 477, acc0.76, loss 0.5414, tol 0, val_acc0.72, val_loss0.5544\n","Epoch 478, acc0.76, loss 0.5460, tol 0, val_acc0.75, val_loss0.5415\n","Epoch 479, acc0.75, loss 0.5510, tol 0, val_acc0.74, val_loss0.5418\n","Epoch 480, acc0.75, loss 0.5380, tol 0, val_acc0.73, val_loss0.5505\n","Epoch 481, acc0.74, loss 0.5534, tol 0, val_acc0.74, val_loss0.5406\n","Epoch 482, acc0.73, loss 0.5624, tol 0, val_acc0.74, val_loss0.5447\n","Epoch 483, acc0.76, loss 0.5475, tol 0, val_acc0.74, val_loss0.5459\n","Epoch 484, acc0.74, loss 0.5427, tol 0, val_acc0.75, val_loss0.5444\n","Epoch 485, acc0.71, loss 0.5693, tol 0, val_acc0.74, val_loss0.5400\n","Epoch 486, acc0.74, loss 0.5489, tol 0, val_acc0.77, val_loss0.5366 -- checkpoint saved\n","Epoch 487, acc0.76, loss 0.5371, tol 0, val_acc0.76, val_loss0.5360\n","Epoch 488, acc0.75, loss 0.5448, tol 0, val_acc0.73, val_loss0.5510\n","Epoch 489, acc0.75, loss 0.5526, tol 0, val_acc0.77, val_loss0.5356 -- checkpoint saved\n","Epoch 490, acc0.77, loss 0.5409, tol 0, val_acc0.76, val_loss0.5363\n","Epoch 491, acc0.77, loss 0.5405, tol 0, val_acc0.74, val_loss0.5352\n","Epoch 492, acc0.78, loss 0.5230, tol 0, val_acc0.76, val_loss0.5352\n","Epoch 493, acc0.73, loss 0.5606, tol 0, val_acc0.76, val_loss0.5352\n","Epoch 494, acc0.75, loss 0.5491, tol 0, val_acc0.76, val_loss0.5319\n","Epoch 495, acc0.77, loss 0.5367, tol 0, val_acc0.75, val_loss0.5338\n","Epoch 496, acc0.75, loss 0.5392, tol 0, val_acc0.77, val_loss0.5318 -- checkpoint saved\n","Epoch 497, acc0.72, loss 0.5599, tol 0, val_acc0.71, val_loss0.5570\n","Epoch 498, acc0.73, loss 0.5533, tol 0, val_acc0.76, val_loss0.5327\n","Epoch 499, acc0.75, loss 0.5368, tol 0, val_acc0.74, val_loss0.5397\n","Epoch 500, acc0.73, loss 0.5446, tol 0, val_acc0.74, val_loss0.5440\n","Epoch 501, acc0.76, loss 0.5398, tol 1, val_acc0.77, val_loss0.5295 -- checkpoint saved\n","Reach Max Epoch Number\n","+------------------------+------------+\n","|        Modules         | Parameters |\n","+------------------------+------------+\n","| layers.0.0.fc_Q.weight |     45     |\n","| layers.0.0.fc_K.weight |     45     |\n","| layers.0.0.fc_V.weight |     45     |\n","| layers.0.1.fc_Q.weight |     45     |\n","| layers.0.1.fc_K.weight |     45     |\n","| layers.0.1.fc_V.weight |     45     |\n","| layers.1.0.fc_Q.weight |    162     |\n","| layers.1.0.fc_K.weight |    162     |\n","| layers.1.0.fc_V.weight |    162     |\n","| layers.1.1.fc_Q.weight |    162     |\n","| layers.1.1.fc_K.weight |    162     |\n","| layers.1.1.fc_V.weight |    162     |\n","|    classify.weight     |     36     |\n","|     classify.bias      |     2      |\n","+------------------------+------------+\n","Total Trainable Params: 1280\n","Epoch 0, acc0.52, loss 1.6122, tol 0, val_acc0.50, val_loss1.1640 -- checkpoint saved\n","Epoch 1, acc0.49, loss 0.8815, tol 0, val_acc0.49, val_loss0.7090\n","Epoch 2, acc0.48, loss 0.7204, tol 0, val_acc0.50, val_loss0.7306\n","Epoch 3, acc0.48, loss 0.7255, tol 0, val_acc0.49, val_loss0.7040\n","Epoch 4, acc0.48, loss 0.7022, tol 0, val_acc0.49, val_loss0.7051\n","Epoch 5, acc0.49, loss 0.7060, tol 0, val_acc0.48, val_loss0.7026\n","Epoch 6, acc0.48, loss 0.7004, tol 0, val_acc0.47, val_loss0.7006\n","Epoch 7, acc0.49, loss 0.6999, tol 0, val_acc0.49, val_loss0.6998\n","Epoch 8, acc0.49, loss 0.6964, tol 0, val_acc0.49, val_loss0.6992\n","Epoch 9, acc0.49, loss 0.6970, tol 0, val_acc0.50, val_loss0.6987\n","Epoch 10, acc0.47, loss 0.7010, tol 0, val_acc0.47, val_loss0.6981\n","Epoch 11, acc0.51, loss 0.6963, tol 0, val_acc0.49, val_loss0.6980\n","Epoch 12, acc0.48, loss 0.7000, tol 0, val_acc0.49, val_loss0.6968\n","Epoch 13, acc0.47, loss 0.7009, tol 0, val_acc0.50, val_loss0.6971\n","Epoch 14, acc0.49, loss 0.6979, tol 0, val_acc0.49, val_loss0.6962\n","Epoch 15, acc0.51, loss 0.6954, tol 0, val_acc0.50, val_loss0.6952\n","Epoch 16, acc0.50, loss 0.6954, tol 0, val_acc0.50, val_loss0.6961\n","Epoch 17, acc0.50, loss 0.6966, tol 0, val_acc0.50, val_loss0.6939\n","Epoch 18, acc0.51, loss 0.6962, tol 0, val_acc0.51, val_loss0.6939 -- checkpoint saved\n","Epoch 19, acc0.52, loss 0.6937, tol 0, val_acc0.52, val_loss0.6930 -- checkpoint saved\n","Epoch 20, acc0.53, loss 0.6940, tol 0, val_acc0.54, val_loss0.6925 -- checkpoint saved\n","Epoch 21, acc0.53, loss 0.6900, tol 0, val_acc0.51, val_loss0.6924\n","Epoch 22, acc0.52, loss 0.6920, tol 0, val_acc0.54, val_loss0.6912 -- checkpoint saved\n","Epoch 23, acc0.54, loss 0.6889, tol 0, val_acc0.52, val_loss0.6930\n","Epoch 24, acc0.52, loss 0.6925, tol 0, val_acc0.52, val_loss0.6899\n","Epoch 25, acc0.52, loss 0.6913, tol 0, val_acc0.55, val_loss0.6890 -- checkpoint saved\n","Epoch 26, acc0.56, loss 0.6859, tol 0, val_acc0.54, val_loss0.6886\n","Epoch 27, acc0.55, loss 0.6869, tol 0, val_acc0.56, val_loss0.6875 -- checkpoint saved\n","Epoch 28, acc0.56, loss 0.6875, tol 0, val_acc0.53, val_loss0.6873\n","Epoch 29, acc0.56, loss 0.6838, tol 0, val_acc0.57, val_loss0.6862 -- checkpoint saved\n","Epoch 30, acc0.54, loss 0.6869, tol 0, val_acc0.55, val_loss0.6863\n","Epoch 31, acc0.54, loss 0.6894, tol 0, val_acc0.53, val_loss0.6855\n","Epoch 32, acc0.55, loss 0.6862, tol 0, val_acc0.57, val_loss0.6844 -- checkpoint saved\n","Epoch 33, acc0.58, loss 0.6849, tol 0, val_acc0.59, val_loss0.6837 -- checkpoint saved\n","Epoch 34, acc0.55, loss 0.6859, tol 0, val_acc0.56, val_loss0.6837\n","Epoch 35, acc0.56, loss 0.6838, tol 0, val_acc0.57, val_loss0.6826\n","Epoch 36, acc0.58, loss 0.6808, tol 0, val_acc0.58, val_loss0.6820\n","Epoch 37, acc0.58, loss 0.6821, tol 0, val_acc0.59, val_loss0.6812 -- checkpoint saved\n","Epoch 38, acc0.60, loss 0.6793, tol 0, val_acc0.60, val_loss0.6807 -- checkpoint saved\n","Epoch 39, acc0.59, loss 0.6780, tol 0, val_acc0.59, val_loss0.6797\n","Epoch 40, acc0.60, loss 0.6776, tol 0, val_acc0.54, val_loss0.6825\n","Epoch 41, acc0.57, loss 0.6792, tol 0, val_acc0.57, val_loss0.6798\n","Epoch 42, acc0.58, loss 0.6794, tol 0, val_acc0.58, val_loss0.6792\n","Epoch 43, acc0.61, loss 0.6748, tol 0, val_acc0.59, val_loss0.6773\n","Epoch 44, acc0.60, loss 0.6732, tol 0, val_acc0.61, val_loss0.6758 -- checkpoint saved\n","Epoch 45, acc0.59, loss 0.6764, tol 0, val_acc0.58, val_loss0.6769\n","Epoch 46, acc0.56, loss 0.6770, tol 0, val_acc0.59, val_loss0.6751\n","Epoch 47, acc0.58, loss 0.6765, tol 0, val_acc0.55, val_loss0.6781\n","Epoch 48, acc0.62, loss 0.6713, tol 0, val_acc0.60, val_loss0.6743\n","Epoch 49, acc0.57, loss 0.6775, tol 0, val_acc0.62, val_loss0.6723 -- checkpoint saved\n","Epoch 50, acc0.60, loss 0.6739, tol 0, val_acc0.62, val_loss0.6716\n","Epoch 51, acc0.62, loss 0.6683, tol 0, val_acc0.61, val_loss0.6711\n","Epoch 52, acc0.60, loss 0.6673, tol 0, val_acc0.59, val_loss0.6718\n","Epoch 53, acc0.61, loss 0.6649, tol 0, val_acc0.62, val_loss0.6692 -- checkpoint saved\n","Epoch 54, acc0.60, loss 0.6682, tol 0, val_acc0.62, val_loss0.6683\n","Epoch 55, acc0.60, loss 0.6739, tol 0, val_acc0.61, val_loss0.6684\n","Epoch 56, acc0.59, loss 0.6711, tol 0, val_acc0.61, val_loss0.6680\n","Epoch 57, acc0.61, loss 0.6669, tol 0, val_acc0.62, val_loss0.6670\n","Epoch 58, acc0.61, loss 0.6629, tol 0, val_acc0.62, val_loss0.6657\n","Epoch 59, acc0.61, loss 0.6709, tol 0, val_acc0.62, val_loss0.6651\n","Epoch 60, acc0.61, loss 0.6640, tol 0, val_acc0.57, val_loss0.6717\n","Epoch 61, acc0.61, loss 0.6628, tol 0, val_acc0.58, val_loss0.6701\n","Epoch 62, acc0.61, loss 0.6617, tol 0, val_acc0.57, val_loss0.6719\n","Epoch 63, acc0.59, loss 0.6637, tol 0, val_acc0.56, val_loss0.6721\n","Epoch 64, acc0.62, loss 0.6623, tol 0, val_acc0.59, val_loss0.6698\n","Epoch 65, acc0.61, loss 0.6647, tol 0, val_acc0.62, val_loss0.6615\n","Epoch 66, acc0.61, loss 0.6614, tol 0, val_acc0.62, val_loss0.6598 -- checkpoint saved\n","Epoch 67, acc0.63, loss 0.6627, tol 0, val_acc0.62, val_loss0.6581\n","Epoch 68, acc0.64, loss 0.6547, tol 0, val_acc0.63, val_loss0.6572 -- checkpoint saved\n","Epoch 69, acc0.62, loss 0.6581, tol 0, val_acc0.63, val_loss0.6557\n","Epoch 70, acc0.63, loss 0.6501, tol 0, val_acc0.65, val_loss0.6554 -- checkpoint saved\n","Epoch 71, acc0.66, loss 0.6483, tol 0, val_acc0.64, val_loss0.6545\n","Epoch 72, acc0.64, loss 0.6473, tol 0, val_acc0.64, val_loss0.6527\n","Epoch 73, acc0.65, loss 0.6502, tol 0, val_acc0.63, val_loss0.6523\n","Epoch 74, acc0.65, loss 0.6499, tol 0, val_acc0.61, val_loss0.6559\n","Epoch 75, acc0.62, loss 0.6518, tol 0, val_acc0.65, val_loss0.6539\n","Epoch 76, acc0.66, loss 0.6423, tol 0, val_acc0.63, val_loss0.6484\n","Epoch 77, acc0.66, loss 0.6444, tol 0, val_acc0.64, val_loss0.6476\n","Epoch 78, acc0.63, loss 0.6553, tol 0, val_acc0.65, val_loss0.6465\n","Epoch 79, acc0.61, loss 0.6520, tol 0, val_acc0.55, val_loss0.6665\n","Epoch 80, acc0.62, loss 0.6442, tol 0, val_acc0.66, val_loss0.6464 -- checkpoint saved\n","Epoch 81, acc0.62, loss 0.6476, tol 0, val_acc0.65, val_loss0.6430\n","Epoch 82, acc0.65, loss 0.6387, tol 0, val_acc0.64, val_loss0.6445\n","Epoch 83, acc0.65, loss 0.6415, tol 0, val_acc0.61, val_loss0.6481\n","Epoch 84, acc0.64, loss 0.6400, tol 0, val_acc0.66, val_loss0.6400 -- checkpoint saved\n","Epoch 85, acc0.63, loss 0.6448, tol 0, val_acc0.63, val_loss0.6526\n","Epoch 86, acc0.64, loss 0.6390, tol 0, val_acc0.60, val_loss0.6479\n","Epoch 87, acc0.65, loss 0.6395, tol 0, val_acc0.65, val_loss0.6371\n","Epoch 88, acc0.65, loss 0.6404, tol 0, val_acc0.65, val_loss0.6439\n","Epoch 89, acc0.64, loss 0.6354, tol 0, val_acc0.62, val_loss0.6405\n","Epoch 90, acc0.63, loss 0.6354, tol 0, val_acc0.67, val_loss0.6345 -- checkpoint saved\n","Epoch 91, acc0.65, loss 0.6417, tol 0, val_acc0.66, val_loss0.6422\n","Epoch 92, acc0.66, loss 0.6346, tol 0, val_acc0.66, val_loss0.6325\n","Epoch 93, acc0.66, loss 0.6294, tol 0, val_acc0.67, val_loss0.6315\n","Epoch 94, acc0.66, loss 0.6285, tol 0, val_acc0.66, val_loss0.6319\n","Epoch 95, acc0.67, loss 0.6256, tol 0, val_acc0.68, val_loss0.6325\n","Epoch 96, acc0.68, loss 0.6207, tol 0, val_acc0.64, val_loss0.6312\n","Epoch 97, acc0.68, loss 0.6184, tol 0, val_acc0.64, val_loss0.6299\n","Epoch 98, acc0.66, loss 0.6315, tol 0, val_acc0.62, val_loss0.6473\n","Epoch 99, acc0.68, loss 0.6186, tol 0, val_acc0.66, val_loss0.6362\n","Epoch 100, acc0.66, loss 0.6236, tol 0, val_acc0.67, val_loss0.6246\n","Epoch 101, acc0.68, loss 0.6145, tol 0, val_acc0.68, val_loss0.6298\n","Epoch 102, acc0.67, loss 0.6193, tol 0, val_acc0.68, val_loss0.6261\n","Epoch 103, acc0.69, loss 0.6154, tol 0, val_acc0.67, val_loss0.6219\n","Epoch 104, acc0.68, loss 0.6118, tol 0, val_acc0.64, val_loss0.6229\n","Epoch 105, acc0.69, loss 0.6133, tol 0, val_acc0.66, val_loss0.6198\n","Epoch 106, acc0.68, loss 0.6169, tol 0, val_acc0.67, val_loss0.6183\n","Epoch 107, acc0.68, loss 0.6090, tol 0, val_acc0.64, val_loss0.6210\n","Epoch 108, acc0.69, loss 0.6002, tol 0, val_acc0.63, val_loss0.6273\n","Epoch 109, acc0.70, loss 0.6006, tol 0, val_acc0.65, val_loss0.6186\n","Epoch 110, acc0.69, loss 0.6002, tol 0, val_acc0.68, val_loss0.6150 -- checkpoint saved\n","Epoch 111, acc0.69, loss 0.6084, tol 0, val_acc0.68, val_loss0.6239\n","Epoch 112, acc0.67, loss 0.6073, tol 0, val_acc0.69, val_loss0.6141 -- checkpoint saved\n","Epoch 113, acc0.67, loss 0.6145, tol 0, val_acc0.66, val_loss0.6139\n","Epoch 114, acc0.70, loss 0.5904, tol 0, val_acc0.68, val_loss0.6140\n","Epoch 115, acc0.68, loss 0.5999, tol 0, val_acc0.68, val_loss0.6114\n","Epoch 116, acc0.70, loss 0.5926, tol 0, val_acc0.69, val_loss0.6091 -- checkpoint saved\n","Epoch 117, acc0.71, loss 0.5881, tol 0, val_acc0.67, val_loss0.6093\n","Epoch 118, acc0.69, loss 0.5976, tol 0, val_acc0.68, val_loss0.6065\n","Epoch 119, acc0.68, loss 0.6065, tol 0, val_acc0.67, val_loss0.6194\n","Epoch 120, acc0.71, loss 0.5943, tol 0, val_acc0.69, val_loss0.6067\n","Epoch 121, acc0.71, loss 0.5857, tol 0, val_acc0.68, val_loss0.6145\n","Epoch 122, acc0.69, loss 0.5999, tol 0, val_acc0.70, val_loss0.6018 -- checkpoint saved\n","Epoch 123, acc0.72, loss 0.5805, tol 0, val_acc0.69, val_loss0.6011\n","Epoch 124, acc0.70, loss 0.5807, tol 0, val_acc0.68, val_loss0.5999\n","Epoch 125, acc0.69, loss 0.5955, tol 0, val_acc0.70, val_loss0.6030\n","Epoch 126, acc0.72, loss 0.5819, tol 0, val_acc0.70, val_loss0.5988 -- checkpoint saved\n","Epoch 127, acc0.71, loss 0.5792, tol 0, val_acc0.65, val_loss0.6033\n","Epoch 128, acc0.70, loss 0.5855, tol 0, val_acc0.68, val_loss0.5970\n","Epoch 129, acc0.71, loss 0.5825, tol 0, val_acc0.68, val_loss0.5958\n","Epoch 130, acc0.72, loss 0.5709, tol 0, val_acc0.70, val_loss0.5949\n","Epoch 131, acc0.71, loss 0.5815, tol 0, val_acc0.70, val_loss0.5980\n","Epoch 132, acc0.72, loss 0.5728, tol 0, val_acc0.66, val_loss0.6291\n","Epoch 133, acc0.70, loss 0.5821, tol 0, val_acc0.70, val_loss0.5980\n","Epoch 134, acc0.71, loss 0.5761, tol 0, val_acc0.68, val_loss0.5895\n","Epoch 135, acc0.70, loss 0.5860, tol 0, val_acc0.66, val_loss0.5919\n","Epoch 136, acc0.73, loss 0.5642, tol 0, val_acc0.70, val_loss0.5874 -- checkpoint saved\n","Epoch 137, acc0.73, loss 0.5596, tol 0, val_acc0.66, val_loss0.5925\n","Epoch 138, acc0.72, loss 0.5616, tol 0, val_acc0.70, val_loss0.5882\n","Epoch 139, acc0.73, loss 0.5639, tol 0, val_acc0.68, val_loss0.5869\n","Epoch 140, acc0.74, loss 0.5603, tol 0, val_acc0.68, val_loss0.5859\n","Epoch 141, acc0.72, loss 0.5673, tol 0, val_acc0.71, val_loss0.5899\n","Epoch 142, acc0.71, loss 0.5762, tol 0, val_acc0.71, val_loss0.5826\n","Epoch 143, acc0.75, loss 0.5588, tol 0, val_acc0.70, val_loss0.5799\n","Epoch 144, acc0.71, loss 0.5727, tol 0, val_acc0.69, val_loss0.5792\n","Epoch 145, acc0.73, loss 0.5517, tol 0, val_acc0.68, val_loss0.5819\n","Epoch 146, acc0.76, loss 0.5457, tol 0, val_acc0.71, val_loss0.5802\n","Epoch 147, acc0.72, loss 0.5624, tol 0, val_acc0.70, val_loss0.5801\n","Epoch 148, acc0.71, loss 0.5769, tol 0, val_acc0.71, val_loss0.5865\n","Epoch 149, acc0.73, loss 0.5549, tol 0, val_acc0.70, val_loss0.5771\n","Epoch 150, acc0.72, loss 0.5637, tol 0, val_acc0.69, val_loss0.5751\n","Epoch 151, acc0.73, loss 0.5547, tol 0, val_acc0.70, val_loss0.5739\n","Epoch 152, acc0.72, loss 0.5520, tol 0, val_acc0.68, val_loss0.5743\n","Epoch 153, acc0.73, loss 0.5538, tol 0, val_acc0.67, val_loss0.5800\n","Epoch 154, acc0.73, loss 0.5595, tol 0, val_acc0.69, val_loss0.5774\n","Epoch 155, acc0.74, loss 0.5517, tol 0, val_acc0.69, val_loss0.5744\n","Epoch 156, acc0.75, loss 0.5439, tol 0, val_acc0.69, val_loss0.5713\n","Epoch 157, acc0.74, loss 0.5377, tol 0, val_acc0.69, val_loss0.5755\n","Epoch 158, acc0.75, loss 0.5378, tol 0, val_acc0.68, val_loss0.5770\n","Epoch 159, acc0.74, loss 0.5409, tol 0, val_acc0.68, val_loss0.5786\n","Epoch 160, acc0.73, loss 0.5555, tol 0, val_acc0.69, val_loss0.5679\n","Epoch 161, acc0.74, loss 0.5465, tol 0, val_acc0.71, val_loss0.5660\n","Epoch 162, acc0.73, loss 0.5494, tol 0, val_acc0.68, val_loss0.5737\n","Epoch 163, acc0.73, loss 0.5539, tol 0, val_acc0.69, val_loss0.5664\n","Epoch 164, acc0.75, loss 0.5281, tol 0, val_acc0.69, val_loss0.5641\n","Epoch 165, acc0.76, loss 0.5318, tol 0, val_acc0.70, val_loss0.5611\n","Epoch 166, acc0.72, loss 0.5544, tol 0, val_acc0.69, val_loss0.5672\n","Epoch 167, acc0.73, loss 0.5473, tol 0, val_acc0.70, val_loss0.5597\n","Epoch 168, acc0.73, loss 0.5452, tol 0, val_acc0.69, val_loss0.5631\n","Epoch 169, acc0.74, loss 0.5407, tol 0, val_acc0.72, val_loss0.5578 -- checkpoint saved\n","Epoch 170, acc0.74, loss 0.5403, tol 0, val_acc0.70, val_loss0.5542\n","Epoch 171, acc0.74, loss 0.5348, tol 0, val_acc0.71, val_loss0.5543\n","Epoch 172, acc0.74, loss 0.5326, tol 0, val_acc0.72, val_loss0.5568\n","Epoch 173, acc0.76, loss 0.5231, tol 0, val_acc0.70, val_loss0.5520\n","Epoch 174, acc0.76, loss 0.5231, tol 0, val_acc0.71, val_loss0.5499\n","Epoch 175, acc0.75, loss 0.5445, tol 0, val_acc0.72, val_loss0.5490 -- checkpoint saved\n","Epoch 176, acc0.74, loss 0.5326, tol 0, val_acc0.72, val_loss0.5492\n","Epoch 177, acc0.73, loss 0.5483, tol 0, val_acc0.69, val_loss0.5575\n","Epoch 178, acc0.76, loss 0.5259, tol 0, val_acc0.73, val_loss0.5500\n","Epoch 179, acc0.76, loss 0.5183, tol 0, val_acc0.71, val_loss0.5423\n","Epoch 180, acc0.77, loss 0.5213, tol 0, val_acc0.72, val_loss0.5436\n","Epoch 181, acc0.74, loss 0.5222, tol 0, val_acc0.73, val_loss0.5464\n","Epoch 182, acc0.75, loss 0.5301, tol 0, val_acc0.73, val_loss0.5464\n","Epoch 183, acc0.79, loss 0.5019, tol 0, val_acc0.72, val_loss0.5409\n","Epoch 184, acc0.76, loss 0.5122, tol 0, val_acc0.73, val_loss0.5451\n","Epoch 185, acc0.75, loss 0.5149, tol 0, val_acc0.73, val_loss0.5382\n","Epoch 186, acc0.80, loss 0.4882, tol 0, val_acc0.72, val_loss0.5399\n","Epoch 187, acc0.76, loss 0.5161, tol 0, val_acc0.74, val_loss0.5351 -- checkpoint saved\n","Epoch 188, acc0.75, loss 0.5163, tol 0, val_acc0.74, val_loss0.5343\n","Epoch 189, acc0.76, loss 0.5128, tol 0, val_acc0.70, val_loss0.5449\n","Epoch 190, acc0.78, loss 0.5000, tol 0, val_acc0.74, val_loss0.5314\n","Epoch 191, acc0.77, loss 0.5077, tol 0, val_acc0.73, val_loss0.5331\n","Epoch 192, acc0.77, loss 0.5097, tol 0, val_acc0.73, val_loss0.5285\n","Epoch 193, acc0.77, loss 0.5029, tol 0, val_acc0.73, val_loss0.5271\n","Epoch 194, acc0.76, loss 0.4987, tol 0, val_acc0.75, val_loss0.5368\n","Epoch 195, acc0.77, loss 0.5077, tol 0, val_acc0.74, val_loss0.5262\n","Epoch 196, acc0.79, loss 0.4933, tol 0, val_acc0.73, val_loss0.5246\n","Epoch 197, acc0.76, loss 0.5016, tol 0, val_acc0.73, val_loss0.5224\n","Epoch 198, acc0.77, loss 0.5024, tol 0, val_acc0.74, val_loss0.5225\n","Epoch 199, acc0.77, loss 0.5021, tol 0, val_acc0.74, val_loss0.5201\n","Epoch 200, acc0.78, loss 0.4966, tol 0, val_acc0.72, val_loss0.5307\n","Epoch 201, acc0.75, loss 0.5175, tol 0, val_acc0.74, val_loss0.5208\n","Epoch 202, acc0.80, loss 0.4777, tol 0, val_acc0.75, val_loss0.5199 -- checkpoint saved\n","Epoch 203, acc0.80, loss 0.4822, tol 0, val_acc0.75, val_loss0.5170 -- checkpoint saved\n","Epoch 204, acc0.78, loss 0.4852, tol 0, val_acc0.75, val_loss0.5155\n","Epoch 205, acc0.77, loss 0.4885, tol 0, val_acc0.76, val_loss0.5135 -- checkpoint saved\n","Epoch 206, acc0.78, loss 0.4846, tol 0, val_acc0.76, val_loss0.5127 -- checkpoint saved\n","Epoch 207, acc0.79, loss 0.4888, tol 0, val_acc0.75, val_loss0.5154\n","Epoch 208, acc0.79, loss 0.4764, tol 0, val_acc0.75, val_loss0.5103\n","Epoch 209, acc0.78, loss 0.4833, tol 0, val_acc0.76, val_loss0.5093\n","Epoch 210, acc0.78, loss 0.4862, tol 0, val_acc0.74, val_loss0.5112\n","Epoch 211, acc0.77, loss 0.4925, tol 0, val_acc0.76, val_loss0.5062\n","Epoch 212, acc0.80, loss 0.4742, tol 0, val_acc0.75, val_loss0.5057\n","Epoch 213, acc0.80, loss 0.4690, tol 0, val_acc0.75, val_loss0.5019\n","Epoch 214, acc0.77, loss 0.4843, tol 0, val_acc0.75, val_loss0.5041\n","Epoch 215, acc0.79, loss 0.4799, tol 0, val_acc0.76, val_loss0.5004 -- checkpoint saved\n","Epoch 216, acc0.79, loss 0.4735, tol 0, val_acc0.75, val_loss0.5021\n","Epoch 217, acc0.81, loss 0.4665, tol 0, val_acc0.76, val_loss0.4926 -- checkpoint saved\n","Epoch 218, acc0.79, loss 0.4707, tol 0, val_acc0.75, val_loss0.4933\n","Epoch 219, acc0.79, loss 0.4597, tol 0, val_acc0.75, val_loss0.4952\n","Epoch 220, acc0.80, loss 0.4610, tol 0, val_acc0.76, val_loss0.4945\n","Epoch 221, acc0.82, loss 0.4520, tol 0, val_acc0.75, val_loss0.4969\n","Epoch 222, acc0.80, loss 0.4565, tol 0, val_acc0.74, val_loss0.4955\n","Epoch 223, acc0.80, loss 0.4573, tol 0, val_acc0.76, val_loss0.4864\n","Epoch 224, acc0.80, loss 0.4636, tol 0, val_acc0.75, val_loss0.4882\n","Epoch 225, acc0.79, loss 0.4688, tol 0, val_acc0.75, val_loss0.4880\n","Epoch 226, acc0.80, loss 0.4542, tol 0, val_acc0.75, val_loss0.4888\n","Epoch 227, acc0.80, loss 0.4605, tol 0, val_acc0.77, val_loss0.4905\n","Epoch 228, acc0.81, loss 0.4654, tol 0, val_acc0.76, val_loss0.4798\n","Epoch 229, acc0.81, loss 0.4525, tol 0, val_acc0.77, val_loss0.4768 -- checkpoint saved\n","Epoch 230, acc0.81, loss 0.4459, tol 0, val_acc0.76, val_loss0.4811\n","Epoch 231, acc0.82, loss 0.4387, tol 0, val_acc0.76, val_loss0.4754\n","Epoch 232, acc0.80, loss 0.4665, tol 0, val_acc0.76, val_loss0.4776\n","Epoch 233, acc0.79, loss 0.4612, tol 0, val_acc0.77, val_loss0.4791\n","Epoch 234, acc0.82, loss 0.4452, tol 0, val_acc0.76, val_loss0.4768\n","Epoch 235, acc0.81, loss 0.4380, tol 0, val_acc0.76, val_loss0.4717\n","Epoch 236, acc0.81, loss 0.4419, tol 0, val_acc0.76, val_loss0.4765\n","Epoch 237, acc0.82, loss 0.4368, tol 0, val_acc0.77, val_loss0.4762\n","Epoch 238, acc0.81, loss 0.4586, tol 0, val_acc0.77, val_loss0.4716\n","Epoch 239, acc0.81, loss 0.4514, tol 0, val_acc0.77, val_loss0.4674 -- checkpoint saved\n","Epoch 240, acc0.81, loss 0.4418, tol 0, val_acc0.77, val_loss0.4656\n","Epoch 241, acc0.82, loss 0.4458, tol 0, val_acc0.77, val_loss0.4647\n","Epoch 242, acc0.81, loss 0.4476, tol 0, val_acc0.77, val_loss0.4600\n","Epoch 243, acc0.82, loss 0.4301, tol 0, val_acc0.79, val_loss0.4691\n","Epoch 244, acc0.81, loss 0.4429, tol 0, val_acc0.79, val_loss0.4702\n","Epoch 245, acc0.81, loss 0.4396, tol 0, val_acc0.78, val_loss0.4604\n","Epoch 246, acc0.81, loss 0.4571, tol 0, val_acc0.77, val_loss0.4571\n","Epoch 247, acc0.81, loss 0.4340, tol 0, val_acc0.77, val_loss0.4568\n","Epoch 248, acc0.81, loss 0.4458, tol 0, val_acc0.77, val_loss0.4661\n","Epoch 249, acc0.82, loss 0.4250, tol 0, val_acc0.78, val_loss0.4542\n","Epoch 250, acc0.82, loss 0.4242, tol 0, val_acc0.78, val_loss0.4563\n","Epoch 251, acc0.82, loss 0.4378, tol 0, val_acc0.78, val_loss0.4688\n","Epoch 252, acc0.81, loss 0.4412, tol 0, val_acc0.78, val_loss0.4589\n","Epoch 253, acc0.82, loss 0.4307, tol 0, val_acc0.78, val_loss0.4491\n","Epoch 254, acc0.83, loss 0.4222, tol 0, val_acc0.80, val_loss0.4475 -- checkpoint saved\n","Epoch 255, acc0.83, loss 0.4077, tol 0, val_acc0.80, val_loss0.4568\n","Epoch 256, acc0.83, loss 0.4043, tol 0, val_acc0.79, val_loss0.4521\n","Epoch 257, acc0.83, loss 0.4110, tol 0, val_acc0.79, val_loss0.4517\n","Epoch 258, acc0.82, loss 0.4210, tol 0, val_acc0.79, val_loss0.4547\n","Epoch 259, acc0.83, loss 0.4207, tol 0, val_acc0.79, val_loss0.4653\n","Epoch 260, acc0.82, loss 0.4073, tol 0, val_acc0.79, val_loss0.4463\n","Epoch 261, acc0.84, loss 0.4041, tol 0, val_acc0.79, val_loss0.4589\n","Epoch 262, acc0.81, loss 0.4196, tol 0, val_acc0.79, val_loss0.4653\n","Epoch 263, acc0.84, loss 0.3923, tol 0, val_acc0.79, val_loss0.4476\n","Epoch 264, acc0.83, loss 0.4139, tol 0, val_acc0.79, val_loss0.4460\n","Epoch 265, acc0.84, loss 0.4039, tol 0, val_acc0.79, val_loss0.4496\n","Epoch 266, acc0.82, loss 0.4169, tol 0, val_acc0.79, val_loss0.4456\n","Epoch 267, acc0.83, loss 0.4093, tol 0, val_acc0.79, val_loss0.4465\n","Epoch 268, acc0.83, loss 0.4136, tol 0, val_acc0.80, val_loss0.4420\n","Epoch 269, acc0.85, loss 0.3851, tol 0, val_acc0.79, val_loss0.4446\n","Epoch 270, acc0.82, loss 0.4148, tol 0, val_acc0.79, val_loss0.4423\n","Epoch 271, acc0.83, loss 0.4019, tol 0, val_acc0.79, val_loss0.4490\n","Epoch 272, acc0.83, loss 0.4067, tol 0, val_acc0.79, val_loss0.4569\n","Epoch 273, acc0.83, loss 0.3988, tol 0, val_acc0.79, val_loss0.4461\n","Epoch 274, acc0.84, loss 0.3851, tol 0, val_acc0.79, val_loss0.4392\n","Epoch 275, acc0.83, loss 0.3999, tol 0, val_acc0.79, val_loss0.4407\n","Epoch 276, acc0.83, loss 0.3908, tol 0, val_acc0.80, val_loss0.4442\n","Epoch 277, acc0.84, loss 0.3884, tol 0, val_acc0.80, val_loss0.4335 -- checkpoint saved\n","Epoch 278, acc0.84, loss 0.3877, tol 0, val_acc0.79, val_loss0.4354\n","Epoch 279, acc0.83, loss 0.4071, tol 0, val_acc0.80, val_loss0.4507\n","Epoch 280, acc0.82, loss 0.4028, tol 0, val_acc0.80, val_loss0.4554\n","Epoch 281, acc0.84, loss 0.3948, tol 0, val_acc0.79, val_loss0.4490\n","Epoch 282, acc0.82, loss 0.4156, tol 0, val_acc0.81, val_loss0.4262 -- checkpoint saved\n","Epoch 283, acc0.85, loss 0.3801, tol 0, val_acc0.80, val_loss0.4315\n","Epoch 284, acc0.84, loss 0.3748, tol 0, val_acc0.80, val_loss0.4277\n","Epoch 285, acc0.84, loss 0.3896, tol 0, val_acc0.81, val_loss0.4258\n","Epoch 286, acc0.84, loss 0.3992, tol 0, val_acc0.81, val_loss0.4349\n","Epoch 287, acc0.84, loss 0.3854, tol 0, val_acc0.81, val_loss0.4251\n","Epoch 288, acc0.83, loss 0.4099, tol 0, val_acc0.81, val_loss0.4251\n","Epoch 289, acc0.84, loss 0.3861, tol 0, val_acc0.81, val_loss0.4194 -- checkpoint saved\n","Epoch 290, acc0.84, loss 0.3912, tol 0, val_acc0.82, val_loss0.4205\n","Epoch 291, acc0.84, loss 0.3933, tol 0, val_acc0.82, val_loss0.4244\n","Epoch 292, acc0.85, loss 0.3791, tol 0, val_acc0.81, val_loss0.4189\n","Epoch 293, acc0.84, loss 0.3814, tol 0, val_acc0.80, val_loss0.4220\n","Epoch 294, acc0.84, loss 0.3919, tol 0, val_acc0.80, val_loss0.4244\n","Epoch 295, acc0.84, loss 0.3742, tol 0, val_acc0.80, val_loss0.4211\n","Epoch 296, acc0.82, loss 0.3960, tol 0, val_acc0.81, val_loss0.4188\n","Epoch 297, acc0.86, loss 0.3761, tol 0, val_acc0.81, val_loss0.4193\n","Epoch 298, acc0.84, loss 0.3837, tol 0, val_acc0.81, val_loss0.4181\n","Epoch 299, acc0.85, loss 0.3754, tol 0, val_acc0.81, val_loss0.4182\n","Epoch 300, acc0.85, loss 0.3590, tol 0, val_acc0.82, val_loss0.4203\n","Epoch 301, acc0.84, loss 0.3743, tol 0, val_acc0.80, val_loss0.4252\n","Epoch 302, acc0.85, loss 0.3771, tol 0, val_acc0.81, val_loss0.4157\n","Epoch 303, acc0.84, loss 0.3725, tol 0, val_acc0.81, val_loss0.4214\n","Epoch 304, acc0.83, loss 0.3908, tol 0, val_acc0.82, val_loss0.4186\n","Epoch 305, acc0.84, loss 0.3769, tol 0, val_acc0.81, val_loss0.4134\n","Epoch 306, acc0.83, loss 0.3810, tol 0, val_acc0.81, val_loss0.4237\n","Epoch 307, acc0.85, loss 0.3661, tol 0, val_acc0.81, val_loss0.4145\n","Epoch 308, acc0.84, loss 0.3654, tol 0, val_acc0.83, val_loss0.4114 -- checkpoint saved\n","Epoch 309, acc0.86, loss 0.3707, tol 0, val_acc0.81, val_loss0.4197\n","Epoch 310, acc0.84, loss 0.3734, tol 0, val_acc0.83, val_loss0.4136\n","Epoch 311, acc0.85, loss 0.3832, tol 0, val_acc0.82, val_loss0.4118\n","Epoch 312, acc0.85, loss 0.3624, tol 0, val_acc0.82, val_loss0.4104\n","Epoch 313, acc0.84, loss 0.3755, tol 0, val_acc0.81, val_loss0.4155\n","Epoch 314, acc0.83, loss 0.3785, tol 0, val_acc0.79, val_loss0.4418\n","Epoch 315, acc0.84, loss 0.3831, tol 0, val_acc0.79, val_loss0.4440\n","Epoch 316, acc0.86, loss 0.3615, tol 0, val_acc0.80, val_loss0.4098\n","Epoch 317, acc0.85, loss 0.3853, tol 0, val_acc0.81, val_loss0.4126\n","Epoch 318, acc0.84, loss 0.3699, tol 0, val_acc0.83, val_loss0.4047 -- checkpoint saved\n","Epoch 319, acc0.84, loss 0.3623, tol 0, val_acc0.81, val_loss0.4077\n","Epoch 320, acc0.85, loss 0.3566, tol 0, val_acc0.83, val_loss0.4030\n","Epoch 321, acc0.86, loss 0.3522, tol 0, val_acc0.82, val_loss0.4078\n","Epoch 322, acc0.86, loss 0.3479, tol 0, val_acc0.83, val_loss0.4039\n","Epoch 323, acc0.85, loss 0.3662, tol 0, val_acc0.82, val_loss0.4074\n","Epoch 324, acc0.86, loss 0.3400, tol 0, val_acc0.83, val_loss0.4015\n","Epoch 325, acc0.85, loss 0.3610, tol 0, val_acc0.82, val_loss0.4096\n","Epoch 326, acc0.86, loss 0.3468, tol 0, val_acc0.81, val_loss0.4303\n","Epoch 327, acc0.84, loss 0.3614, tol 0, val_acc0.81, val_loss0.4071\n","Epoch 328, acc0.85, loss 0.3497, tol 0, val_acc0.81, val_loss0.4029\n","Epoch 329, acc0.85, loss 0.3533, tol 0, val_acc0.83, val_loss0.3981\n","Epoch 330, acc0.85, loss 0.3547, tol 0, val_acc0.82, val_loss0.3991\n","Epoch 331, acc0.86, loss 0.3521, tol 0, val_acc0.83, val_loss0.4024\n","Epoch 332, acc0.87, loss 0.3262, tol 0, val_acc0.84, val_loss0.3965 -- checkpoint saved\n","Epoch 333, acc0.85, loss 0.3559, tol 0, val_acc0.84, val_loss0.3959 -- checkpoint saved\n","Epoch 334, acc0.85, loss 0.3562, tol 0, val_acc0.83, val_loss0.4027\n","Epoch 335, acc0.83, loss 0.3742, tol 0, val_acc0.83, val_loss0.4048\n","Epoch 336, acc0.85, loss 0.3548, tol 0, val_acc0.82, val_loss0.4070\n","Epoch 337, acc0.86, loss 0.3575, tol 0, val_acc0.83, val_loss0.3986\n","Epoch 338, acc0.85, loss 0.3533, tol 0, val_acc0.84, val_loss0.3968\n","Epoch 339, acc0.86, loss 0.3487, tol 0, val_acc0.83, val_loss0.3937\n","Epoch 340, acc0.87, loss 0.3382, tol 0, val_acc0.84, val_loss0.3895 -- checkpoint saved\n","Epoch 341, acc0.86, loss 0.3427, tol 0, val_acc0.83, val_loss0.3924\n","Epoch 342, acc0.86, loss 0.3446, tol 0, val_acc0.84, val_loss0.3984\n","Epoch 343, acc0.86, loss 0.3479, tol 0, val_acc0.84, val_loss0.3956\n","Epoch 344, acc0.86, loss 0.3298, tol 0, val_acc0.83, val_loss0.3931\n","Epoch 345, acc0.84, loss 0.3766, tol 0, val_acc0.83, val_loss0.3909\n","Epoch 346, acc0.87, loss 0.3314, tol 0, val_acc0.83, val_loss0.4072\n","Epoch 347, acc0.85, loss 0.3610, tol 0, val_acc0.83, val_loss0.4016\n","Epoch 348, acc0.86, loss 0.3533, tol 0, val_acc0.83, val_loss0.3936\n","Epoch 349, acc0.86, loss 0.3418, tol 0, val_acc0.84, val_loss0.3931\n","Epoch 350, acc0.87, loss 0.3484, tol 0, val_acc0.84, val_loss0.3858\n","Epoch 351, acc0.86, loss 0.3351, tol 0, val_acc0.84, val_loss0.3820\n","Epoch 352, acc0.87, loss 0.3325, tol 0, val_acc0.83, val_loss0.3828\n","Epoch 353, acc0.86, loss 0.3288, tol 0, val_acc0.83, val_loss0.3850\n","Epoch 354, acc0.84, loss 0.3588, tol 0, val_acc0.82, val_loss0.3863\n","Epoch 355, acc0.86, loss 0.3448, tol 0, val_acc0.84, val_loss0.3840\n","Epoch 356, acc0.85, loss 0.3449, tol 0, val_acc0.84, val_loss0.3818\n","Epoch 357, acc0.86, loss 0.3374, tol 0, val_acc0.84, val_loss0.3820\n","Epoch 358, acc0.86, loss 0.3400, tol 0, val_acc0.84, val_loss0.3851\n","Epoch 359, acc0.86, loss 0.3256, tol 0, val_acc0.84, val_loss0.3869\n","Epoch 360, acc0.86, loss 0.3363, tol 0, val_acc0.83, val_loss0.3812\n","Epoch 361, acc0.87, loss 0.3272, tol 0, val_acc0.83, val_loss0.3843\n","Epoch 362, acc0.87, loss 0.3276, tol 0, val_acc0.83, val_loss0.3755\n","Epoch 363, acc0.87, loss 0.3272, tol 0, val_acc0.83, val_loss0.3742\n","Epoch 364, acc0.87, loss 0.3316, tol 0, val_acc0.84, val_loss0.3779\n","Epoch 365, acc0.86, loss 0.3236, tol 0, val_acc0.85, val_loss0.3837\n","Epoch 366, acc0.87, loss 0.3249, tol 0, val_acc0.85, val_loss0.3785\n","Epoch 367, acc0.86, loss 0.3307, tol 0, val_acc0.84, val_loss0.3738\n","Epoch 368, acc0.87, loss 0.3199, tol 0, val_acc0.84, val_loss0.3763\n","Epoch 369, acc0.88, loss 0.3172, tol 0, val_acc0.84, val_loss0.3755\n","Epoch 370, acc0.85, loss 0.3382, tol 0, val_acc0.83, val_loss0.3803\n","Epoch 371, acc0.86, loss 0.3314, tol 0, val_acc0.84, val_loss0.3746\n","Epoch 372, acc0.87, loss 0.3166, tol 0, val_acc0.85, val_loss0.3772\n","Epoch 373, acc0.88, loss 0.3293, tol 0, val_acc0.86, val_loss0.3719 -- checkpoint saved\n","Epoch 374, acc0.88, loss 0.3187, tol 0, val_acc0.85, val_loss0.3710\n","Epoch 375, acc0.87, loss 0.3242, tol 0, val_acc0.85, val_loss0.3753\n","Epoch 376, acc0.87, loss 0.3178, tol 0, val_acc0.85, val_loss0.3690\n","Epoch 377, acc0.89, loss 0.3027, tol 0, val_acc0.84, val_loss0.3745\n","Epoch 378, acc0.87, loss 0.3147, tol 0, val_acc0.85, val_loss0.3723\n","Epoch 379, acc0.86, loss 0.3215, tol 0, val_acc0.85, val_loss0.3696\n","Epoch 380, acc0.87, loss 0.3145, tol 0, val_acc0.85, val_loss0.3744\n","Epoch 381, acc0.87, loss 0.3127, tol 0, val_acc0.85, val_loss0.3724\n","Epoch 382, acc0.87, loss 0.3213, tol 0, val_acc0.86, val_loss0.3713\n","Epoch 383, acc0.88, loss 0.3131, tol 0, val_acc0.84, val_loss0.3705\n","Epoch 384, acc0.87, loss 0.3081, tol 0, val_acc0.85, val_loss0.3760\n","Epoch 385, acc0.86, loss 0.3298, tol 0, val_acc0.84, val_loss0.3685\n","Epoch 386, acc0.87, loss 0.3170, tol 0, val_acc0.85, val_loss0.3698\n","Epoch 387, acc0.87, loss 0.3203, tol 0, val_acc0.85, val_loss0.3653\n","Epoch 388, acc0.86, loss 0.3232, tol 0, val_acc0.86, val_loss0.3667\n","Epoch 389, acc0.86, loss 0.3292, tol 0, val_acc0.84, val_loss0.3622\n","Epoch 390, acc0.86, loss 0.3267, tol 0, val_acc0.84, val_loss0.3624\n","Epoch 391, acc0.88, loss 0.2961, tol 0, val_acc0.85, val_loss0.3703\n","Epoch 392, acc0.87, loss 0.3169, tol 0, val_acc0.84, val_loss0.3642\n","Epoch 393, acc0.88, loss 0.2915, tol 0, val_acc0.85, val_loss0.3604\n","Epoch 394, acc0.87, loss 0.3068, tol 0, val_acc0.86, val_loss0.3595 -- checkpoint saved\n","Epoch 395, acc0.87, loss 0.3249, tol 0, val_acc0.85, val_loss0.3650\n","Epoch 396, acc0.86, loss 0.3261, tol 0, val_acc0.84, val_loss0.3622\n","Epoch 397, acc0.86, loss 0.3267, tol 0, val_acc0.85, val_loss0.3603\n","Epoch 398, acc0.87, loss 0.3128, tol 0, val_acc0.85, val_loss0.3560\n","Epoch 399, acc0.86, loss 0.3322, tol 0, val_acc0.84, val_loss0.3568\n","Epoch 400, acc0.87, loss 0.3048, tol 0, val_acc0.86, val_loss0.3573\n","Epoch 401, acc0.87, loss 0.3097, tol 0, val_acc0.85, val_loss0.3576\n","Epoch 402, acc0.87, loss 0.3206, tol 0, val_acc0.84, val_loss0.3575\n","Epoch 403, acc0.88, loss 0.2938, tol 0, val_acc0.85, val_loss0.3596\n","Epoch 404, acc0.86, loss 0.3133, tol 0, val_acc0.86, val_loss0.3562\n","Epoch 405, acc0.88, loss 0.2761, tol 0, val_acc0.84, val_loss0.3572\n","Epoch 406, acc0.88, loss 0.2873, tol 0, val_acc0.86, val_loss0.3559\n","Epoch 407, acc0.87, loss 0.2978, tol 0, val_acc0.85, val_loss0.3563\n","Epoch 408, acc0.86, loss 0.3148, tol 0, val_acc0.85, val_loss0.3572\n","Epoch 409, acc0.87, loss 0.3183, tol 0, val_acc0.85, val_loss0.3584\n","Epoch 410, acc0.88, loss 0.2997, tol 0, val_acc0.85, val_loss0.3586\n","Epoch 411, acc0.88, loss 0.3011, tol 0, val_acc0.85, val_loss0.3635\n","Epoch 412, acc0.88, loss 0.2914, tol 0, val_acc0.85, val_loss0.3542\n","Epoch 413, acc0.87, loss 0.3159, tol 0, val_acc0.85, val_loss0.3579\n","Epoch 414, acc0.87, loss 0.3145, tol 0, val_acc0.86, val_loss0.3581\n","Epoch 415, acc0.88, loss 0.2933, tol 0, val_acc0.86, val_loss0.3515 -- checkpoint saved\n","Epoch 416, acc0.87, loss 0.3155, tol 0, val_acc0.85, val_loss0.3655\n","Epoch 417, acc0.88, loss 0.3059, tol 0, val_acc0.85, val_loss0.3523\n","Epoch 418, acc0.87, loss 0.3097, tol 0, val_acc0.85, val_loss0.3555\n","Epoch 419, acc0.87, loss 0.3030, tol 0, val_acc0.85, val_loss0.3579\n","Epoch 420, acc0.90, loss 0.2790, tol 0, val_acc0.85, val_loss0.3529\n","Epoch 421, acc0.88, loss 0.2946, tol 0, val_acc0.86, val_loss0.3500\n","Epoch 422, acc0.87, loss 0.3154, tol 0, val_acc0.85, val_loss0.3502\n","Epoch 423, acc0.87, loss 0.3119, tol 0, val_acc0.86, val_loss0.3473\n","Epoch 424, acc0.86, loss 0.3186, tol 0, val_acc0.86, val_loss0.3537\n","Epoch 425, acc0.89, loss 0.2868, tol 0, val_acc0.86, val_loss0.3483\n","Epoch 426, acc0.88, loss 0.3049, tol 0, val_acc0.85, val_loss0.3564\n","Epoch 427, acc0.87, loss 0.3095, tol 0, val_acc0.85, val_loss0.3505\n","Epoch 428, acc0.88, loss 0.2970, tol 0, val_acc0.86, val_loss0.3465\n","Epoch 429, acc0.87, loss 0.3079, tol 0, val_acc0.86, val_loss0.3470\n","Epoch 430, acc0.87, loss 0.3049, tol 0, val_acc0.86, val_loss0.3467\n","Epoch 431, acc0.89, loss 0.2822, tol 0, val_acc0.86, val_loss0.3476\n","Epoch 432, acc0.87, loss 0.3142, tol 0, val_acc0.86, val_loss0.3473\n","Epoch 433, acc0.88, loss 0.2935, tol 0, val_acc0.86, val_loss0.3479\n","Epoch 434, acc0.86, loss 0.3154, tol 0, val_acc0.86, val_loss0.3482\n","Epoch 435, acc0.87, loss 0.3007, tol 0, val_acc0.86, val_loss0.3539\n","Epoch 436, acc0.89, loss 0.2903, tol 0, val_acc0.85, val_loss0.3512\n","Epoch 437, acc0.86, loss 0.3202, tol 0, val_acc0.86, val_loss0.3442\n","Epoch 438, acc0.89, loss 0.2925, tol 0, val_acc0.86, val_loss0.3457\n","Epoch 439, acc0.87, loss 0.3067, tol 0, val_acc0.87, val_loss0.3437 -- checkpoint saved\n","Epoch 440, acc0.88, loss 0.2919, tol 0, val_acc0.86, val_loss0.3453\n","Epoch 441, acc0.88, loss 0.2853, tol 0, val_acc0.86, val_loss0.3383\n","Epoch 442, acc0.86, loss 0.3050, tol 0, val_acc0.86, val_loss0.3445\n","Epoch 443, acc0.88, loss 0.2797, tol 0, val_acc0.85, val_loss0.3573\n","Epoch 444, acc0.88, loss 0.2883, tol 0, val_acc0.85, val_loss0.3681\n","Epoch 445, acc0.88, loss 0.2940, tol 0, val_acc0.84, val_loss0.3589\n","Epoch 446, acc0.87, loss 0.3010, tol 0, val_acc0.85, val_loss0.3503\n","Epoch 447, acc0.87, loss 0.2874, tol 0, val_acc0.87, val_loss0.3430\n","Epoch 448, acc0.89, loss 0.2725, tol 0, val_acc0.87, val_loss0.3459\n","Epoch 449, acc0.88, loss 0.2923, tol 0, val_acc0.87, val_loss0.3409\n","Epoch 450, acc0.88, loss 0.2892, tol 0, val_acc0.86, val_loss0.3407\n","Epoch 451, acc0.89, loss 0.2718, tol 0, val_acc0.86, val_loss0.3506\n","Epoch 452, acc0.88, loss 0.2898, tol 0, val_acc0.86, val_loss0.3471\n","Epoch 453, acc0.88, loss 0.2770, tol 0, val_acc0.86, val_loss0.3451\n","Epoch 454, acc0.88, loss 0.2798, tol 0, val_acc0.85, val_loss0.3504\n","Epoch 455, acc0.89, loss 0.2722, tol 0, val_acc0.85, val_loss0.3437\n","Epoch 456, acc0.89, loss 0.2731, tol 0, val_acc0.85, val_loss0.3462\n","Epoch 457, acc0.88, loss 0.2908, tol 0, val_acc0.86, val_loss0.3424\n","Epoch 458, acc0.90, loss 0.2617, tol 0, val_acc0.86, val_loss0.3380\n","Epoch 459, acc0.89, loss 0.2846, tol 0, val_acc0.86, val_loss0.3362\n","Epoch 460, acc0.88, loss 0.2766, tol 0, val_acc0.86, val_loss0.3379\n","Epoch 461, acc0.88, loss 0.2895, tol 0, val_acc0.87, val_loss0.3370\n","Epoch 462, acc0.88, loss 0.2857, tol 0, val_acc0.86, val_loss0.3382\n","Epoch 463, acc0.88, loss 0.2759, tol 0, val_acc0.87, val_loss0.3328\n","Epoch 464, acc0.88, loss 0.2745, tol 0, val_acc0.87, val_loss0.3352\n","Epoch 465, acc0.89, loss 0.2715, tol 0, val_acc0.87, val_loss0.3370\n","Epoch 466, acc0.89, loss 0.2752, tol 0, val_acc0.87, val_loss0.3330\n","Epoch 467, acc0.89, loss 0.2629, tol 0, val_acc0.86, val_loss0.3348\n","Epoch 468, acc0.89, loss 0.2699, tol 0, val_acc0.88, val_loss0.3322 -- checkpoint saved\n","Epoch 469, acc0.89, loss 0.2717, tol 0, val_acc0.88, val_loss0.3265 -- checkpoint saved\n","Epoch 470, acc0.88, loss 0.2937, tol 0, val_acc0.88, val_loss0.3288\n","Epoch 471, acc0.89, loss 0.2611, tol 0, val_acc0.87, val_loss0.3274\n","Epoch 472, acc0.88, loss 0.2752, tol 0, val_acc0.87, val_loss0.3243\n","Epoch 473, acc0.90, loss 0.2603, tol 0, val_acc0.88, val_loss0.3245\n","Epoch 474, acc0.89, loss 0.2675, tol 0, val_acc0.88, val_loss0.3227\n","Epoch 475, acc0.90, loss 0.2588, tol 0, val_acc0.88, val_loss0.3251\n","Epoch 476, acc0.88, loss 0.2771, tol 0, val_acc0.87, val_loss0.3250\n","Epoch 477, acc0.89, loss 0.2695, tol 0, val_acc0.88, val_loss0.3229\n","Epoch 478, acc0.89, loss 0.2677, tol 0, val_acc0.87, val_loss0.3374\n","Epoch 479, acc0.89, loss 0.2684, tol 0, val_acc0.87, val_loss0.3236\n","Epoch 480, acc0.89, loss 0.2662, tol 0, val_acc0.88, val_loss0.3229\n","Epoch 481, acc0.90, loss 0.2587, tol 0, val_acc0.88, val_loss0.3179 -- checkpoint saved\n","Epoch 482, acc0.89, loss 0.2837, tol 0, val_acc0.88, val_loss0.3275\n","Epoch 483, acc0.91, loss 0.2446, tol 0, val_acc0.87, val_loss0.3344\n","Epoch 484, acc0.89, loss 0.2688, tol 0, val_acc0.88, val_loss0.3212\n","Epoch 485, acc0.89, loss 0.2748, tol 0, val_acc0.88, val_loss0.3272\n","Epoch 486, acc0.88, loss 0.2649, tol 0, val_acc0.88, val_loss0.3187\n","Epoch 487, acc0.88, loss 0.2815, tol 0, val_acc0.87, val_loss0.3187\n","Epoch 488, acc0.88, loss 0.2809, tol 0, val_acc0.88, val_loss0.3165\n","Epoch 489, acc0.90, loss 0.2526, tol 0, val_acc0.87, val_loss0.3294\n","Epoch 490, acc0.88, loss 0.2885, tol 0, val_acc0.88, val_loss0.3171\n","Epoch 491, acc0.90, loss 0.2546, tol 0, val_acc0.88, val_loss0.3170\n","Epoch 492, acc0.89, loss 0.2776, tol 0, val_acc0.88, val_loss0.3166\n","Epoch 493, acc0.89, loss 0.2704, tol 0, val_acc0.88, val_loss0.3159 -- checkpoint saved\n","Epoch 494, acc0.90, loss 0.2507, tol 0, val_acc0.88, val_loss0.3159\n","Epoch 495, acc0.90, loss 0.2630, tol 0, val_acc0.88, val_loss0.3190\n","Epoch 496, acc0.91, loss 0.2561, tol 0, val_acc0.88, val_loss0.3193\n","Epoch 497, acc0.89, loss 0.2627, tol 0, val_acc0.88, val_loss0.3129\n","Epoch 498, acc0.90, loss 0.2658, tol 0, val_acc0.88, val_loss0.3207\n","Epoch 499, acc0.89, loss 0.2698, tol 0, val_acc0.89, val_loss0.3150\n","Epoch 500, acc0.90, loss 0.2507, tol 0, val_acc0.88, val_loss0.3170\n","Epoch 501, acc0.89, loss 0.2545, tol 1, val_acc0.88, val_loss0.3145\n","Reach Max Epoch Number\n","+------------------------+------------+\n","|        Modules         | Parameters |\n","+------------------------+------------+\n","| layers.0.0.fc_Q.weight |     45     |\n","| layers.0.0.fc_K.weight |     45     |\n","| layers.0.0.fc_V.weight |     45     |\n","| layers.0.1.fc_Q.weight |     45     |\n","| layers.0.1.fc_K.weight |     45     |\n","| layers.0.1.fc_V.weight |     45     |\n","| layers.1.0.fc_Q.weight |    162     |\n","| layers.1.0.fc_K.weight |    162     |\n","| layers.1.0.fc_V.weight |    162     |\n","| layers.1.1.fc_Q.weight |    162     |\n","| layers.1.1.fc_K.weight |    162     |\n","| layers.1.1.fc_V.weight |    162     |\n","|    classify.weight     |     36     |\n","|     classify.bias      |     2      |\n","+------------------------+------------+\n","Total Trainable Params: 1280\n","Epoch 0, acc0.47, loss 0.8616, tol 0, val_acc0.46, val_loss0.7687 -- checkpoint saved\n","Epoch 1, acc0.47, loss 0.7532, tol 0, val_acc0.47, val_loss0.7401 -- checkpoint saved\n","Epoch 2, acc0.47, loss 0.7282, tol 0, val_acc0.46, val_loss0.7341\n","Epoch 3, acc0.46, loss 0.7272, tol 0, val_acc0.47, val_loss0.7291 -- checkpoint saved\n","Epoch 4, acc0.46, loss 0.7249, tol 0, val_acc0.44, val_loss0.7243\n","Epoch 5, acc0.45, loss 0.7199, tol 0, val_acc0.45, val_loss0.7199\n","Epoch 6, acc0.47, loss 0.7197, tol 0, val_acc0.45, val_loss0.7164\n","Epoch 7, acc0.47, loss 0.7091, tol 0, val_acc0.46, val_loss0.7144\n","Epoch 8, acc0.46, loss 0.7125, tol 0, val_acc0.45, val_loss0.7116\n","Epoch 9, acc0.46, loss 0.7071, tol 0, val_acc0.45, val_loss0.7087\n","Epoch 10, acc0.45, loss 0.7080, tol 0, val_acc0.46, val_loss0.7071\n","Epoch 11, acc0.47, loss 0.7013, tol 0, val_acc0.47, val_loss0.7045 -- checkpoint saved\n","Epoch 12, acc0.49, loss 0.7000, tol 0, val_acc0.47, val_loss0.7030 -- checkpoint saved\n","Epoch 13, acc0.49, loss 0.7013, tol 0, val_acc0.48, val_loss0.7018 -- checkpoint saved\n","Epoch 14, acc0.48, loss 0.7015, tol 0, val_acc0.48, val_loss0.7007 -- checkpoint saved\n","Epoch 15, acc0.47, loss 0.7014, tol 0, val_acc0.47, val_loss0.6991\n","Epoch 16, acc0.47, loss 0.6976, tol 0, val_acc0.48, val_loss0.6981 -- checkpoint saved\n","Epoch 17, acc0.49, loss 0.6962, tol 0, val_acc0.51, val_loss0.6987\n","Epoch 18, acc0.47, loss 0.7011, tol 0, val_acc0.49, val_loss0.7004\n","Epoch 19, acc0.50, loss 0.6962, tol 0, val_acc0.50, val_loss0.6951\n","Epoch 20, acc0.49, loss 0.6943, tol 0, val_acc0.48, val_loss0.6947\n","Epoch 21, acc0.51, loss 0.6912, tol 0, val_acc0.51, val_loss0.6929 -- checkpoint saved\n","Epoch 22, acc0.52, loss 0.6925, tol 0, val_acc0.50, val_loss0.6996\n","Epoch 23, acc0.50, loss 0.6944, tol 0, val_acc0.52, val_loss0.6932\n","Epoch 24, acc0.51, loss 0.6931, tol 0, val_acc0.54, val_loss0.6912 -- checkpoint saved\n","Epoch 25, acc0.54, loss 0.6909, tol 0, val_acc0.50, val_loss0.6946\n","Epoch 26, acc0.50, loss 0.6977, tol 0, val_acc0.54, val_loss0.6897 -- checkpoint saved\n","Epoch 27, acc0.53, loss 0.6906, tol 0, val_acc0.50, val_loss0.6951\n","Epoch 28, acc0.49, loss 0.7008, tol 0, val_acc0.50, val_loss0.6947\n","Epoch 29, acc0.55, loss 0.6879, tol 0, val_acc0.52, val_loss0.6908\n","Epoch 30, acc0.54, loss 0.6895, tol 0, val_acc0.53, val_loss0.6886\n","Epoch 31, acc0.56, loss 0.6870, tol 0, val_acc0.56, val_loss0.6867 -- checkpoint saved\n","Epoch 32, acc0.54, loss 0.6892, tol 0, val_acc0.53, val_loss0.6879\n","Epoch 33, acc0.54, loss 0.6907, tol 0, val_acc0.56, val_loss0.6855 -- checkpoint saved\n","Epoch 34, acc0.59, loss 0.6843, tol 0, val_acc0.59, val_loss0.6844 -- checkpoint saved\n","Epoch 35, acc0.56, loss 0.6862, tol 0, val_acc0.59, val_loss0.6836 -- checkpoint saved\n","Epoch 36, acc0.57, loss 0.6857, tol 0, val_acc0.58, val_loss0.6827\n","Epoch 37, acc0.56, loss 0.6866, tol 0, val_acc0.56, val_loss0.6840\n","Epoch 38, acc0.54, loss 0.6854, tol 0, val_acc0.53, val_loss0.6860\n","Epoch 39, acc0.55, loss 0.6850, tol 0, val_acc0.58, val_loss0.6812\n","Epoch 40, acc0.59, loss 0.6793, tol 0, val_acc0.58, val_loss0.6809\n","Epoch 41, acc0.56, loss 0.6815, tol 0, val_acc0.59, val_loss0.6800 -- checkpoint saved\n","Epoch 42, acc0.58, loss 0.6799, tol 0, val_acc0.58, val_loss0.6791\n","Epoch 43, acc0.57, loss 0.6774, tol 0, val_acc0.51, val_loss0.6874\n","Epoch 44, acc0.54, loss 0.6886, tol 0, val_acc0.58, val_loss0.6774\n","Epoch 45, acc0.58, loss 0.6804, tol 0, val_acc0.59, val_loss0.6776\n","Epoch 46, acc0.59, loss 0.6753, tol 0, val_acc0.59, val_loss0.6751\n","Epoch 47, acc0.58, loss 0.6768, tol 0, val_acc0.60, val_loss0.6773\n","Epoch 48, acc0.59, loss 0.6751, tol 0, val_acc0.60, val_loss0.6732 -- checkpoint saved\n","Epoch 49, acc0.59, loss 0.6763, tol 0, val_acc0.61, val_loss0.6727 -- checkpoint saved\n","Epoch 50, acc0.62, loss 0.6691, tol 0, val_acc0.60, val_loss0.6707\n","Epoch 51, acc0.61, loss 0.6705, tol 0, val_acc0.60, val_loss0.6696\n","Epoch 52, acc0.60, loss 0.6678, tol 0, val_acc0.55, val_loss0.6826\n","Epoch 53, acc0.57, loss 0.6714, tol 0, val_acc0.61, val_loss0.6680 -- checkpoint saved\n","Epoch 54, acc0.62, loss 0.6639, tol 0, val_acc0.62, val_loss0.6650 -- checkpoint saved\n","Epoch 55, acc0.60, loss 0.6644, tol 0, val_acc0.58, val_loss0.6720\n","Epoch 56, acc0.61, loss 0.6636, tol 0, val_acc0.61, val_loss0.6651\n","Epoch 57, acc0.61, loss 0.6571, tol 0, val_acc0.58, val_loss0.6730\n","Epoch 58, acc0.60, loss 0.6688, tol 0, val_acc0.61, val_loss0.6633\n","Epoch 59, acc0.62, loss 0.6552, tol 0, val_acc0.63, val_loss0.6546 -- checkpoint saved\n","Epoch 60, acc0.63, loss 0.6541, tol 0, val_acc0.64, val_loss0.6526 -- checkpoint saved\n","Epoch 61, acc0.65, loss 0.6433, tol 0, val_acc0.65, val_loss0.6523 -- checkpoint saved\n","Epoch 62, acc0.64, loss 0.6370, tol 0, val_acc0.62, val_loss0.6558\n","Epoch 63, acc0.67, loss 0.6332, tol 0, val_acc0.61, val_loss0.6627\n","Epoch 64, acc0.66, loss 0.6343, tol 0, val_acc0.67, val_loss0.6328 -- checkpoint saved\n","Epoch 65, acc0.69, loss 0.6137, tol 0, val_acc0.67, val_loss0.6253 -- checkpoint saved\n","Epoch 66, acc0.70, loss 0.5978, tol 0, val_acc0.68, val_loss0.6144 -- checkpoint saved\n","Epoch 67, acc0.72, loss 0.5817, tol 0, val_acc0.70, val_loss0.6003 -- checkpoint saved\n","Epoch 68, acc0.74, loss 0.5607, tol 0, val_acc0.67, val_loss0.6048\n","Epoch 69, acc0.74, loss 0.5522, tol 0, val_acc0.72, val_loss0.5626 -- checkpoint saved\n","Epoch 70, acc0.75, loss 0.5198, tol 0, val_acc0.72, val_loss0.5599 -- checkpoint saved\n","Epoch 71, acc0.76, loss 0.5187, tol 0, val_acc0.72, val_loss0.5440 -- checkpoint saved\n","Epoch 72, acc0.76, loss 0.5173, tol 0, val_acc0.75, val_loss0.5208 -- checkpoint saved\n","Epoch 73, acc0.76, loss 0.5051, tol 0, val_acc0.76, val_loss0.5087 -- checkpoint saved\n","Epoch 74, acc0.78, loss 0.4877, tol 0, val_acc0.77, val_loss0.4980 -- checkpoint saved\n","Epoch 75, acc0.81, loss 0.4596, tol 0, val_acc0.79, val_loss0.4806 -- checkpoint saved\n","Epoch 76, acc0.81, loss 0.4563, tol 0, val_acc0.78, val_loss0.4717\n","Epoch 77, acc0.78, loss 0.4427, tol 0, val_acc0.80, val_loss0.4729\n","Epoch 78, acc0.79, loss 0.4610, tol 0, val_acc0.78, val_loss0.4778\n","Epoch 79, acc0.80, loss 0.4519, tol 0, val_acc0.81, val_loss0.4544 -- checkpoint saved\n","Epoch 80, acc0.81, loss 0.4244, tol 0, val_acc0.80, val_loss0.4389\n","Epoch 81, acc0.81, loss 0.4192, tol 0, val_acc0.80, val_loss0.4422\n","Epoch 82, acc0.82, loss 0.4129, tol 0, val_acc0.81, val_loss0.4227 -- checkpoint saved\n","Epoch 83, acc0.82, loss 0.4072, tol 0, val_acc0.82, val_loss0.4147 -- checkpoint saved\n","Epoch 84, acc0.83, loss 0.3902, tol 0, val_acc0.81, val_loss0.4165\n","Epoch 85, acc0.81, loss 0.4303, tol 0, val_acc0.83, val_loss0.4061 -- checkpoint saved\n","Epoch 86, acc0.82, loss 0.3917, tol 0, val_acc0.83, val_loss0.4023 -- checkpoint saved\n","Epoch 87, acc0.84, loss 0.3681, tol 0, val_acc0.83, val_loss0.4017 -- checkpoint saved\n","Epoch 88, acc0.83, loss 0.3790, tol 0, val_acc0.84, val_loss0.3962 -- checkpoint saved\n","Epoch 89, acc0.81, loss 0.4021, tol 0, val_acc0.85, val_loss0.3863 -- checkpoint saved\n","Epoch 90, acc0.83, loss 0.3879, tol 0, val_acc0.84, val_loss0.3854\n","Epoch 91, acc0.83, loss 0.3687, tol 0, val_acc0.83, val_loss0.3995\n","Epoch 92, acc0.83, loss 0.3695, tol 0, val_acc0.84, val_loss0.3881\n","Epoch 93, acc0.85, loss 0.3595, tol 0, val_acc0.84, val_loss0.3805\n","Epoch 94, acc0.84, loss 0.3649, tol 0, val_acc0.85, val_loss0.3816\n","Epoch 95, acc0.84, loss 0.3611, tol 0, val_acc0.84, val_loss0.3856\n","Epoch 96, acc0.84, loss 0.3680, tol 0, val_acc0.85, val_loss0.3745 -- checkpoint saved\n","Epoch 97, acc0.85, loss 0.3428, tol 0, val_acc0.84, val_loss0.3819\n","Epoch 98, acc0.83, loss 0.3515, tol 0, val_acc0.84, val_loss0.3880\n","Epoch 99, acc0.85, loss 0.3354, tol 0, val_acc0.86, val_loss0.3597 -- checkpoint saved\n","Epoch 100, acc0.83, loss 0.3681, tol 0, val_acc0.87, val_loss0.3409 -- checkpoint saved\n","Epoch 101, acc0.85, loss 0.3565, tol 0, val_acc0.86, val_loss0.3433\n","Epoch 102, acc0.84, loss 0.3555, tol 0, val_acc0.86, val_loss0.3454\n","Epoch 103, acc0.85, loss 0.3539, tol 0, val_acc0.86, val_loss0.3516\n","Epoch 104, acc0.85, loss 0.3449, tol 0, val_acc0.86, val_loss0.3438\n","Epoch 105, acc0.85, loss 0.3512, tol 0, val_acc0.86, val_loss0.3407\n","Epoch 106, acc0.87, loss 0.3167, tol 0, val_acc0.86, val_loss0.3402\n","Epoch 107, acc0.86, loss 0.3272, tol 0, val_acc0.86, val_loss0.3386\n","Epoch 108, acc0.87, loss 0.3077, tol 0, val_acc0.87, val_loss0.3336 -- checkpoint saved\n","Epoch 109, acc0.85, loss 0.3422, tol 0, val_acc0.86, val_loss0.3304\n","Epoch 110, acc0.88, loss 0.3032, tol 0, val_acc0.86, val_loss0.3363\n","Epoch 111, acc0.88, loss 0.3030, tol 0, val_acc0.86, val_loss0.3290\n","Epoch 112, acc0.87, loss 0.3094, tol 0, val_acc0.85, val_loss0.3337\n","Epoch 113, acc0.87, loss 0.3203, tol 0, val_acc0.86, val_loss0.3288\n","Epoch 114, acc0.86, loss 0.3129, tol 0, val_acc0.86, val_loss0.3266\n","Epoch 115, acc0.88, loss 0.3049, tol 0, val_acc0.86, val_loss0.3237\n","Epoch 116, acc0.87, loss 0.3247, tol 0, val_acc0.86, val_loss0.3325\n","Epoch 117, acc0.87, loss 0.3057, tol 0, val_acc0.86, val_loss0.3270\n","Epoch 118, acc0.88, loss 0.2985, tol 0, val_acc0.87, val_loss0.3189 -- checkpoint saved\n","Epoch 119, acc0.88, loss 0.2988, tol 0, val_acc0.84, val_loss0.3488\n","Epoch 120, acc0.86, loss 0.3248, tol 0, val_acc0.87, val_loss0.3135 -- checkpoint saved\n","Epoch 121, acc0.88, loss 0.2911, tol 0, val_acc0.87, val_loss0.3109 -- checkpoint saved\n","Epoch 122, acc0.88, loss 0.2965, tol 0, val_acc0.88, val_loss0.3139\n","Epoch 123, acc0.88, loss 0.2957, tol 0, val_acc0.87, val_loss0.3107\n","Epoch 124, acc0.88, loss 0.3143, tol 0, val_acc0.87, val_loss0.3155\n","Epoch 125, acc0.87, loss 0.3084, tol 0, val_acc0.88, val_loss0.3111\n","Epoch 126, acc0.88, loss 0.3023, tol 0, val_acc0.88, val_loss0.3035 -- checkpoint saved\n","Epoch 127, acc0.89, loss 0.3009, tol 0, val_acc0.87, val_loss0.3164\n","Epoch 128, acc0.88, loss 0.2897, tol 0, val_acc0.88, val_loss0.2995 -- checkpoint saved\n","Epoch 129, acc0.89, loss 0.2718, tol 0, val_acc0.88, val_loss0.3042\n","Epoch 130, acc0.88, loss 0.2887, tol 0, val_acc0.88, val_loss0.3068\n","Epoch 131, acc0.89, loss 0.2754, tol 0, val_acc0.88, val_loss0.3021\n","Epoch 132, acc0.88, loss 0.2942, tol 0, val_acc0.89, val_loss0.3005\n","Epoch 133, acc0.90, loss 0.2858, tol 0, val_acc0.88, val_loss0.3009\n","Epoch 134, acc0.89, loss 0.2742, tol 0, val_acc0.88, val_loss0.3058\n","Epoch 135, acc0.88, loss 0.3071, tol 0, val_acc0.88, val_loss0.2996\n","Epoch 136, acc0.88, loss 0.3072, tol 0, val_acc0.89, val_loss0.2999\n","Epoch 137, acc0.88, loss 0.3037, tol 0, val_acc0.88, val_loss0.2961\n","Epoch 138, acc0.88, loss 0.2935, tol 0, val_acc0.87, val_loss0.3030\n","Epoch 139, acc0.91, loss 0.2572, tol 0, val_acc0.88, val_loss0.2876\n","Epoch 140, acc0.89, loss 0.2915, tol 0, val_acc0.89, val_loss0.2873 -- checkpoint saved\n","Epoch 141, acc0.88, loss 0.2981, tol 0, val_acc0.88, val_loss0.2890\n","Epoch 142, acc0.90, loss 0.2613, tol 0, val_acc0.88, val_loss0.2922\n","Epoch 143, acc0.88, loss 0.2813, tol 0, val_acc0.89, val_loss0.2935\n","Epoch 144, acc0.89, loss 0.2697, tol 0, val_acc0.90, val_loss0.2869 -- checkpoint saved\n","Epoch 145, acc0.88, loss 0.2940, tol 0, val_acc0.89, val_loss0.2876\n","Epoch 146, acc0.89, loss 0.2776, tol 0, val_acc0.89, val_loss0.2898\n","Epoch 147, acc0.89, loss 0.2950, tol 0, val_acc0.89, val_loss0.2905\n","Epoch 148, acc0.91, loss 0.2484, tol 0, val_acc0.90, val_loss0.2883\n","Epoch 149, acc0.90, loss 0.2631, tol 0, val_acc0.90, val_loss0.2894\n","Epoch 150, acc0.89, loss 0.2891, tol 0, val_acc0.90, val_loss0.2829 -- checkpoint saved\n","Epoch 151, acc0.89, loss 0.2870, tol 0, val_acc0.90, val_loss0.2834\n","Epoch 152, acc0.88, loss 0.2841, tol 0, val_acc0.89, val_loss0.2894\n","Epoch 153, acc0.91, loss 0.2618, tol 0, val_acc0.90, val_loss0.2833\n","Epoch 154, acc0.90, loss 0.2633, tol 0, val_acc0.89, val_loss0.2873\n","Epoch 155, acc0.90, loss 0.2786, tol 0, val_acc0.89, val_loss0.2807\n","Epoch 156, acc0.89, loss 0.2669, tol 0, val_acc0.88, val_loss0.2903\n","Epoch 157, acc0.89, loss 0.2764, tol 0, val_acc0.89, val_loss0.2885\n","Epoch 158, acc0.89, loss 0.2932, tol 0, val_acc0.89, val_loss0.2867\n","Epoch 159, acc0.90, loss 0.2749, tol 0, val_acc0.88, val_loss0.2819\n","Epoch 160, acc0.91, loss 0.2562, tol 0, val_acc0.89, val_loss0.2836\n","Epoch 161, acc0.90, loss 0.2719, tol 0, val_acc0.89, val_loss0.2773\n","Epoch 162, acc0.89, loss 0.2884, tol 0, val_acc0.89, val_loss0.2797\n","Epoch 163, acc0.89, loss 0.2734, tol 0, val_acc0.90, val_loss0.2751\n","Epoch 164, acc0.90, loss 0.2677, tol 0, val_acc0.90, val_loss0.2782\n","Epoch 165, acc0.91, loss 0.2541, tol 0, val_acc0.89, val_loss0.2844\n","Epoch 166, acc0.91, loss 0.2292, tol 0, val_acc0.90, val_loss0.2788\n","Epoch 167, acc0.89, loss 0.2792, tol 0, val_acc0.89, val_loss0.2862\n","Epoch 168, acc0.90, loss 0.2548, tol 0, val_acc0.91, val_loss0.2785\n","Epoch 169, acc0.90, loss 0.2724, tol 0, val_acc0.90, val_loss0.2771\n","Epoch 170, acc0.89, loss 0.2867, tol 0, val_acc0.90, val_loss0.2840\n","Epoch 171, acc0.90, loss 0.2663, tol 0, val_acc0.90, val_loss0.2811\n","Epoch 172, acc0.90, loss 0.2580, tol 0, val_acc0.90, val_loss0.2774\n","Epoch 173, acc0.89, loss 0.2828, tol 0, val_acc0.89, val_loss0.2811\n","Epoch 174, acc0.90, loss 0.2611, tol 0, val_acc0.89, val_loss0.2812\n","Epoch 175, acc0.90, loss 0.2643, tol 0, val_acc0.90, val_loss0.2735\n","Epoch 176, acc0.90, loss 0.2761, tol 0, val_acc0.91, val_loss0.2718 -- checkpoint saved\n","Epoch 177, acc0.91, loss 0.2603, tol 0, val_acc0.90, val_loss0.2743\n","Epoch 178, acc0.90, loss 0.2829, tol 0, val_acc0.90, val_loss0.2702\n","Epoch 179, acc0.91, loss 0.2390, tol 0, val_acc0.88, val_loss0.2853\n","Epoch 180, acc0.90, loss 0.2588, tol 0, val_acc0.90, val_loss0.2758\n","Epoch 181, acc0.90, loss 0.2723, tol 0, val_acc0.90, val_loss0.2722\n","Epoch 182, acc0.91, loss 0.2382, tol 0, val_acc0.90, val_loss0.2728\n","Epoch 183, acc0.90, loss 0.2553, tol 0, val_acc0.88, val_loss0.2801\n","Epoch 184, acc0.91, loss 0.2380, tol 0, val_acc0.89, val_loss0.2758\n","Epoch 185, acc0.89, loss 0.2657, tol 0, val_acc0.90, val_loss0.2716\n","Epoch 186, acc0.90, loss 0.2733, tol 0, val_acc0.90, val_loss0.2675\n","Epoch 187, acc0.91, loss 0.2223, tol 0, val_acc0.90, val_loss0.2701\n","Epoch 188, acc0.91, loss 0.2508, tol 0, val_acc0.91, val_loss0.2688\n","Epoch 189, acc0.90, loss 0.2539, tol 0, val_acc0.90, val_loss0.2703\n","Epoch 190, acc0.91, loss 0.2421, tol 0, val_acc0.89, val_loss0.2745\n","Epoch 191, acc0.91, loss 0.2575, tol 0, val_acc0.89, val_loss0.2844\n","Epoch 192, acc0.92, loss 0.2402, tol 0, val_acc0.90, val_loss0.2738\n","Epoch 193, acc0.91, loss 0.2545, tol 0, val_acc0.89, val_loss0.2777\n","Epoch 194, acc0.90, loss 0.2580, tol 0, val_acc0.90, val_loss0.2722\n","Epoch 195, acc0.90, loss 0.2765, tol 0, val_acc0.90, val_loss0.2722\n","Epoch 196, acc0.90, loss 0.2455, tol 0, val_acc0.90, val_loss0.2657\n","Epoch 197, acc0.90, loss 0.2551, tol 0, val_acc0.90, val_loss0.2710\n","Epoch 198, acc0.91, loss 0.2447, tol 0, val_acc0.90, val_loss0.2738\n","Epoch 199, acc0.91, loss 0.2500, tol 0, val_acc0.90, val_loss0.2697\n","Epoch 200, acc0.91, loss 0.2449, tol 0, val_acc0.90, val_loss0.2677\n","Epoch 201, acc0.92, loss 0.2349, tol 0, val_acc0.90, val_loss0.2666\n","Epoch 202, acc0.91, loss 0.2484, tol 0, val_acc0.90, val_loss0.2632\n","Epoch 203, acc0.91, loss 0.2599, tol 0, val_acc0.90, val_loss0.2667\n","Epoch 204, acc0.92, loss 0.2416, tol 0, val_acc0.90, val_loss0.2674\n","Epoch 205, acc0.91, loss 0.2475, tol 0, val_acc0.91, val_loss0.2634\n","Epoch 206, acc0.91, loss 0.2368, tol 0, val_acc0.91, val_loss0.2688\n","Epoch 207, acc0.91, loss 0.2282, tol 0, val_acc0.90, val_loss0.2591\n","Epoch 208, acc0.91, loss 0.2380, tol 0, val_acc0.91, val_loss0.2633\n","Epoch 209, acc0.92, loss 0.2359, tol 0, val_acc0.91, val_loss0.2593\n","Epoch 210, acc0.91, loss 0.2393, tol 0, val_acc0.90, val_loss0.2684\n","Epoch 211, acc0.91, loss 0.2512, tol 0, val_acc0.89, val_loss0.2690\n","Epoch 212, acc0.90, loss 0.2549, tol 0, val_acc0.90, val_loss0.2658\n","Epoch 213, acc0.91, loss 0.2470, tol 0, val_acc0.90, val_loss0.2676\n","Epoch 214, acc0.91, loss 0.2626, tol 0, val_acc0.91, val_loss0.2624\n","Epoch 215, acc0.91, loss 0.2510, tol 0, val_acc0.90, val_loss0.2622\n","Epoch 216, acc0.91, loss 0.2469, tol 0, val_acc0.91, val_loss0.2655\n","Epoch 217, acc0.91, loss 0.2638, tol 0, val_acc0.90, val_loss0.2645\n","Epoch 218, acc0.91, loss 0.2642, tol 0, val_acc0.91, val_loss0.2575\n","Epoch 219, acc0.91, loss 0.2404, tol 0, val_acc0.90, val_loss0.2610\n","Epoch 220, acc0.90, loss 0.2556, tol 0, val_acc0.90, val_loss0.2678\n","Epoch 221, acc0.92, loss 0.2506, tol 0, val_acc0.90, val_loss0.2584\n","Epoch 222, acc0.92, loss 0.2414, tol 0, val_acc0.89, val_loss0.2654\n","Epoch 223, acc0.91, loss 0.2587, tol 0, val_acc0.90, val_loss0.2631\n","Epoch 224, acc0.90, loss 0.2611, tol 0, val_acc0.90, val_loss0.2632\n","Epoch 225, acc0.92, loss 0.2404, tol 0, val_acc0.90, val_loss0.2612\n","Epoch 226, acc0.90, loss 0.2606, tol 0, val_acc0.90, val_loss0.2597\n","Epoch 227, acc0.91, loss 0.2714, tol 0, val_acc0.90, val_loss0.2616\n","Epoch 228, acc0.92, loss 0.2329, tol 0, val_acc0.90, val_loss0.2566\n","Epoch 229, acc0.92, loss 0.2427, tol 0, val_acc0.90, val_loss0.2651\n","Epoch 230, acc0.92, loss 0.2255, tol 0, val_acc0.90, val_loss0.2611\n","Epoch 231, acc0.92, loss 0.2360, tol 0, val_acc0.89, val_loss0.2642\n","Epoch 232, acc0.91, loss 0.2399, tol 0, val_acc0.90, val_loss0.2622\n","Epoch 233, acc0.92, loss 0.2460, tol 0, val_acc0.90, val_loss0.2589\n","Epoch 234, acc0.92, loss 0.2302, tol 0, val_acc0.91, val_loss0.2590\n","Epoch 235, acc0.91, loss 0.2595, tol 0, val_acc0.90, val_loss0.2587\n","Epoch 236, acc0.91, loss 0.2437, tol 0, val_acc0.91, val_loss0.2621\n","Epoch 237, acc0.90, loss 0.2611, tol 0, val_acc0.90, val_loss0.2641\n","Epoch 238, acc0.91, loss 0.2409, tol 0, val_acc0.90, val_loss0.2634\n","Epoch 239, acc0.91, loss 0.2440, tol 0, val_acc0.90, val_loss0.2668\n","Epoch 240, acc0.92, loss 0.2335, tol 0, val_acc0.90, val_loss0.2604\n","Epoch 241, acc0.92, loss 0.2266, tol 0, val_acc0.90, val_loss0.2623\n","Epoch 242, acc0.92, loss 0.2343, tol 0, val_acc0.90, val_loss0.2627\n","Epoch 243, acc0.92, loss 0.2259, tol 0, val_acc0.91, val_loss0.2570\n","Epoch 244, acc0.92, loss 0.2277, tol 0, val_acc0.91, val_loss0.2592\n","Epoch 245, acc0.90, loss 0.2592, tol 0, val_acc0.89, val_loss0.2660\n","Epoch 246, acc0.91, loss 0.2560, tol 0, val_acc0.89, val_loss0.2671\n","Epoch 247, acc0.92, loss 0.2249, tol 0, val_acc0.90, val_loss0.2715\n","Epoch 248, acc0.92, loss 0.2270, tol 0, val_acc0.90, val_loss0.2588\n","Epoch 249, acc0.92, loss 0.2262, tol 0, val_acc0.90, val_loss0.2594\n","Epoch 250, acc0.92, loss 0.2230, tol 0, val_acc0.90, val_loss0.2561\n","Epoch 251, acc0.92, loss 0.2359, tol 0, val_acc0.90, val_loss0.2607\n","Epoch 252, acc0.93, loss 0.2149, tol 0, val_acc0.90, val_loss0.2560\n","Epoch 253, acc0.93, loss 0.2287, tol 0, val_acc0.91, val_loss0.2597\n","Epoch 254, acc0.91, loss 0.2594, tol 0, val_acc0.90, val_loss0.2603\n","Epoch 255, acc0.92, loss 0.2357, tol 0, val_acc0.91, val_loss0.2631\n","Epoch 256, acc0.93, loss 0.2211, tol 0, val_acc0.90, val_loss0.2601\n","Epoch 257, acc0.91, loss 0.2321, tol 0, val_acc0.90, val_loss0.2558\n","Epoch 258, acc0.90, loss 0.2620, tol 0, val_acc0.90, val_loss0.2565\n","Epoch 259, acc0.92, loss 0.2365, tol 0, val_acc0.90, val_loss0.2717\n","Epoch 260, acc0.92, loss 0.2315, tol 0, val_acc0.90, val_loss0.2604\n","Epoch 261, acc0.90, loss 0.2620, tol 0, val_acc0.90, val_loss0.2598\n","Epoch 262, acc0.92, loss 0.2172, tol 0, val_acc0.89, val_loss0.2628\n","Epoch 263, acc0.93, loss 0.2116, tol 0, val_acc0.90, val_loss0.2532\n","Epoch 264, acc0.92, loss 0.2456, tol 0, val_acc0.90, val_loss0.2617\n","Epoch 265, acc0.92, loss 0.2324, tol 0, val_acc0.90, val_loss0.2550\n","Epoch 266, acc0.92, loss 0.2403, tol 0, val_acc0.90, val_loss0.2557\n","Epoch 267, acc0.91, loss 0.2426, tol 0, val_acc0.90, val_loss0.2598\n","Epoch 268, acc0.92, loss 0.2349, tol 0, val_acc0.90, val_loss0.2551\n","Epoch 269, acc0.92, loss 0.2329, tol 0, val_acc0.90, val_loss0.2599\n","Epoch 270, acc0.92, loss 0.2339, tol 0, val_acc0.90, val_loss0.2576\n","Epoch 271, acc0.92, loss 0.2268, tol 0, val_acc0.90, val_loss0.2584\n","Epoch 272, acc0.91, loss 0.2610, tol 0, val_acc0.90, val_loss0.2614\n","Epoch 273, acc0.92, loss 0.2275, tol 0, val_acc0.90, val_loss0.2601\n","Epoch 274, acc0.92, loss 0.2356, tol 0, val_acc0.89, val_loss0.2585\n","Epoch 275, acc0.92, loss 0.2125, tol 0, val_acc0.89, val_loss0.2658\n","Epoch 276, acc0.91, loss 0.2406, tol 0, val_acc0.89, val_loss0.2604\n","Epoch 277, acc0.92, loss 0.2188, tol 0, val_acc0.90, val_loss0.2610\n","Epoch 278, acc0.92, loss 0.2433, tol 0, val_acc0.89, val_loss0.2618\n","Epoch 279, acc0.92, loss 0.2295, tol 0, val_acc0.90, val_loss0.2621\n","Epoch 280, acc0.92, loss 0.2318, tol 0, val_acc0.90, val_loss0.2639\n","Epoch 281, acc0.91, loss 0.2316, tol 0, val_acc0.89, val_loss0.2645\n","Epoch 282, acc0.92, loss 0.2267, tol 0, val_acc0.90, val_loss0.2690\n","Epoch 283, acc0.92, loss 0.2209, tol 0, val_acc0.90, val_loss0.2633\n","Epoch 284, acc0.91, loss 0.2448, tol 0, val_acc0.90, val_loss0.2545\n","Epoch 285, acc0.92, loss 0.2025, tol 0, val_acc0.89, val_loss0.2642\n","Epoch 286, acc0.91, loss 0.2535, tol 0, val_acc0.89, val_loss0.2668\n","Epoch 287, acc0.91, loss 0.2564, tol 0, val_acc0.90, val_loss0.2592\n","Epoch 288, acc0.93, loss 0.2041, tol 0, val_acc0.90, val_loss0.2579\n","Epoch 289, acc0.93, loss 0.2123, tol 0, val_acc0.90, val_loss0.2571\n","Epoch 290, acc0.93, loss 0.2106, tol 0, val_acc0.90, val_loss0.2522\n","Epoch 291, acc0.92, loss 0.1999, tol 0, val_acc0.89, val_loss0.2588\n","Epoch 292, acc0.93, loss 0.2305, tol 0, val_acc0.90, val_loss0.2566\n","Epoch 293, acc0.93, loss 0.2302, tol 0, val_acc0.90, val_loss0.2556\n","Epoch 294, acc0.92, loss 0.2206, tol 0, val_acc0.90, val_loss0.2570\n","Epoch 295, acc0.93, loss 0.2158, tol 0, val_acc0.90, val_loss0.2597\n","Epoch 296, acc0.92, loss 0.2179, tol 0, val_acc0.90, val_loss0.2619\n","Epoch 297, acc0.92, loss 0.2233, tol 0, val_acc0.90, val_loss0.2633\n","Epoch 298, acc0.91, loss 0.2219, tol 0, val_acc0.90, val_loss0.2652\n","Epoch 299, acc0.92, loss 0.2199, tol 0, val_acc0.89, val_loss0.2690\n","Epoch 300, acc0.92, loss 0.2212, tol 0, val_acc0.90, val_loss0.2586\n","Epoch 301, acc0.93, loss 0.2112, tol 0, val_acc0.90, val_loss0.2614\n","Epoch 302, acc0.91, loss 0.2465, tol 0, val_acc0.90, val_loss0.2646\n","Epoch 303, acc0.92, loss 0.2259, tol 0, val_acc0.91, val_loss0.2598\n","Epoch 304, acc0.92, loss 0.2176, tol 0, val_acc0.90, val_loss0.2633\n","Epoch 305, acc0.91, loss 0.2472, tol 0, val_acc0.90, val_loss0.2613\n","Epoch 306, acc0.92, loss 0.2288, tol 0, val_acc0.90, val_loss0.2585\n","Epoch 307, acc0.92, loss 0.2227, tol 0, val_acc0.89, val_loss0.2609\n","Epoch 308, acc0.92, loss 0.2108, tol 0, val_acc0.90, val_loss0.2647\n","Epoch 309, acc0.93, loss 0.2158, tol 0, val_acc0.90, val_loss0.2591\n","Epoch 310, acc0.92, loss 0.2208, tol 0, val_acc0.90, val_loss0.2619\n","Epoch 311, acc0.92, loss 0.2369, tol 0, val_acc0.91, val_loss0.2608\n","Epoch 312, acc0.92, loss 0.2340, tol 0, val_acc0.90, val_loss0.2595\n","Epoch 313, acc0.92, loss 0.2246, tol 0, val_acc0.90, val_loss0.2545\n","Epoch 314, acc0.93, loss 0.1967, tol 0, val_acc0.91, val_loss0.2551\n","Epoch 315, acc0.93, loss 0.2007, tol 0, val_acc0.90, val_loss0.2551\n","Epoch 316, acc0.91, loss 0.2583, tol 0, val_acc0.90, val_loss0.2584\n","Epoch 317, acc0.93, loss 0.2148, tol 0, val_acc0.90, val_loss0.2552\n","Epoch 318, acc0.91, loss 0.2319, tol 0, val_acc0.90, val_loss0.2542\n","Epoch 319, acc0.93, loss 0.2063, tol 0, val_acc0.90, val_loss0.2588\n","Epoch 320, acc0.94, loss 0.1886, tol 0, val_acc0.90, val_loss0.2551\n","Epoch 321, acc0.93, loss 0.1891, tol 0, val_acc0.91, val_loss0.2593\n","Epoch 322, acc0.92, loss 0.2055, tol 0, val_acc0.91, val_loss0.2559\n","Epoch 323, acc0.93, loss 0.2019, tol 0, val_acc0.90, val_loss0.2537\n","Epoch 324, acc0.93, loss 0.1920, tol 0, val_acc0.91, val_loss0.2556\n","Epoch 325, acc0.92, loss 0.2235, tol 0, val_acc0.90, val_loss0.2591\n","Epoch 326, acc0.92, loss 0.2200, tol 0, val_acc0.90, val_loss0.2574\n","Epoch 327, acc0.93, loss 0.2097, tol 0, val_acc0.90, val_loss0.2543\n","Epoch 328, acc0.93, loss 0.2067, tol 0, val_acc0.90, val_loss0.2543\n","Epoch 329, acc0.92, loss 0.2137, tol 0, val_acc0.90, val_loss0.2619\n","Epoch 330, acc0.92, loss 0.2286, tol 0, val_acc0.89, val_loss0.2612\n","Epoch 331, acc0.93, loss 0.1959, tol 0, val_acc0.90, val_loss0.2605\n","Epoch 332, acc0.92, loss 0.2114, tol 0, val_acc0.90, val_loss0.2598\n","Epoch 333, acc0.93, loss 0.2070, tol 0, val_acc0.90, val_loss0.2615\n","Epoch 334, acc0.93, loss 0.2019, tol 0, val_acc0.90, val_loss0.2649\n","Epoch 335, acc0.93, loss 0.1894, tol 0, val_acc0.90, val_loss0.2665\n","Epoch 336, acc0.91, loss 0.2365, tol 0, val_acc0.90, val_loss0.2530\n","Epoch 337, acc0.92, loss 0.2189, tol 0, val_acc0.90, val_loss0.2564\n","Epoch 338, acc0.91, loss 0.2287, tol 0, val_acc0.90, val_loss0.2571\n","Epoch 339, acc0.92, loss 0.2220, tol 0, val_acc0.91, val_loss0.2565\n","Epoch 340, acc0.91, loss 0.2236, tol 0, val_acc0.91, val_loss0.2576\n","Epoch 341, acc0.94, loss 0.1980, tol 0, val_acc0.89, val_loss0.2598\n","Epoch 342, acc0.92, loss 0.2057, tol 0, val_acc0.90, val_loss0.2558\n","Epoch 343, acc0.93, loss 0.2140, tol 0, val_acc0.91, val_loss0.2538\n","Epoch 344, acc0.92, loss 0.2160, tol 0, val_acc0.91, val_loss0.2619\n","Epoch 345, acc0.92, loss 0.2256, tol 0, val_acc0.90, val_loss0.2670\n","Epoch 346, acc0.92, loss 0.2174, tol 0, val_acc0.90, val_loss0.2561\n","Epoch 347, acc0.93, loss 0.2125, tol 0, val_acc0.91, val_loss0.2555\n","Epoch 348, acc0.92, loss 0.2145, tol 0, val_acc0.90, val_loss0.2558\n","Epoch 349, acc0.92, loss 0.2133, tol 0, val_acc0.90, val_loss0.2548\n","Epoch 350, acc0.93, loss 0.2059, tol 0, val_acc0.91, val_loss0.2495\n","Epoch 351, acc0.93, loss 0.1983, tol 0, val_acc0.91, val_loss0.2584\n","Epoch 352, acc0.91, loss 0.2331, tol 0, val_acc0.90, val_loss0.2615\n","Epoch 353, acc0.93, loss 0.2039, tol 0, val_acc0.91, val_loss0.2569\n","Epoch 354, acc0.93, loss 0.1921, tol 0, val_acc0.90, val_loss0.2566\n","Epoch 355, acc0.93, loss 0.1877, tol 0, val_acc0.89, val_loss0.2606\n","Epoch 356, acc0.93, loss 0.1992, tol 0, val_acc0.91, val_loss0.2507\n","Epoch 357, acc0.92, loss 0.2222, tol 0, val_acc0.90, val_loss0.2521\n","Epoch 358, acc0.93, loss 0.1953, tol 0, val_acc0.90, val_loss0.2579\n","Epoch 359, acc0.92, loss 0.2210, tol 0, val_acc0.90, val_loss0.2570\n","Epoch 360, acc0.92, loss 0.2168, tol 0, val_acc0.91, val_loss0.2550\n","Epoch 361, acc0.91, loss 0.2333, tol 0, val_acc0.89, val_loss0.2575\n","Epoch 362, acc0.92, loss 0.2224, tol 0, val_acc0.89, val_loss0.2583\n","Epoch 363, acc0.93, loss 0.2060, tol 0, val_acc0.91, val_loss0.2556\n","Epoch 364, acc0.93, loss 0.2116, tol 0, val_acc0.89, val_loss0.2684\n","Epoch 365, acc0.92, loss 0.2132, tol 0, val_acc0.90, val_loss0.2585\n","Epoch 366, acc0.91, loss 0.2251, tol 0, val_acc0.90, val_loss0.2539\n","Epoch 367, acc0.93, loss 0.2076, tol 0, val_acc0.90, val_loss0.2538\n","Epoch 368, acc0.93, loss 0.1948, tol 0, val_acc0.90, val_loss0.2489\n","Epoch 369, acc0.92, loss 0.2121, tol 0, val_acc0.90, val_loss0.2527\n","Epoch 370, acc0.93, loss 0.1921, tol 0, val_acc0.90, val_loss0.2522\n","Epoch 371, acc0.91, loss 0.2254, tol 0, val_acc0.90, val_loss0.2585\n","Epoch 372, acc0.91, loss 0.2282, tol 0, val_acc0.88, val_loss0.2837\n","Epoch 373, acc0.92, loss 0.2140, tol 0, val_acc0.90, val_loss0.2517\n","Epoch 374, acc0.91, loss 0.2426, tol 0, val_acc0.90, val_loss0.2613\n","Epoch 375, acc0.92, loss 0.2084, tol 0, val_acc0.91, val_loss0.2529\n","Epoch 376, acc0.93, loss 0.1976, tol 0, val_acc0.90, val_loss0.2473\n","Epoch 377, acc0.93, loss 0.2006, tol 0, val_acc0.90, val_loss0.2548\n","Epoch 378, acc0.93, loss 0.1942, tol 0, val_acc0.90, val_loss0.2554\n","Epoch 379, acc0.94, loss 0.1857, tol 0, val_acc0.90, val_loss0.2547\n","Epoch 380, acc0.93, loss 0.1912, tol 0, val_acc0.90, val_loss0.2527\n","Epoch 381, acc0.92, loss 0.2231, tol 0, val_acc0.90, val_loss0.2563\n","Epoch 382, acc0.93, loss 0.1858, tol 0, val_acc0.90, val_loss0.2526\n","Epoch 383, acc0.93, loss 0.2089, tol 0, val_acc0.90, val_loss0.2534\n","Epoch 384, acc0.92, loss 0.2218, tol 0, val_acc0.90, val_loss0.2535\n","Epoch 385, acc0.93, loss 0.1943, tol 0, val_acc0.90, val_loss0.2532\n","Epoch 386, acc0.92, loss 0.2254, tol 0, val_acc0.89, val_loss0.2525\n","Epoch 387, acc0.92, loss 0.1950, tol 0, val_acc0.91, val_loss0.2514\n","Epoch 388, acc0.93, loss 0.1899, tol 0, val_acc0.90, val_loss0.2498\n","Epoch 389, acc0.93, loss 0.1937, tol 0, val_acc0.91, val_loss0.2480\n","Epoch 390, acc0.92, loss 0.1973, tol 0, val_acc0.90, val_loss0.2478\n","Epoch 391, acc0.93, loss 0.1991, tol 0, val_acc0.90, val_loss0.2509\n","Epoch 392, acc0.93, loss 0.1866, tol 0, val_acc0.90, val_loss0.2540\n","Epoch 393, acc0.93, loss 0.1954, tol 0, val_acc0.90, val_loss0.2612\n","Epoch 394, acc0.93, loss 0.1986, tol 0, val_acc0.90, val_loss0.2621\n","Epoch 395, acc0.92, loss 0.2100, tol 0, val_acc0.90, val_loss0.2530\n","Epoch 396, acc0.94, loss 0.1944, tol 0, val_acc0.90, val_loss0.2543\n","Epoch 397, acc0.93, loss 0.2129, tol 0, val_acc0.90, val_loss0.2559\n","Epoch 398, acc0.93, loss 0.2007, tol 0, val_acc0.90, val_loss0.2537\n","Epoch 399, acc0.93, loss 0.2006, tol 0, val_acc0.90, val_loss0.2491\n","Epoch 400, acc0.92, loss 0.2148, tol 0, val_acc0.90, val_loss0.2506\n","Epoch 401, acc0.91, loss 0.2235, tol 0, val_acc0.90, val_loss0.2504\n","Epoch 402, acc0.93, loss 0.1814, tol 0, val_acc0.90, val_loss0.2546\n","Epoch 403, acc0.93, loss 0.1965, tol 0, val_acc0.90, val_loss0.2565\n","Epoch 404, acc0.93, loss 0.2019, tol 0, val_acc0.90, val_loss0.2513\n","Epoch 405, acc0.92, loss 0.2061, tol 0, val_acc0.90, val_loss0.2500\n","Epoch 406, acc0.92, loss 0.2083, tol 0, val_acc0.90, val_loss0.2600\n","Epoch 407, acc0.92, loss 0.1993, tol 0, val_acc0.90, val_loss0.2454\n","Epoch 408, acc0.91, loss 0.2289, tol 0, val_acc0.90, val_loss0.2536\n","Epoch 409, acc0.93, loss 0.2056, tol 0, val_acc0.90, val_loss0.2433\n","Epoch 410, acc0.93, loss 0.2076, tol 0, val_acc0.91, val_loss0.2516\n","Epoch 411, acc0.93, loss 0.1761, tol 0, val_acc0.90, val_loss0.2446\n","Epoch 412, acc0.93, loss 0.2067, tol 0, val_acc0.90, val_loss0.2439\n","Epoch 413, acc0.92, loss 0.2167, tol 0, val_acc0.89, val_loss0.2643\n","Epoch 414, acc0.92, loss 0.2088, tol 0, val_acc0.90, val_loss0.2459\n","Epoch 415, acc0.94, loss 0.1803, tol 0, val_acc0.90, val_loss0.2473\n","Epoch 416, acc0.92, loss 0.2019, tol 0, val_acc0.90, val_loss0.2577\n","Epoch 417, acc0.92, loss 0.2028, tol 0, val_acc0.90, val_loss0.2547\n","Epoch 418, acc0.92, loss 0.2262, tol 0, val_acc0.90, val_loss0.2508\n","Epoch 419, acc0.93, loss 0.1949, tol 0, val_acc0.91, val_loss0.2466\n","Epoch 420, acc0.93, loss 0.1856, tol 0, val_acc0.90, val_loss0.2529\n","Epoch 421, acc0.92, loss 0.1962, tol 0, val_acc0.90, val_loss0.2499\n","Epoch 422, acc0.91, loss 0.2169, tol 0, val_acc0.90, val_loss0.2450\n","Epoch 423, acc0.92, loss 0.2011, tol 0, val_acc0.90, val_loss0.2401\n","Epoch 424, acc0.91, loss 0.2181, tol 0, val_acc0.90, val_loss0.2487\n","Epoch 425, acc0.93, loss 0.2000, tol 0, val_acc0.90, val_loss0.2437\n","Epoch 426, acc0.93, loss 0.1862, tol 0, val_acc0.90, val_loss0.2485\n","Epoch 427, acc0.94, loss 0.1685, tol 0, val_acc0.91, val_loss0.2468\n","Epoch 428, acc0.93, loss 0.1946, tol 0, val_acc0.90, val_loss0.2499\n","Epoch 429, acc0.92, loss 0.2049, tol 0, val_acc0.90, val_loss0.2455\n","Epoch 430, acc0.93, loss 0.1901, tol 0, val_acc0.90, val_loss0.2479\n","Epoch 431, acc0.93, loss 0.2031, tol 0, val_acc0.90, val_loss0.2633\n","Epoch 432, acc0.93, loss 0.1851, tol 0, val_acc0.90, val_loss0.2559\n","Epoch 433, acc0.93, loss 0.1918, tol 0, val_acc0.90, val_loss0.2443\n","Epoch 434, acc0.93, loss 0.1761, tol 0, val_acc0.90, val_loss0.2453\n","Epoch 435, acc0.93, loss 0.1868, tol 0, val_acc0.90, val_loss0.2503\n","Epoch 436, acc0.93, loss 0.1898, tol 0, val_acc0.90, val_loss0.2576\n","Epoch 437, acc0.92, loss 0.2026, tol 0, val_acc0.90, val_loss0.2446\n","Epoch 438, acc0.92, loss 0.2104, tol 0, val_acc0.90, val_loss0.2442\n","Epoch 439, acc0.93, loss 0.1903, tol 0, val_acc0.90, val_loss0.2504\n","Epoch 440, acc0.93, loss 0.2035, tol 0, val_acc0.90, val_loss0.2461\n","Epoch 441, acc0.94, loss 0.1960, tol 0, val_acc0.91, val_loss0.2546\n","Epoch 442, acc0.93, loss 0.2031, tol 0, val_acc0.90, val_loss0.2468\n","Epoch 443, acc0.92, loss 0.1935, tol 0, val_acc0.90, val_loss0.2469\n","Epoch 444, acc0.93, loss 0.2034, tol 0, val_acc0.90, val_loss0.2447\n","Epoch 445, acc0.93, loss 0.2156, tol 0, val_acc0.90, val_loss0.2486\n","Epoch 446, acc0.93, loss 0.2052, tol 0, val_acc0.90, val_loss0.2486\n","Epoch 447, acc0.92, loss 0.2155, tol 0, val_acc0.90, val_loss0.2465\n","Epoch 448, acc0.93, loss 0.1995, tol 0, val_acc0.90, val_loss0.2438\n","Epoch 449, acc0.93, loss 0.1810, tol 0, val_acc0.90, val_loss0.2422\n","Epoch 450, acc0.93, loss 0.1899, tol 0, val_acc0.90, val_loss0.2477\n","Epoch 451, acc0.93, loss 0.1976, tol 0, val_acc0.90, val_loss0.2459\n","Epoch 452, acc0.93, loss 0.2022, tol 0, val_acc0.90, val_loss0.2430\n","Epoch 453, acc0.93, loss 0.2021, tol 0, val_acc0.90, val_loss0.2418\n","Epoch 454, acc0.92, loss 0.2068, tol 0, val_acc0.90, val_loss0.2460\n","Epoch 455, acc0.93, loss 0.1761, tol 0, val_acc0.90, val_loss0.2454\n","Epoch 456, acc0.93, loss 0.1836, tol 0, val_acc0.91, val_loss0.2488\n","Epoch 457, acc0.93, loss 0.1951, tol 0, val_acc0.90, val_loss0.2523\n","Epoch 458, acc0.93, loss 0.1851, tol 0, val_acc0.91, val_loss0.2438\n","Epoch 459, acc0.93, loss 0.1838, tol 0, val_acc0.90, val_loss0.2425\n","Epoch 460, acc0.92, loss 0.1808, tol 0, val_acc0.91, val_loss0.2419\n","Epoch 461, acc0.93, loss 0.1932, tol 0, val_acc0.90, val_loss0.2435\n","Epoch 462, acc0.93, loss 0.1971, tol 0, val_acc0.90, val_loss0.2511\n","Epoch 463, acc0.93, loss 0.1975, tol 0, val_acc0.90, val_loss0.2455\n","Epoch 464, acc0.92, loss 0.1816, tol 0, val_acc0.90, val_loss0.2426\n","Epoch 465, acc0.94, loss 0.1777, tol 0, val_acc0.90, val_loss0.2547\n","Epoch 466, acc0.93, loss 0.1990, tol 0, val_acc0.90, val_loss0.2406\n","Epoch 467, acc0.94, loss 0.1774, tol 0, val_acc0.91, val_loss0.2453\n","Epoch 468, acc0.92, loss 0.1847, tol 0, val_acc0.90, val_loss0.2482\n","Epoch 469, acc0.93, loss 0.1859, tol 0, val_acc0.90, val_loss0.2444\n","Epoch 470, acc0.94, loss 0.1490, tol 0, val_acc0.90, val_loss0.2527\n","Epoch 471, acc0.93, loss 0.1808, tol 0, val_acc0.91, val_loss0.2603\n","Epoch 472, acc0.93, loss 0.1838, tol 0, val_acc0.91, val_loss0.2428\n","Epoch 473, acc0.93, loss 0.1833, tol 0, val_acc0.91, val_loss0.2482\n","Epoch 474, acc0.92, loss 0.1960, tol 0, val_acc0.91, val_loss0.2488\n","Epoch 475, acc0.93, loss 0.1987, tol 0, val_acc0.90, val_loss0.2496\n","Epoch 476, acc0.93, loss 0.1771, tol 0, val_acc0.90, val_loss0.2489\n","Epoch 477, acc0.93, loss 0.1783, tol 0, val_acc0.90, val_loss0.2425\n","Epoch 478, acc0.92, loss 0.1945, tol 0, val_acc0.90, val_loss0.2528\n","Epoch 479, acc0.93, loss 0.1875, tol 0, val_acc0.90, val_loss0.2502\n","Epoch 480, acc0.92, loss 0.2131, tol 0, val_acc0.90, val_loss0.2465\n","Epoch 481, acc0.93, loss 0.2030, tol 0, val_acc0.90, val_loss0.2388\n","Epoch 482, acc0.93, loss 0.1787, tol 0, val_acc0.90, val_loss0.2490\n","Epoch 483, acc0.94, loss 0.1744, tol 0, val_acc0.90, val_loss0.2471\n","Epoch 484, acc0.93, loss 0.1666, tol 0, val_acc0.90, val_loss0.2584\n","Epoch 485, acc0.93, loss 0.1794, tol 0, val_acc0.90, val_loss0.2464\n","Epoch 486, acc0.92, loss 0.2008, tol 0, val_acc0.90, val_loss0.2540\n","Epoch 487, acc0.92, loss 0.1920, tol 0, val_acc0.91, val_loss0.2503\n","Epoch 488, acc0.92, loss 0.1903, tol 0, val_acc0.90, val_loss0.2441\n","Epoch 489, acc0.92, loss 0.2069, tol 0, val_acc0.90, val_loss0.2492\n","Epoch 490, acc0.93, loss 0.2006, tol 0, val_acc0.90, val_loss0.2495\n","Epoch 491, acc0.92, loss 0.2067, tol 0, val_acc0.90, val_loss0.2458\n","Epoch 492, acc0.93, loss 0.1852, tol 0, val_acc0.91, val_loss0.2496\n","Epoch 493, acc0.93, loss 0.1705, tol 0, val_acc0.91, val_loss0.2473\n","Epoch 494, acc0.92, loss 0.1960, tol 0, val_acc0.90, val_loss0.2560\n","Epoch 495, acc0.92, loss 0.1941, tol 0, val_acc0.91, val_loss0.2488\n","Epoch 496, acc0.93, loss 0.1731, tol 0, val_acc0.90, val_loss0.2490\n","Epoch 497, acc0.93, loss 0.1935, tol 0, val_acc0.90, val_loss0.2444\n","Epoch 498, acc0.93, loss 0.1792, tol 0, val_acc0.91, val_loss0.2613\n","Epoch 499, acc0.93, loss 0.2002, tol 0, val_acc0.90, val_loss0.2550\n","Epoch 500, acc0.92, loss 0.2043, tol 0, val_acc0.90, val_loss0.2539\n","Epoch 501, acc0.93, loss 0.1828, tol 1, val_acc0.90, val_loss0.2494\n","Reach Max Epoch Number\n","+-----------------+------------+\n","|     Modules     | Parameters |\n","+-----------------+------------+\n","|   conv1.weight  |    160     |\n","|    conv1.bias   |     32     |\n","|   conv2.weight  |    1024    |\n","|    conv2.bias   |     32     |\n","| classify.weight |     64     |\n","|  classify.bias  |     2      |\n","+-----------------+------------+\n","Total Trainable Params: 1314\n","Epoch 0, acc0.51, loss 0.8650, tol 0, val_acc0.54, val_loss0.6894 -- checkpoint saved\n","Epoch 1, acc0.52, loss 0.7047, tol 0, val_acc0.50, val_loss0.7153\n","Epoch 2, acc0.52, loss 0.6966, tol 0, val_acc0.51, val_loss0.6918\n","Epoch 3, acc0.50, loss 0.6948, tol 0, val_acc0.51, val_loss0.6911\n","Epoch 4, acc0.54, loss 0.6893, tol 0, val_acc0.51, val_loss0.6926\n","Epoch 5, acc0.53, loss 0.6918, tol 0, val_acc0.56, val_loss0.6882 -- checkpoint saved\n","Epoch 6, acc0.53, loss 0.6900, tol 0, val_acc0.54, val_loss0.6884\n","Epoch 7, acc0.54, loss 0.6885, tol 0, val_acc0.55, val_loss0.6877\n","Epoch 8, acc0.56, loss 0.6873, tol 0, val_acc0.56, val_loss0.6869 -- checkpoint saved\n","Epoch 9, acc0.57, loss 0.6849, tol 0, val_acc0.55, val_loss0.6861\n","Epoch 10, acc0.53, loss 0.6886, tol 0, val_acc0.55, val_loss0.6858\n","Epoch 11, acc0.54, loss 0.6871, tol 0, val_acc0.54, val_loss0.6879\n","Epoch 12, acc0.53, loss 0.6888, tol 0, val_acc0.55, val_loss0.6874\n","Epoch 13, acc0.52, loss 0.6899, tol 0, val_acc0.56, val_loss0.6845\n","Epoch 14, acc0.57, loss 0.6836, tol 0, val_acc0.54, val_loss0.6897\n","Epoch 15, acc0.53, loss 0.6897, tol 0, val_acc0.53, val_loss0.6871\n","Epoch 16, acc0.55, loss 0.6868, tol 0, val_acc0.54, val_loss0.6873\n","Epoch 17, acc0.54, loss 0.6854, tol 0, val_acc0.56, val_loss0.6835\n","Epoch 18, acc0.56, loss 0.6834, tol 0, val_acc0.57, val_loss0.6836\n","Epoch 19, acc0.56, loss 0.6860, tol 0, val_acc0.55, val_loss0.6846\n","Epoch 20, acc0.57, loss 0.6815, tol 0, val_acc0.57, val_loss0.6834\n","Epoch 21, acc0.53, loss 0.6879, tol 0, val_acc0.57, val_loss0.6810\n","Epoch 22, acc0.56, loss 0.6841, tol 0, val_acc0.57, val_loss0.6820\n","Epoch 23, acc0.56, loss 0.6828, tol 0, val_acc0.54, val_loss0.6839\n","Epoch 24, acc0.56, loss 0.6867, tol 0, val_acc0.57, val_loss0.6800 -- checkpoint saved\n","Epoch 25, acc0.58, loss 0.6798, tol 0, val_acc0.59, val_loss0.6801\n","Epoch 26, acc0.57, loss 0.6811, tol 0, val_acc0.58, val_loss0.6788\n","Epoch 27, acc0.57, loss 0.6823, tol 0, val_acc0.58, val_loss0.6794\n","Epoch 28, acc0.57, loss 0.6768, tol 0, val_acc0.58, val_loss0.6785\n","Epoch 29, acc0.58, loss 0.6741, tol 0, val_acc0.54, val_loss0.6852\n","Epoch 30, acc0.59, loss 0.6773, tol 0, val_acc0.56, val_loss0.6806\n","Epoch 31, acc0.58, loss 0.6797, tol 0, val_acc0.54, val_loss0.6871\n","Epoch 32, acc0.55, loss 0.6870, tol 0, val_acc0.52, val_loss0.6863\n","Epoch 33, acc0.54, loss 0.6864, tol 0, val_acc0.58, val_loss0.6765\n","Epoch 34, acc0.54, loss 0.6837, tol 0, val_acc0.59, val_loss0.6758 -- checkpoint saved\n","Epoch 35, acc0.57, loss 0.6776, tol 0, val_acc0.55, val_loss0.6810\n","Epoch 36, acc0.54, loss 0.6851, tol 0, val_acc0.50, val_loss0.6981\n","Epoch 37, acc0.54, loss 0.6884, tol 0, val_acc0.54, val_loss0.6803\n","Epoch 38, acc0.56, loss 0.6834, tol 0, val_acc0.59, val_loss0.6752 -- checkpoint saved\n","Epoch 39, acc0.58, loss 0.6752, tol 0, val_acc0.60, val_loss0.6749 -- checkpoint saved\n","Epoch 40, acc0.57, loss 0.6782, tol 0, val_acc0.60, val_loss0.6745 -- checkpoint saved\n","Epoch 41, acc0.56, loss 0.6803, tol 0, val_acc0.59, val_loss0.6749\n","Epoch 42, acc0.59, loss 0.6755, tol 0, val_acc0.60, val_loss0.6738\n","Epoch 43, acc0.57, loss 0.6784, tol 0, val_acc0.60, val_loss0.6737 -- checkpoint saved\n","Epoch 44, acc0.57, loss 0.6794, tol 0, val_acc0.58, val_loss0.6759\n","Epoch 45, acc0.57, loss 0.6785, tol 0, val_acc0.60, val_loss0.6728\n","Epoch 46, acc0.55, loss 0.6833, tol 0, val_acc0.57, val_loss0.6777\n","Epoch 47, acc0.57, loss 0.6806, tol 0, val_acc0.59, val_loss0.6728\n","Epoch 48, acc0.58, loss 0.6760, tol 0, val_acc0.58, val_loss0.6740\n","Epoch 49, acc0.56, loss 0.6809, tol 0, val_acc0.59, val_loss0.6734\n","Epoch 50, acc0.57, loss 0.6769, tol 0, val_acc0.60, val_loss0.6721\n","Epoch 51, acc0.60, loss 0.6687, tol 0, val_acc0.60, val_loss0.6717\n","Epoch 52, acc0.58, loss 0.6755, tol 0, val_acc0.61, val_loss0.6710 -- checkpoint saved\n","Epoch 53, acc0.61, loss 0.6734, tol 0, val_acc0.61, val_loss0.6708\n","Epoch 54, acc0.58, loss 0.6723, tol 0, val_acc0.55, val_loss0.6802\n","Epoch 55, acc0.57, loss 0.6798, tol 0, val_acc0.57, val_loss0.6757\n","Epoch 56, acc0.59, loss 0.6694, tol 0, val_acc0.58, val_loss0.6712\n","Epoch 57, acc0.58, loss 0.6732, tol 0, val_acc0.61, val_loss0.6693 -- checkpoint saved\n","Epoch 58, acc0.59, loss 0.6730, tol 0, val_acc0.60, val_loss0.6705\n","Epoch 59, acc0.58, loss 0.6709, tol 0, val_acc0.58, val_loss0.6708\n","Epoch 60, acc0.57, loss 0.6736, tol 0, val_acc0.58, val_loss0.6694\n","Epoch 61, acc0.60, loss 0.6689, tol 0, val_acc0.62, val_loss0.6679 -- checkpoint saved\n","Epoch 62, acc0.58, loss 0.6717, tol 0, val_acc0.59, val_loss0.6722\n","Epoch 63, acc0.59, loss 0.6701, tol 0, val_acc0.59, val_loss0.6682\n","Epoch 64, acc0.59, loss 0.6730, tol 0, val_acc0.57, val_loss0.6745\n","Epoch 65, acc0.58, loss 0.6764, tol 0, val_acc0.57, val_loss0.6744\n","Epoch 66, acc0.58, loss 0.6738, tol 0, val_acc0.60, val_loss0.6698\n","Epoch 67, acc0.58, loss 0.6731, tol 0, val_acc0.59, val_loss0.6679\n","Epoch 68, acc0.60, loss 0.6740, tol 0, val_acc0.57, val_loss0.6696\n","Epoch 69, acc0.59, loss 0.6647, tol 0, val_acc0.57, val_loss0.6768\n","Epoch 70, acc0.58, loss 0.6718, tol 0, val_acc0.62, val_loss0.6656 -- checkpoint saved\n","Epoch 71, acc0.58, loss 0.6717, tol 0, val_acc0.62, val_loss0.6654 -- checkpoint saved\n","Epoch 72, acc0.59, loss 0.6703, tol 0, val_acc0.62, val_loss0.6648\n","Epoch 73, acc0.60, loss 0.6683, tol 0, val_acc0.61, val_loss0.6666\n","Epoch 74, acc0.57, loss 0.6759, tol 0, val_acc0.56, val_loss0.6768\n","Epoch 75, acc0.59, loss 0.6705, tol 0, val_acc0.58, val_loss0.6675\n","Epoch 76, acc0.56, loss 0.6802, tol 0, val_acc0.63, val_loss0.6642 -- checkpoint saved\n","Epoch 77, acc0.60, loss 0.6726, tol 0, val_acc0.63, val_loss0.6639 -- checkpoint saved\n","Epoch 78, acc0.58, loss 0.6712, tol 0, val_acc0.63, val_loss0.6634\n","Epoch 79, acc0.58, loss 0.6716, tol 0, val_acc0.60, val_loss0.6661\n","Epoch 80, acc0.58, loss 0.6705, tol 0, val_acc0.59, val_loss0.6688\n","Epoch 81, acc0.59, loss 0.6723, tol 0, val_acc0.63, val_loss0.6627 -- checkpoint saved\n","Epoch 82, acc0.59, loss 0.6703, tol 0, val_acc0.57, val_loss0.6722\n","Epoch 83, acc0.59, loss 0.6699, tol 0, val_acc0.58, val_loss0.6665\n","Epoch 84, acc0.60, loss 0.6667, tol 0, val_acc0.62, val_loss0.6624\n","Epoch 85, acc0.57, loss 0.6743, tol 0, val_acc0.58, val_loss0.6727\n","Epoch 86, acc0.55, loss 0.6785, tol 0, val_acc0.56, val_loss0.6753\n","Epoch 87, acc0.59, loss 0.6668, tol 0, val_acc0.58, val_loss0.6683\n","Epoch 88, acc0.59, loss 0.6672, tol 0, val_acc0.58, val_loss0.6653\n","Epoch 89, acc0.60, loss 0.6662, tol 0, val_acc0.61, val_loss0.6657\n","Epoch 90, acc0.58, loss 0.6720, tol 0, val_acc0.62, val_loss0.6615\n","Epoch 91, acc0.59, loss 0.6678, tol 0, val_acc0.61, val_loss0.6614\n","Epoch 92, acc0.61, loss 0.6623, tol 0, val_acc0.63, val_loss0.6605\n","Epoch 93, acc0.59, loss 0.6698, tol 0, val_acc0.58, val_loss0.6694\n","Epoch 94, acc0.56, loss 0.6770, tol 0, val_acc0.63, val_loss0.6600\n","Epoch 95, acc0.60, loss 0.6678, tol 0, val_acc0.62, val_loss0.6601\n","Epoch 96, acc0.61, loss 0.6645, tol 0, val_acc0.62, val_loss0.6611\n","Epoch 97, acc0.59, loss 0.6699, tol 0, val_acc0.59, val_loss0.6657\n","Epoch 98, acc0.58, loss 0.6712, tol 0, val_acc0.64, val_loss0.6594 -- checkpoint saved\n","Epoch 99, acc0.60, loss 0.6649, tol 0, val_acc0.55, val_loss0.6792\n","Epoch 100, acc0.57, loss 0.6737, tol 0, val_acc0.62, val_loss0.6598\n","Epoch 101, acc0.60, loss 0.6636, tol 0, val_acc0.56, val_loss0.6726\n","Epoch 102, acc0.59, loss 0.6709, tol 0, val_acc0.63, val_loss0.6591\n","Epoch 103, acc0.60, loss 0.6651, tol 0, val_acc0.64, val_loss0.6588 -- checkpoint saved\n","Epoch 104, acc0.61, loss 0.6651, tol 0, val_acc0.64, val_loss0.6582 -- checkpoint saved\n","Epoch 105, acc0.60, loss 0.6602, tol 0, val_acc0.62, val_loss0.6596\n","Epoch 106, acc0.59, loss 0.6668, tol 0, val_acc0.62, val_loss0.6595\n","Epoch 107, acc0.61, loss 0.6586, tol 0, val_acc0.62, val_loss0.6590\n","Epoch 108, acc0.61, loss 0.6623, tol 0, val_acc0.63, val_loss0.6569\n","Epoch 109, acc0.62, loss 0.6592, tol 0, val_acc0.63, val_loss0.6569\n","Epoch 110, acc0.59, loss 0.6668, tol 0, val_acc0.63, val_loss0.6560\n","Epoch 111, acc0.60, loss 0.6606, tol 0, val_acc0.59, val_loss0.6636\n","Epoch 112, acc0.60, loss 0.6622, tol 0, val_acc0.63, val_loss0.6558\n","Epoch 113, acc0.61, loss 0.6623, tol 0, val_acc0.61, val_loss0.6621\n","Epoch 114, acc0.60, loss 0.6597, tol 0, val_acc0.57, val_loss0.6686\n","Epoch 115, acc0.59, loss 0.6700, tol 0, val_acc0.63, val_loss0.6551\n","Epoch 116, acc0.63, loss 0.6592, tol 0, val_acc0.63, val_loss0.6553\n","Epoch 117, acc0.63, loss 0.6552, tol 0, val_acc0.63, val_loss0.6538\n","Epoch 118, acc0.63, loss 0.6568, tol 0, val_acc0.63, val_loss0.6536\n","Epoch 119, acc0.63, loss 0.6596, tol 0, val_acc0.63, val_loss0.6540\n","Epoch 120, acc0.60, loss 0.6571, tol 0, val_acc0.61, val_loss0.6591\n","Epoch 121, acc0.59, loss 0.6625, tol 0, val_acc0.59, val_loss0.6620\n","Epoch 122, acc0.60, loss 0.6562, tol 0, val_acc0.58, val_loss0.6733\n","Epoch 123, acc0.60, loss 0.6660, tol 0, val_acc0.64, val_loss0.6529\n","Epoch 124, acc0.64, loss 0.6495, tol 0, val_acc0.63, val_loss0.6525\n","Epoch 125, acc0.62, loss 0.6551, tol 0, val_acc0.62, val_loss0.6545\n","Epoch 126, acc0.62, loss 0.6553, tol 0, val_acc0.63, val_loss0.6524\n","Epoch 127, acc0.63, loss 0.6560, tol 0, val_acc0.64, val_loss0.6515\n","Epoch 128, acc0.62, loss 0.6527, tol 0, val_acc0.62, val_loss0.6567\n","Epoch 129, acc0.62, loss 0.6561, tol 0, val_acc0.63, val_loss0.6503\n","Epoch 130, acc0.63, loss 0.6539, tol 0, val_acc0.63, val_loss0.6517\n","Epoch 131, acc0.61, loss 0.6541, tol 0, val_acc0.65, val_loss0.6495 -- checkpoint saved\n","Epoch 132, acc0.61, loss 0.6607, tol 0, val_acc0.62, val_loss0.6507\n","Epoch 133, acc0.61, loss 0.6600, tol 0, val_acc0.63, val_loss0.6495\n","Epoch 134, acc0.64, loss 0.6517, tol 0, val_acc0.63, val_loss0.6512\n","Epoch 135, acc0.63, loss 0.6545, tol 0, val_acc0.64, val_loss0.6486\n","Epoch 136, acc0.61, loss 0.6562, tol 0, val_acc0.63, val_loss0.6500\n","Epoch 137, acc0.64, loss 0.6535, tol 0, val_acc0.64, val_loss0.6476\n","Epoch 138, acc0.64, loss 0.6525, tol 0, val_acc0.62, val_loss0.6492\n","Epoch 139, acc0.64, loss 0.6488, tol 0, val_acc0.62, val_loss0.6520\n","Epoch 140, acc0.66, loss 0.6418, tol 0, val_acc0.64, val_loss0.6476\n","Epoch 141, acc0.62, loss 0.6535, tol 0, val_acc0.64, val_loss0.6464\n","Epoch 142, acc0.63, loss 0.6522, tol 0, val_acc0.64, val_loss0.6468\n","Epoch 143, acc0.64, loss 0.6445, tol 0, val_acc0.62, val_loss0.6501\n","Epoch 144, acc0.62, loss 0.6503, tol 0, val_acc0.61, val_loss0.6559\n","Epoch 145, acc0.63, loss 0.6544, tol 0, val_acc0.65, val_loss0.6448\n","Epoch 146, acc0.61, loss 0.6605, tol 0, val_acc0.63, val_loss0.6500\n","Epoch 147, acc0.64, loss 0.6486, tol 0, val_acc0.64, val_loss0.6470\n","Epoch 148, acc0.61, loss 0.6542, tol 0, val_acc0.65, val_loss0.6436\n","Epoch 149, acc0.62, loss 0.6484, tol 0, val_acc0.62, val_loss0.6491\n","Epoch 150, acc0.64, loss 0.6490, tol 0, val_acc0.61, val_loss0.6527\n","Epoch 151, acc0.63, loss 0.6501, tol 0, val_acc0.67, val_loss0.6437\n","Epoch 152, acc0.63, loss 0.6473, tol 0, val_acc0.63, val_loss0.6470\n","Epoch 153, acc0.62, loss 0.6519, tol 0, val_acc0.64, val_loss0.6443\n","Epoch 154, acc0.64, loss 0.6496, tol 0, val_acc0.64, val_loss0.6423\n","Epoch 155, acc0.64, loss 0.6419, tol 0, val_acc0.64, val_loss0.6415\n","Epoch 156, acc0.63, loss 0.6492, tol 0, val_acc0.64, val_loss0.6432\n","Epoch 157, acc0.65, loss 0.6395, tol 0, val_acc0.65, val_loss0.6413\n","Epoch 158, acc0.62, loss 0.6563, tol 0, val_acc0.64, val_loss0.6447\n","Epoch 159, acc0.60, loss 0.6628, tol 0, val_acc0.65, val_loss0.6416\n","Epoch 160, acc0.62, loss 0.6494, tol 0, val_acc0.65, val_loss0.6435\n","Epoch 161, acc0.63, loss 0.6437, tol 0, val_acc0.65, val_loss0.6424\n","Epoch 162, acc0.64, loss 0.6434, tol 0, val_acc0.63, val_loss0.6464\n","Epoch 163, acc0.64, loss 0.6482, tol 0, val_acc0.65, val_loss0.6394\n","Epoch 164, acc0.64, loss 0.6463, tol 0, val_acc0.63, val_loss0.6440\n","Epoch 165, acc0.64, loss 0.6478, tol 0, val_acc0.62, val_loss0.6475\n","Epoch 166, acc0.62, loss 0.6510, tol 0, val_acc0.65, val_loss0.6414\n","Epoch 167, acc0.65, loss 0.6406, tol 0, val_acc0.65, val_loss0.6391\n","Epoch 168, acc0.63, loss 0.6475, tol 0, val_acc0.65, val_loss0.6365\n","Epoch 169, acc0.64, loss 0.6406, tol 0, val_acc0.65, val_loss0.6420\n","Epoch 170, acc0.61, loss 0.6506, tol 0, val_acc0.65, val_loss0.6360\n","Epoch 171, acc0.65, loss 0.6440, tol 0, val_acc0.65, val_loss0.6381\n","Epoch 172, acc0.64, loss 0.6446, tol 0, val_acc0.64, val_loss0.6401\n","Epoch 173, acc0.66, loss 0.6283, tol 0, val_acc0.65, val_loss0.6346\n","Epoch 174, acc0.63, loss 0.6480, tol 0, val_acc0.65, val_loss0.6360\n","Epoch 175, acc0.64, loss 0.6444, tol 0, val_acc0.65, val_loss0.6347\n","Epoch 176, acc0.66, loss 0.6303, tol 0, val_acc0.65, val_loss0.6340\n","Epoch 177, acc0.63, loss 0.6423, tol 0, val_acc0.65, val_loss0.6327\n","Epoch 178, acc0.64, loss 0.6415, tol 0, val_acc0.66, val_loss0.6337\n","Epoch 179, acc0.64, loss 0.6361, tol 0, val_acc0.63, val_loss0.6443\n","Epoch 180, acc0.65, loss 0.6370, tol 0, val_acc0.65, val_loss0.6315\n","Epoch 181, acc0.67, loss 0.6319, tol 0, val_acc0.65, val_loss0.6343\n","Epoch 182, acc0.64, loss 0.6463, tol 0, val_acc0.65, val_loss0.6312\n","Epoch 183, acc0.64, loss 0.6388, tol 0, val_acc0.66, val_loss0.6354\n","Epoch 184, acc0.65, loss 0.6394, tol 0, val_acc0.65, val_loss0.6298\n","Epoch 185, acc0.68, loss 0.6231, tol 0, val_acc0.66, val_loss0.6296\n","Epoch 186, acc0.66, loss 0.6355, tol 0, val_acc0.64, val_loss0.6318\n","Epoch 187, acc0.66, loss 0.6296, tol 0, val_acc0.65, val_loss0.6283\n","Epoch 188, acc0.64, loss 0.6392, tol 0, val_acc0.66, val_loss0.6326\n","Epoch 189, acc0.65, loss 0.6361, tol 0, val_acc0.65, val_loss0.6285\n","Epoch 190, acc0.64, loss 0.6405, tol 0, val_acc0.66, val_loss0.6298\n","Epoch 191, acc0.63, loss 0.6398, tol 0, val_acc0.64, val_loss0.6292\n","Epoch 192, acc0.65, loss 0.6424, tol 0, val_acc0.65, val_loss0.6325\n","Epoch 193, acc0.64, loss 0.6417, tol 0, val_acc0.61, val_loss0.6508\n","Epoch 194, acc0.65, loss 0.6368, tol 0, val_acc0.66, val_loss0.6257\n","Epoch 195, acc0.68, loss 0.6258, tol 0, val_acc0.66, val_loss0.6248\n","Epoch 196, acc0.64, loss 0.6357, tol 0, val_acc0.66, val_loss0.6243\n","Epoch 197, acc0.65, loss 0.6291, tol 0, val_acc0.66, val_loss0.6239\n","Epoch 198, acc0.65, loss 0.6340, tol 0, val_acc0.66, val_loss0.6234\n","Epoch 199, acc0.68, loss 0.6266, tol 0, val_acc0.66, val_loss0.6227\n","Epoch 200, acc0.68, loss 0.6238, tol 0, val_acc0.66, val_loss0.6229\n","Epoch 201, acc0.67, loss 0.6183, tol 0, val_acc0.66, val_loss0.6222\n","Epoch 202, acc0.67, loss 0.6187, tol 0, val_acc0.66, val_loss0.6210\n","Epoch 203, acc0.64, loss 0.6421, tol 0, val_acc0.66, val_loss0.6208\n","Epoch 204, acc0.66, loss 0.6229, tol 0, val_acc0.66, val_loss0.6202\n","Epoch 205, acc0.68, loss 0.6235, tol 0, val_acc0.66, val_loss0.6192\n","Epoch 206, acc0.67, loss 0.6296, tol 0, val_acc0.67, val_loss0.6218\n","Epoch 207, acc0.66, loss 0.6257, tol 0, val_acc0.66, val_loss0.6183\n","Epoch 208, acc0.65, loss 0.6266, tol 0, val_acc0.67, val_loss0.6218\n","Epoch 209, acc0.67, loss 0.6248, tol 0, val_acc0.67, val_loss0.6197\n","Epoch 210, acc0.66, loss 0.6225, tol 0, val_acc0.67, val_loss0.6210\n","Epoch 211, acc0.70, loss 0.6016, tol 0, val_acc0.67, val_loss0.6258\n","Epoch 212, acc0.67, loss 0.6201, tol 0, val_acc0.67, val_loss0.6172\n","Epoch 213, acc0.67, loss 0.6251, tol 0, val_acc0.67, val_loss0.6173\n","Epoch 214, acc0.68, loss 0.6199, tol 0, val_acc0.67, val_loss0.6238\n","Epoch 215, acc0.66, loss 0.6300, tol 0, val_acc0.68, val_loss0.6136 -- checkpoint saved\n","Epoch 216, acc0.64, loss 0.6325, tol 0, val_acc0.67, val_loss0.6133\n","Epoch 217, acc0.66, loss 0.6254, tol 0, val_acc0.66, val_loss0.6131\n","Epoch 218, acc0.67, loss 0.6210, tol 0, val_acc0.66, val_loss0.6127\n","Epoch 219, acc0.66, loss 0.6219, tol 0, val_acc0.68, val_loss0.6117 -- checkpoint saved\n","Epoch 220, acc0.69, loss 0.6210, tol 0, val_acc0.69, val_loss0.6108 -- checkpoint saved\n","Epoch 221, acc0.69, loss 0.6137, tol 0, val_acc0.68, val_loss0.6101\n","Epoch 222, acc0.66, loss 0.6259, tol 0, val_acc0.67, val_loss0.6097\n","Epoch 223, acc0.70, loss 0.6035, tol 0, val_acc0.68, val_loss0.6125\n","Epoch 224, acc0.67, loss 0.6123, tol 0, val_acc0.68, val_loss0.6079\n","Epoch 225, acc0.69, loss 0.6139, tol 0, val_acc0.68, val_loss0.6072\n","Epoch 226, acc0.67, loss 0.6138, tol 0, val_acc0.68, val_loss0.6081\n","Epoch 227, acc0.64, loss 0.6315, tol 0, val_acc0.67, val_loss0.6125\n","Epoch 228, acc0.68, loss 0.6082, tol 0, val_acc0.67, val_loss0.6068\n","Epoch 229, acc0.68, loss 0.6051, tol 0, val_acc0.68, val_loss0.6055\n","Epoch 230, acc0.68, loss 0.6153, tol 0, val_acc0.67, val_loss0.6049\n","Epoch 231, acc0.70, loss 0.6059, tol 0, val_acc0.69, val_loss0.6084\n","Epoch 232, acc0.67, loss 0.6158, tol 0, val_acc0.68, val_loss0.6101\n","Epoch 233, acc0.69, loss 0.6144, tol 0, val_acc0.69, val_loss0.6085\n","Epoch 234, acc0.69, loss 0.6093, tol 0, val_acc0.70, val_loss0.6058\n","Epoch 235, acc0.68, loss 0.6094, tol 0, val_acc0.69, val_loss0.6098\n","Epoch 236, acc0.67, loss 0.6178, tol 0, val_acc0.67, val_loss0.6120\n","Epoch 237, acc0.69, loss 0.6065, tol 0, val_acc0.69, val_loss0.6078\n","Epoch 238, acc0.67, loss 0.6214, tol 0, val_acc0.70, val_loss0.6022 -- checkpoint saved\n","Epoch 239, acc0.70, loss 0.6102, tol 0, val_acc0.69, val_loss0.5997\n","Epoch 240, acc0.69, loss 0.6147, tol 0, val_acc0.70, val_loss0.5995\n","Epoch 241, acc0.69, loss 0.6122, tol 0, val_acc0.68, val_loss0.5985\n","Epoch 242, acc0.68, loss 0.6105, tol 0, val_acc0.71, val_loss0.5995\n","Epoch 243, acc0.68, loss 0.6137, tol 0, val_acc0.70, val_loss0.5956\n","Epoch 244, acc0.68, loss 0.6069, tol 0, val_acc0.70, val_loss0.5993\n","Epoch 245, acc0.69, loss 0.5988, tol 0, val_acc0.68, val_loss0.5959\n","Epoch 246, acc0.70, loss 0.6035, tol 0, val_acc0.69, val_loss0.5935\n","Epoch 247, acc0.71, loss 0.5912, tol 0, val_acc0.69, val_loss0.5934\n","Epoch 248, acc0.69, loss 0.5998, tol 0, val_acc0.70, val_loss0.5940\n","Epoch 249, acc0.70, loss 0.5953, tol 0, val_acc0.69, val_loss0.5911\n","Epoch 250, acc0.71, loss 0.5913, tol 0, val_acc0.70, val_loss0.5902\n","Epoch 251, acc0.71, loss 0.5865, tol 0, val_acc0.71, val_loss0.5941\n","Epoch 252, acc0.69, loss 0.6024, tol 0, val_acc0.66, val_loss0.6081\n","Epoch 253, acc0.69, loss 0.5981, tol 0, val_acc0.70, val_loss0.5925\n","Epoch 254, acc0.70, loss 0.5897, tol 0, val_acc0.69, val_loss0.5954\n","Epoch 255, acc0.70, loss 0.5957, tol 0, val_acc0.70, val_loss0.5867\n","Epoch 256, acc0.71, loss 0.6009, tol 0, val_acc0.72, val_loss0.5889\n","Epoch 257, acc0.71, loss 0.5920, tol 0, val_acc0.70, val_loss0.5849\n","Epoch 258, acc0.71, loss 0.5950, tol 0, val_acc0.70, val_loss0.5843\n","Epoch 259, acc0.69, loss 0.6103, tol 0, val_acc0.70, val_loss0.5839\n","Epoch 260, acc0.71, loss 0.5913, tol 0, val_acc0.71, val_loss0.5830\n","Epoch 261, acc0.73, loss 0.5809, tol 0, val_acc0.70, val_loss0.5828\n","Epoch 262, acc0.70, loss 0.5876, tol 0, val_acc0.69, val_loss0.5845\n","Epoch 263, acc0.74, loss 0.5759, tol 0, val_acc0.71, val_loss0.5845\n","Epoch 264, acc0.71, loss 0.5908, tol 0, val_acc0.71, val_loss0.5797\n","Epoch 265, acc0.71, loss 0.5827, tol 0, val_acc0.73, val_loss0.5834\n","Epoch 266, acc0.71, loss 0.5833, tol 0, val_acc0.70, val_loss0.5776\n","Epoch 267, acc0.69, loss 0.6000, tol 0, val_acc0.70, val_loss0.5847\n","Epoch 268, acc0.68, loss 0.5967, tol 0, val_acc0.71, val_loss0.5815\n","Epoch 269, acc0.71, loss 0.5864, tol 0, val_acc0.69, val_loss0.5768\n","Epoch 270, acc0.72, loss 0.5848, tol 0, val_acc0.71, val_loss0.5749\n","Epoch 271, acc0.71, loss 0.5830, tol 0, val_acc0.72, val_loss0.5755\n","Epoch 272, acc0.70, loss 0.5931, tol 0, val_acc0.70, val_loss0.5761\n","Epoch 273, acc0.74, loss 0.5681, tol 0, val_acc0.71, val_loss0.5722\n","Epoch 274, acc0.74, loss 0.5700, tol 0, val_acc0.71, val_loss0.5739\n","Epoch 275, acc0.71, loss 0.5860, tol 0, val_acc0.71, val_loss0.5720\n","Epoch 276, acc0.74, loss 0.5670, tol 0, val_acc0.72, val_loss0.5714\n","Epoch 277, acc0.71, loss 0.5782, tol 0, val_acc0.71, val_loss0.5728\n","Epoch 278, acc0.71, loss 0.5927, tol 0, val_acc0.70, val_loss0.5845\n","Epoch 279, acc0.72, loss 0.5822, tol 0, val_acc0.71, val_loss0.5677\n","Epoch 280, acc0.72, loss 0.5789, tol 0, val_acc0.74, val_loss0.5716\n","Epoch 281, acc0.72, loss 0.5770, tol 0, val_acc0.72, val_loss0.5769\n","Epoch 282, acc0.71, loss 0.5795, tol 0, val_acc0.73, val_loss0.5703\n","Epoch 283, acc0.71, loss 0.5776, tol 0, val_acc0.74, val_loss0.5714\n","Epoch 284, acc0.72, loss 0.5717, tol 0, val_acc0.72, val_loss0.5732\n","Epoch 285, acc0.72, loss 0.5816, tol 0, val_acc0.73, val_loss0.5630\n","Epoch 286, acc0.73, loss 0.5730, tol 0, val_acc0.72, val_loss0.5623\n","Epoch 287, acc0.75, loss 0.5540, tol 0, val_acc0.71, val_loss0.5611\n","Epoch 288, acc0.74, loss 0.5588, tol 0, val_acc0.71, val_loss0.5606\n","Epoch 289, acc0.72, loss 0.5728, tol 0, val_acc0.71, val_loss0.5594\n","Epoch 290, acc0.73, loss 0.5686, tol 0, val_acc0.73, val_loss0.5614\n","Epoch 291, acc0.73, loss 0.5614, tol 0, val_acc0.72, val_loss0.5603\n","Epoch 292, acc0.73, loss 0.5590, tol 0, val_acc0.72, val_loss0.5569\n","Epoch 293, acc0.72, loss 0.5718, tol 0, val_acc0.71, val_loss0.5610\n","Epoch 294, acc0.72, loss 0.5731, tol 0, val_acc0.72, val_loss0.5571\n","Epoch 295, acc0.73, loss 0.5641, tol 0, val_acc0.73, val_loss0.5585\n","Epoch 296, acc0.75, loss 0.5589, tol 0, val_acc0.72, val_loss0.5545\n","Epoch 297, acc0.74, loss 0.5593, tol 0, val_acc0.72, val_loss0.5532\n","Epoch 298, acc0.72, loss 0.5709, tol 0, val_acc0.73, val_loss0.5609\n","Epoch 299, acc0.72, loss 0.5763, tol 0, val_acc0.72, val_loss0.5655\n","Epoch 300, acc0.73, loss 0.5597, tol 0, val_acc0.73, val_loss0.5518\n","Epoch 301, acc0.73, loss 0.5634, tol 0, val_acc0.73, val_loss0.5512\n","Epoch 302, acc0.73, loss 0.5596, tol 0, val_acc0.73, val_loss0.5509\n","Epoch 303, acc0.74, loss 0.5567, tol 0, val_acc0.74, val_loss0.5588\n","Epoch 304, acc0.75, loss 0.5491, tol 0, val_acc0.73, val_loss0.5490\n","Epoch 305, acc0.74, loss 0.5533, tol 0, val_acc0.72, val_loss0.5504\n","Epoch 306, acc0.75, loss 0.5594, tol 0, val_acc0.73, val_loss0.5487\n","Epoch 307, acc0.75, loss 0.5593, tol 0, val_acc0.74, val_loss0.5460 -- checkpoint saved\n","Epoch 308, acc0.75, loss 0.5538, tol 0, val_acc0.74, val_loss0.5533\n","Epoch 309, acc0.74, loss 0.5598, tol 0, val_acc0.74, val_loss0.5452\n","Epoch 310, acc0.75, loss 0.5484, tol 0, val_acc0.74, val_loss0.5433\n","Epoch 311, acc0.75, loss 0.5545, tol 0, val_acc0.73, val_loss0.5433\n","Epoch 312, acc0.76, loss 0.5410, tol 0, val_acc0.73, val_loss0.5435\n","Epoch 313, acc0.71, loss 0.5668, tol 0, val_acc0.73, val_loss0.5499\n","Epoch 314, acc0.73, loss 0.5631, tol 0, val_acc0.75, val_loss0.5403 -- checkpoint saved\n","Epoch 315, acc0.76, loss 0.5454, tol 0, val_acc0.74, val_loss0.5397\n","Epoch 316, acc0.76, loss 0.5422, tol 0, val_acc0.73, val_loss0.5400\n","Epoch 317, acc0.75, loss 0.5433, tol 0, val_acc0.75, val_loss0.5385 -- checkpoint saved\n","Epoch 318, acc0.74, loss 0.5437, tol 0, val_acc0.75, val_loss0.5375 -- checkpoint saved\n","Epoch 319, acc0.74, loss 0.5528, tol 0, val_acc0.74, val_loss0.5366\n","Epoch 320, acc0.74, loss 0.5499, tol 0, val_acc0.73, val_loss0.5423\n","Epoch 321, acc0.76, loss 0.5312, tol 0, val_acc0.73, val_loss0.5350\n","Epoch 322, acc0.74, loss 0.5517, tol 0, val_acc0.74, val_loss0.5339\n","Epoch 323, acc0.73, loss 0.5558, tol 0, val_acc0.76, val_loss0.5351\n","Epoch 324, acc0.75, loss 0.5409, tol 0, val_acc0.75, val_loss0.5341\n","Epoch 325, acc0.75, loss 0.5427, tol 0, val_acc0.74, val_loss0.5322\n","Epoch 326, acc0.76, loss 0.5273, tol 0, val_acc0.74, val_loss0.5314\n","Epoch 327, acc0.74, loss 0.5510, tol 0, val_acc0.74, val_loss0.5311\n","Epoch 328, acc0.74, loss 0.5420, tol 0, val_acc0.74, val_loss0.5353\n","Epoch 329, acc0.74, loss 0.5496, tol 0, val_acc0.75, val_loss0.5294\n","Epoch 330, acc0.74, loss 0.5603, tol 0, val_acc0.75, val_loss0.5338\n","Epoch 331, acc0.76, loss 0.5357, tol 0, val_acc0.76, val_loss0.5281 -- checkpoint saved\n","Epoch 332, acc0.76, loss 0.5326, tol 0, val_acc0.74, val_loss0.5280\n","Epoch 333, acc0.76, loss 0.5282, tol 0, val_acc0.75, val_loss0.5268\n","Epoch 334, acc0.74, loss 0.5454, tol 0, val_acc0.74, val_loss0.5295\n","Epoch 335, acc0.74, loss 0.5405, tol 0, val_acc0.76, val_loss0.5248 -- checkpoint saved\n","Epoch 336, acc0.76, loss 0.5227, tol 0, val_acc0.76, val_loss0.5256\n","Epoch 337, acc0.75, loss 0.5358, tol 0, val_acc0.76, val_loss0.5237\n","Epoch 338, acc0.77, loss 0.5201, tol 0, val_acc0.76, val_loss0.5223\n","Epoch 339, acc0.75, loss 0.5405, tol 0, val_acc0.76, val_loss0.5226\n","Epoch 340, acc0.75, loss 0.5374, tol 0, val_acc0.75, val_loss0.5246\n","Epoch 341, acc0.76, loss 0.5286, tol 0, val_acc0.75, val_loss0.5231\n","Epoch 342, acc0.76, loss 0.5375, tol 0, val_acc0.75, val_loss0.5231\n","Epoch 343, acc0.76, loss 0.5346, tol 0, val_acc0.75, val_loss0.5194\n","Epoch 344, acc0.74, loss 0.5345, tol 0, val_acc0.76, val_loss0.5217\n","Epoch 345, acc0.76, loss 0.5198, tol 0, val_acc0.75, val_loss0.5212\n","Epoch 346, acc0.76, loss 0.5337, tol 0, val_acc0.76, val_loss0.5169\n","Epoch 347, acc0.77, loss 0.5218, tol 0, val_acc0.76, val_loss0.5207\n","Epoch 348, acc0.73, loss 0.5533, tol 0, val_acc0.74, val_loss0.5259\n","Epoch 349, acc0.77, loss 0.5196, tol 0, val_acc0.75, val_loss0.5220\n","Epoch 350, acc0.76, loss 0.5255, tol 0, val_acc0.76, val_loss0.5161 -- checkpoint saved\n","Epoch 351, acc0.76, loss 0.5146, tol 0, val_acc0.77, val_loss0.5136 -- checkpoint saved\n","Epoch 352, acc0.76, loss 0.5276, tol 0, val_acc0.76, val_loss0.5128\n","Epoch 353, acc0.77, loss 0.5214, tol 0, val_acc0.76, val_loss0.5133\n","Epoch 354, acc0.74, loss 0.5295, tol 0, val_acc0.77, val_loss0.5111\n","Epoch 355, acc0.78, loss 0.5092, tol 0, val_acc0.76, val_loss0.5189\n","Epoch 356, acc0.77, loss 0.5144, tol 0, val_acc0.77, val_loss0.5116\n","Epoch 357, acc0.76, loss 0.5258, tol 0, val_acc0.76, val_loss0.5138\n","Epoch 358, acc0.77, loss 0.5103, tol 0, val_acc0.77, val_loss0.5085\n","Epoch 359, acc0.75, loss 0.5262, tol 0, val_acc0.77, val_loss0.5074\n","Epoch 360, acc0.77, loss 0.5296, tol 0, val_acc0.76, val_loss0.5119\n","Epoch 361, acc0.74, loss 0.5291, tol 0, val_acc0.78, val_loss0.5063 -- checkpoint saved\n","Epoch 362, acc0.78, loss 0.5083, tol 0, val_acc0.76, val_loss0.5113\n","Epoch 363, acc0.76, loss 0.5246, tol 0, val_acc0.76, val_loss0.5127\n","Epoch 364, acc0.75, loss 0.5246, tol 0, val_acc0.77, val_loss0.5061\n","Epoch 365, acc0.77, loss 0.5199, tol 0, val_acc0.77, val_loss0.5052\n","Epoch 366, acc0.76, loss 0.5176, tol 0, val_acc0.76, val_loss0.5130\n","Epoch 367, acc0.76, loss 0.5148, tol 0, val_acc0.76, val_loss0.5064\n","Epoch 368, acc0.78, loss 0.5099, tol 0, val_acc0.76, val_loss0.5128\n","Epoch 369, acc0.74, loss 0.5401, tol 0, val_acc0.77, val_loss0.5085\n","Epoch 370, acc0.77, loss 0.5208, tol 0, val_acc0.77, val_loss0.5104\n","Epoch 371, acc0.76, loss 0.5321, tol 0, val_acc0.77, val_loss0.5058\n","Epoch 372, acc0.77, loss 0.5093, tol 0, val_acc0.78, val_loss0.4997\n","Epoch 373, acc0.76, loss 0.5125, tol 0, val_acc0.78, val_loss0.4989 -- checkpoint saved\n","Epoch 374, acc0.75, loss 0.5245, tol 0, val_acc0.75, val_loss0.5124\n","Epoch 375, acc0.78, loss 0.5066, tol 0, val_acc0.76, val_loss0.5057\n","Epoch 376, acc0.75, loss 0.5237, tol 0, val_acc0.76, val_loss0.5090\n","Epoch 377, acc0.74, loss 0.5214, tol 0, val_acc0.76, val_loss0.5150\n","Epoch 378, acc0.78, loss 0.5045, tol 0, val_acc0.77, val_loss0.5018\n","Epoch 379, acc0.77, loss 0.5084, tol 0, val_acc0.76, val_loss0.5049\n","Epoch 380, acc0.77, loss 0.5111, tol 0, val_acc0.78, val_loss0.4950 -- checkpoint saved\n","Epoch 381, acc0.76, loss 0.5135, tol 0, val_acc0.79, val_loss0.4935 -- checkpoint saved\n","Epoch 382, acc0.76, loss 0.5212, tol 0, val_acc0.79, val_loss0.4937\n","Epoch 383, acc0.77, loss 0.5095, tol 0, val_acc0.78, val_loss0.4917\n","Epoch 384, acc0.78, loss 0.5011, tol 0, val_acc0.77, val_loss0.4924\n","Epoch 385, acc0.77, loss 0.5118, tol 0, val_acc0.77, val_loss0.4941\n","Epoch 386, acc0.77, loss 0.5086, tol 0, val_acc0.79, val_loss0.4932\n","Epoch 387, acc0.75, loss 0.5133, tol 0, val_acc0.78, val_loss0.4919\n","Epoch 388, acc0.79, loss 0.5005, tol 0, val_acc0.78, val_loss0.4911\n","Epoch 389, acc0.76, loss 0.5206, tol 0, val_acc0.78, val_loss0.4912\n","Epoch 390, acc0.78, loss 0.4908, tol 0, val_acc0.79, val_loss0.4879\n","Epoch 391, acc0.79, loss 0.4956, tol 0, val_acc0.78, val_loss0.4904\n","Epoch 392, acc0.78, loss 0.5072, tol 0, val_acc0.77, val_loss0.4914\n","Epoch 393, acc0.79, loss 0.4895, tol 0, val_acc0.78, val_loss0.4855\n","Epoch 394, acc0.76, loss 0.5060, tol 0, val_acc0.78, val_loss0.4852\n","Epoch 395, acc0.78, loss 0.4936, tol 0, val_acc0.78, val_loss0.4839\n","Epoch 396, acc0.78, loss 0.4927, tol 0, val_acc0.79, val_loss0.4831\n","Epoch 397, acc0.78, loss 0.5044, tol 0, val_acc0.79, val_loss0.4845\n","Epoch 398, acc0.78, loss 0.4919, tol 0, val_acc0.79, val_loss0.4842\n","Epoch 399, acc0.77, loss 0.5068, tol 0, val_acc0.78, val_loss0.4915\n","Epoch 400, acc0.79, loss 0.4848, tol 0, val_acc0.77, val_loss0.4900\n","Epoch 401, acc0.78, loss 0.4940, tol 0, val_acc0.78, val_loss0.4862\n","Epoch 402, acc0.77, loss 0.5000, tol 0, val_acc0.79, val_loss0.4819\n","Epoch 403, acc0.76, loss 0.5098, tol 0, val_acc0.79, val_loss0.4808\n","Epoch 404, acc0.76, loss 0.5046, tol 0, val_acc0.79, val_loss0.4803\n","Epoch 405, acc0.77, loss 0.5084, tol 0, val_acc0.79, val_loss0.4784\n","Epoch 406, acc0.79, loss 0.4905, tol 0, val_acc0.79, val_loss0.4785\n","Epoch 407, acc0.80, loss 0.4878, tol 0, val_acc0.79, val_loss0.4778\n","Epoch 408, acc0.77, loss 0.5002, tol 0, val_acc0.79, val_loss0.4761\n","Epoch 409, acc0.78, loss 0.5033, tol 0, val_acc0.78, val_loss0.4812\n","Epoch 410, acc0.76, loss 0.5061, tol 0, val_acc0.77, val_loss0.4860\n","Epoch 411, acc0.77, loss 0.5024, tol 0, val_acc0.77, val_loss0.4841\n","Epoch 412, acc0.78, loss 0.4770, tol 0, val_acc0.79, val_loss0.4751\n","Epoch 413, acc0.78, loss 0.4828, tol 0, val_acc0.78, val_loss0.4740\n","Epoch 414, acc0.78, loss 0.4873, tol 0, val_acc0.78, val_loss0.4820\n","Epoch 415, acc0.77, loss 0.4993, tol 0, val_acc0.79, val_loss0.4780\n","Epoch 416, acc0.78, loss 0.4866, tol 0, val_acc0.79, val_loss0.4760\n","Epoch 417, acc0.79, loss 0.4848, tol 0, val_acc0.79, val_loss0.4724\n","Epoch 418, acc0.78, loss 0.4828, tol 0, val_acc0.79, val_loss0.4716\n","Epoch 419, acc0.75, loss 0.5165, tol 0, val_acc0.79, val_loss0.4712\n","Epoch 420, acc0.79, loss 0.4778, tol 0, val_acc0.80, val_loss0.4710 -- checkpoint saved\n","Epoch 421, acc0.79, loss 0.4906, tol 0, val_acc0.79, val_loss0.4713\n","Epoch 422, acc0.78, loss 0.4845, tol 0, val_acc0.78, val_loss0.4818\n","Epoch 423, acc0.77, loss 0.4942, tol 0, val_acc0.74, val_loss0.5154\n","Epoch 424, acc0.76, loss 0.5179, tol 0, val_acc0.75, val_loss0.4994\n","Epoch 425, acc0.75, loss 0.5074, tol 0, val_acc0.79, val_loss0.4762\n","Epoch 426, acc0.79, loss 0.4833, tol 0, val_acc0.79, val_loss0.4717\n","Epoch 427, acc0.79, loss 0.4784, tol 0, val_acc0.79, val_loss0.4666\n","Epoch 428, acc0.78, loss 0.4862, tol 0, val_acc0.79, val_loss0.4653\n","Epoch 429, acc0.79, loss 0.4817, tol 0, val_acc0.79, val_loss0.4707\n","Epoch 430, acc0.77, loss 0.4900, tol 0, val_acc0.80, val_loss0.4682\n","Epoch 431, acc0.79, loss 0.4780, tol 0, val_acc0.78, val_loss0.4776\n","Epoch 432, acc0.76, loss 0.4994, tol 0, val_acc0.79, val_loss0.4695\n","Epoch 433, acc0.79, loss 0.4750, tol 0, val_acc0.79, val_loss0.4718\n","Epoch 434, acc0.79, loss 0.4748, tol 0, val_acc0.79, val_loss0.4624\n","Epoch 435, acc0.79, loss 0.4704, tol 0, val_acc0.79, val_loss0.4614\n","Epoch 436, acc0.79, loss 0.4763, tol 0, val_acc0.81, val_loss0.4615\n","Epoch 437, acc0.79, loss 0.4865, tol 0, val_acc0.79, val_loss0.4607\n","Epoch 438, acc0.79, loss 0.4794, tol 0, val_acc0.79, val_loss0.4653\n","Epoch 439, acc0.78, loss 0.4811, tol 0, val_acc0.79, val_loss0.4618\n","Epoch 440, acc0.78, loss 0.4760, tol 0, val_acc0.79, val_loss0.4684\n","Epoch 441, acc0.79, loss 0.4680, tol 0, val_acc0.80, val_loss0.4586\n","Epoch 442, acc0.81, loss 0.4693, tol 0, val_acc0.79, val_loss0.4573\n","Epoch 443, acc0.79, loss 0.4873, tol 0, val_acc0.79, val_loss0.4607\n","Epoch 444, acc0.79, loss 0.4647, tol 0, val_acc0.79, val_loss0.4568\n","Epoch 445, acc0.78, loss 0.4866, tol 0, val_acc0.80, val_loss0.4572\n","Epoch 446, acc0.78, loss 0.4881, tol 0, val_acc0.80, val_loss0.4581\n","Epoch 447, acc0.78, loss 0.4745, tol 0, val_acc0.79, val_loss0.4549\n","Epoch 448, acc0.79, loss 0.4827, tol 0, val_acc0.81, val_loss0.4557\n","Epoch 449, acc0.79, loss 0.4614, tol 0, val_acc0.80, val_loss0.4562\n","Epoch 450, acc0.80, loss 0.4689, tol 0, val_acc0.80, val_loss0.4581\n","Epoch 451, acc0.80, loss 0.4623, tol 0, val_acc0.78, val_loss0.4701\n","Epoch 452, acc0.77, loss 0.4875, tol 0, val_acc0.78, val_loss0.4774\n","Epoch 453, acc0.79, loss 0.4702, tol 0, val_acc0.79, val_loss0.4672\n","Epoch 454, acc0.76, loss 0.5017, tol 0, val_acc0.79, val_loss0.4518\n","Epoch 455, acc0.79, loss 0.4718, tol 0, val_acc0.81, val_loss0.4531\n","Epoch 456, acc0.80, loss 0.4699, tol 0, val_acc0.79, val_loss0.4506\n","Epoch 457, acc0.80, loss 0.4563, tol 0, val_acc0.79, val_loss0.4500\n","Epoch 458, acc0.78, loss 0.4831, tol 0, val_acc0.81, val_loss0.4510\n","Epoch 459, acc0.79, loss 0.4730, tol 0, val_acc0.80, val_loss0.4499\n","Epoch 460, acc0.77, loss 0.4924, tol 0, val_acc0.80, val_loss0.4500\n","Epoch 461, acc0.80, loss 0.4693, tol 0, val_acc0.81, val_loss0.4514\n","Epoch 462, acc0.80, loss 0.4553, tol 0, val_acc0.81, val_loss0.4514\n","Epoch 463, acc0.80, loss 0.4551, tol 0, val_acc0.81, val_loss0.4487\n","Epoch 464, acc0.80, loss 0.4652, tol 0, val_acc0.80, val_loss0.4472\n","Epoch 465, acc0.79, loss 0.4650, tol 0, val_acc0.80, val_loss0.4468\n","Epoch 466, acc0.81, loss 0.4575, tol 0, val_acc0.81, val_loss0.4485\n","Epoch 467, acc0.79, loss 0.4640, tol 0, val_acc0.81, val_loss0.4493\n","Epoch 468, acc0.79, loss 0.4710, tol 0, val_acc0.80, val_loss0.4452\n","Epoch 469, acc0.80, loss 0.4738, tol 0, val_acc0.80, val_loss0.4477\n","Epoch 470, acc0.79, loss 0.4705, tol 0, val_acc0.80, val_loss0.4461\n","Epoch 471, acc0.81, loss 0.4470, tol 0, val_acc0.79, val_loss0.4441\n","Epoch 472, acc0.81, loss 0.4412, tol 0, val_acc0.81, val_loss0.4481\n","Epoch 473, acc0.79, loss 0.4586, tol 0, val_acc0.79, val_loss0.4575\n","Epoch 474, acc0.81, loss 0.4549, tol 0, val_acc0.80, val_loss0.4423\n","Epoch 475, acc0.77, loss 0.4898, tol 0, val_acc0.80, val_loss0.4446\n","Epoch 476, acc0.80, loss 0.4581, tol 0, val_acc0.81, val_loss0.4437\n","Epoch 477, acc0.80, loss 0.4626, tol 0, val_acc0.79, val_loss0.4499\n","Epoch 478, acc0.80, loss 0.4603, tol 0, val_acc0.80, val_loss0.4401\n","Epoch 479, acc0.81, loss 0.4520, tol 0, val_acc0.80, val_loss0.4396\n","Epoch 480, acc0.81, loss 0.4428, tol 0, val_acc0.81, val_loss0.4444\n","Epoch 481, acc0.80, loss 0.4618, tol 0, val_acc0.80, val_loss0.4397\n","Epoch 482, acc0.79, loss 0.4680, tol 0, val_acc0.80, val_loss0.4406\n","Epoch 483, acc0.82, loss 0.4419, tol 0, val_acc0.80, val_loss0.4431\n","Epoch 484, acc0.80, loss 0.4493, tol 0, val_acc0.79, val_loss0.4483\n","Epoch 485, acc0.78, loss 0.4698, tol 0, val_acc0.81, val_loss0.4447\n","Epoch 486, acc0.80, loss 0.4573, tol 0, val_acc0.80, val_loss0.4363\n","Epoch 487, acc0.81, loss 0.4522, tol 0, val_acc0.80, val_loss0.4357\n","Epoch 488, acc0.79, loss 0.4664, tol 0, val_acc0.79, val_loss0.4528\n","Epoch 489, acc0.80, loss 0.4538, tol 0, val_acc0.81, val_loss0.4393\n","Epoch 490, acc0.82, loss 0.4239, tol 0, val_acc0.80, val_loss0.4394\n","Epoch 491, acc0.79, loss 0.4716, tol 0, val_acc0.80, val_loss0.4398\n","Epoch 492, acc0.79, loss 0.4536, tol 0, val_acc0.80, val_loss0.4365\n","Epoch 493, acc0.80, loss 0.4473, tol 0, val_acc0.81, val_loss0.4337\n","Epoch 494, acc0.80, loss 0.4560, tol 0, val_acc0.81, val_loss0.4353\n","Epoch 495, acc0.80, loss 0.4454, tol 0, val_acc0.81, val_loss0.4367\n","Epoch 496, acc0.79, loss 0.4653, tol 0, val_acc0.81, val_loss0.4353\n","Epoch 497, acc0.80, loss 0.4489, tol 0, val_acc0.81, val_loss0.4339\n","Epoch 498, acc0.81, loss 0.4452, tol 0, val_acc0.81, val_loss0.4375\n","Epoch 499, acc0.78, loss 0.4637, tol 0, val_acc0.79, val_loss0.4517\n","Epoch 500, acc0.80, loss 0.4484, tol 0, val_acc0.79, val_loss0.4519\n","Epoch 501, acc0.82, loss 0.4414, tol 1, val_acc0.79, val_loss0.4423\n","Reach Max Epoch Number\n","+------------------------+------------+\n","|        Modules         | Parameters |\n","+------------------------+------------+\n","| layers.0.0.fc_Q.weight |     45     |\n","| layers.0.0.fc_K.weight |     45     |\n","| layers.0.0.fc_V.weight |     45     |\n","| layers.0.1.fc_Q.weight |     45     |\n","| layers.0.1.fc_K.weight |     45     |\n","| layers.0.1.fc_V.weight |     45     |\n","| layers.1.0.fc_Q.weight |    162     |\n","| layers.1.0.fc_K.weight |    162     |\n","| layers.1.0.fc_V.weight |    162     |\n","| layers.1.1.fc_Q.weight |    162     |\n","| layers.1.1.fc_K.weight |    162     |\n","| layers.1.1.fc_V.weight |    162     |\n","|    classify.weight     |     36     |\n","|     classify.bias      |     2      |\n","+------------------------+------------+\n","Total Trainable Params: 1280\n","Epoch 0, acc0.50, loss 0.7077, tol 0, val_acc0.50, val_loss0.7071 -- checkpoint saved\n","Epoch 1, acc0.49, loss 0.7003, tol 0, val_acc0.48, val_loss0.6959\n","Epoch 2, acc0.51, loss 0.6990, tol 0, val_acc0.50, val_loss0.6946 -- checkpoint saved\n","Epoch 3, acc0.50, loss 0.6957, tol 0, val_acc0.51, val_loss0.6936 -- checkpoint saved\n","Epoch 4, acc0.51, loss 0.6953, tol 0, val_acc0.51, val_loss0.6942\n","Epoch 5, acc0.51, loss 0.6950, tol 0, val_acc0.52, val_loss0.6926 -- checkpoint saved\n","Epoch 6, acc0.50, loss 0.6940, tol 0, val_acc0.52, val_loss0.6922\n","Epoch 7, acc0.51, loss 0.6936, tol 0, val_acc0.51, val_loss0.6925\n","Epoch 8, acc0.50, loss 0.6995, tol 0, val_acc0.54, val_loss0.6912 -- checkpoint saved\n","Epoch 9, acc0.51, loss 0.6952, tol 0, val_acc0.52, val_loss0.6912\n","Epoch 10, acc0.51, loss 0.6940, tol 0, val_acc0.54, val_loss0.6893 -- checkpoint saved\n","Epoch 11, acc0.53, loss 0.6918, tol 0, val_acc0.53, val_loss0.6907\n","Epoch 12, acc0.52, loss 0.6895, tol 0, val_acc0.55, val_loss0.6882 -- checkpoint saved\n","Epoch 13, acc0.54, loss 0.6878, tol 0, val_acc0.56, val_loss0.6877 -- checkpoint saved\n","Epoch 14, acc0.52, loss 0.6887, tol 0, val_acc0.54, val_loss0.6880\n","Epoch 15, acc0.54, loss 0.6883, tol 0, val_acc0.56, val_loss0.6868\n","Epoch 16, acc0.54, loss 0.6892, tol 0, val_acc0.52, val_loss0.6890\n","Epoch 17, acc0.54, loss 0.6875, tol 0, val_acc0.57, val_loss0.6852 -- checkpoint saved\n","Epoch 18, acc0.56, loss 0.6841, tol 0, val_acc0.58, val_loss0.6841 -- checkpoint saved\n","Epoch 19, acc0.55, loss 0.6887, tol 0, val_acc0.54, val_loss0.6856\n","Epoch 20, acc0.56, loss 0.6848, tol 0, val_acc0.60, val_loss0.6823 -- checkpoint saved\n","Epoch 21, acc0.58, loss 0.6826, tol 0, val_acc0.60, val_loss0.6813 -- checkpoint saved\n","Epoch 22, acc0.57, loss 0.6798, tol 0, val_acc0.60, val_loss0.6804\n","Epoch 23, acc0.56, loss 0.6821, tol 0, val_acc0.57, val_loss0.6805\n","Epoch 24, acc0.57, loss 0.6822, tol 0, val_acc0.56, val_loss0.6802\n","Epoch 25, acc0.55, loss 0.6836, tol 0, val_acc0.54, val_loss0.6811\n","Epoch 26, acc0.57, loss 0.6791, tol 0, val_acc0.60, val_loss0.6770\n","Epoch 27, acc0.60, loss 0.6762, tol 0, val_acc0.55, val_loss0.6797\n","Epoch 28, acc0.57, loss 0.6827, tol 0, val_acc0.53, val_loss0.6818\n","Epoch 29, acc0.56, loss 0.6789, tol 0, val_acc0.58, val_loss0.6761\n","Epoch 30, acc0.60, loss 0.6741, tol 0, val_acc0.60, val_loss0.6734\n","Epoch 31, acc0.58, loss 0.6750, tol 0, val_acc0.62, val_loss0.6727 -- checkpoint saved\n","Epoch 32, acc0.59, loss 0.6693, tol 0, val_acc0.60, val_loss0.6725\n","Epoch 33, acc0.59, loss 0.6727, tol 0, val_acc0.58, val_loss0.6734\n","Epoch 34, acc0.57, loss 0.6762, tol 0, val_acc0.60, val_loss0.6707\n","Epoch 35, acc0.59, loss 0.6710, tol 0, val_acc0.60, val_loss0.6691\n","Epoch 36, acc0.60, loss 0.6703, tol 0, val_acc0.58, val_loss0.6715\n","Epoch 37, acc0.58, loss 0.6723, tol 0, val_acc0.61, val_loss0.6676\n","Epoch 38, acc0.62, loss 0.6636, tol 0, val_acc0.61, val_loss0.6658\n","Epoch 39, acc0.60, loss 0.6719, tol 0, val_acc0.60, val_loss0.6693\n","Epoch 40, acc0.60, loss 0.6689, tol 0, val_acc0.60, val_loss0.6642\n","Epoch 41, acc0.61, loss 0.6599, tol 0, val_acc0.58, val_loss0.6686\n","Epoch 42, acc0.61, loss 0.6656, tol 0, val_acc0.62, val_loss0.6609 -- checkpoint saved\n","Epoch 43, acc0.60, loss 0.6669, tol 0, val_acc0.64, val_loss0.6579 -- checkpoint saved\n","Epoch 44, acc0.63, loss 0.6565, tol 0, val_acc0.64, val_loss0.6564 -- checkpoint saved\n","Epoch 45, acc0.63, loss 0.6564, tol 0, val_acc0.62, val_loss0.6605\n","Epoch 46, acc0.61, loss 0.6627, tol 0, val_acc0.64, val_loss0.6536 -- checkpoint saved\n","Epoch 47, acc0.60, loss 0.6627, tol 0, val_acc0.61, val_loss0.6574\n","Epoch 48, acc0.64, loss 0.6506, tol 0, val_acc0.61, val_loss0.6554\n","Epoch 49, acc0.61, loss 0.6513, tol 0, val_acc0.63, val_loss0.6500\n","Epoch 50, acc0.64, loss 0.6503, tol 0, val_acc0.64, val_loss0.6534\n","Epoch 51, acc0.62, loss 0.6525, tol 0, val_acc0.65, val_loss0.6463 -- checkpoint saved\n","Epoch 52, acc0.63, loss 0.6542, tol 0, val_acc0.64, val_loss0.6481\n","Epoch 53, acc0.63, loss 0.6450, tol 0, val_acc0.66, val_loss0.6433 -- checkpoint saved\n","Epoch 54, acc0.62, loss 0.6442, tol 0, val_acc0.62, val_loss0.6503\n","Epoch 55, acc0.63, loss 0.6491, tol 0, val_acc0.66, val_loss0.6428\n","Epoch 56, acc0.64, loss 0.6529, tol 0, val_acc0.63, val_loss0.6462\n","Epoch 57, acc0.63, loss 0.6421, tol 0, val_acc0.62, val_loss0.6482\n","Epoch 58, acc0.63, loss 0.6469, tol 0, val_acc0.66, val_loss0.6351\n","Epoch 59, acc0.65, loss 0.6386, tol 0, val_acc0.68, val_loss0.6330 -- checkpoint saved\n","Epoch 60, acc0.66, loss 0.6348, tol 0, val_acc0.68, val_loss0.6310 -- checkpoint saved\n","Epoch 61, acc0.65, loss 0.6388, tol 0, val_acc0.69, val_loss0.6303 -- checkpoint saved\n","Epoch 62, acc0.66, loss 0.6349, tol 0, val_acc0.63, val_loss0.6351\n","Epoch 63, acc0.64, loss 0.6401, tol 0, val_acc0.68, val_loss0.6246\n","Epoch 64, acc0.66, loss 0.6280, tol 0, val_acc0.68, val_loss0.6258\n","Epoch 65, acc0.65, loss 0.6328, tol 0, val_acc0.70, val_loss0.6232 -- checkpoint saved\n","Epoch 66, acc0.67, loss 0.6311, tol 0, val_acc0.67, val_loss0.6200\n","Epoch 67, acc0.64, loss 0.6314, tol 0, val_acc0.67, val_loss0.6190\n","Epoch 68, acc0.67, loss 0.6198, tol 0, val_acc0.71, val_loss0.6198\n","Epoch 69, acc0.68, loss 0.6189, tol 0, val_acc0.68, val_loss0.6138\n","Epoch 70, acc0.66, loss 0.6234, tol 0, val_acc0.71, val_loss0.6114 -- checkpoint saved\n","Epoch 71, acc0.65, loss 0.6236, tol 0, val_acc0.71, val_loss0.6094\n","Epoch 72, acc0.67, loss 0.6160, tol 0, val_acc0.70, val_loss0.6081\n","Epoch 73, acc0.66, loss 0.6205, tol 0, val_acc0.72, val_loss0.6065 -- checkpoint saved\n","Epoch 74, acc0.66, loss 0.6105, tol 0, val_acc0.70, val_loss0.6042\n","Epoch 75, acc0.67, loss 0.6173, tol 0, val_acc0.71, val_loss0.6048\n","Epoch 76, acc0.68, loss 0.6081, tol 0, val_acc0.70, val_loss0.6039\n","Epoch 77, acc0.69, loss 0.6009, tol 0, val_acc0.74, val_loss0.5958 -- checkpoint saved\n","Epoch 78, acc0.67, loss 0.6081, tol 0, val_acc0.72, val_loss0.5939\n","Epoch 79, acc0.70, loss 0.5986, tol 0, val_acc0.73, val_loss0.5918\n","Epoch 80, acc0.69, loss 0.5938, tol 0, val_acc0.73, val_loss0.5899\n","Epoch 81, acc0.70, loss 0.5845, tol 0, val_acc0.73, val_loss0.5876\n","Epoch 82, acc0.71, loss 0.5849, tol 0, val_acc0.73, val_loss0.5838\n","Epoch 83, acc0.69, loss 0.5990, tol 0, val_acc0.74, val_loss0.5816\n","Epoch 84, acc0.67, loss 0.5994, tol 0, val_acc0.74, val_loss0.5796 -- checkpoint saved\n","Epoch 85, acc0.70, loss 0.5888, tol 0, val_acc0.74, val_loss0.5765 -- checkpoint saved\n","Epoch 86, acc0.73, loss 0.5720, tol 0, val_acc0.75, val_loss0.5757 -- checkpoint saved\n","Epoch 87, acc0.72, loss 0.5783, tol 0, val_acc0.76, val_loss0.5715 -- checkpoint saved\n","Epoch 88, acc0.70, loss 0.5801, tol 0, val_acc0.75, val_loss0.5677\n","Epoch 89, acc0.71, loss 0.5662, tol 0, val_acc0.73, val_loss0.5717\n","Epoch 90, acc0.70, loss 0.5809, tol 0, val_acc0.76, val_loss0.5645 -- checkpoint saved\n","Epoch 91, acc0.73, loss 0.5689, tol 0, val_acc0.72, val_loss0.5701\n","Epoch 92, acc0.72, loss 0.5694, tol 0, val_acc0.68, val_loss0.6002\n","Epoch 93, acc0.70, loss 0.5831, tol 0, val_acc0.74, val_loss0.5687\n","Epoch 94, acc0.71, loss 0.5779, tol 0, val_acc0.77, val_loss0.5560 -- checkpoint saved\n","Epoch 95, acc0.73, loss 0.5593, tol 0, val_acc0.75, val_loss0.5598\n","Epoch 96, acc0.73, loss 0.5593, tol 0, val_acc0.77, val_loss0.5519 -- checkpoint saved\n","Epoch 97, acc0.74, loss 0.5579, tol 0, val_acc0.78, val_loss0.5489 -- checkpoint saved\n","Epoch 98, acc0.73, loss 0.5613, tol 0, val_acc0.76, val_loss0.5462\n","Epoch 99, acc0.73, loss 0.5615, tol 0, val_acc0.77, val_loss0.5451\n","Epoch 100, acc0.74, loss 0.5376, tol 0, val_acc0.71, val_loss0.5627\n","Epoch 101, acc0.73, loss 0.5580, tol 0, val_acc0.74, val_loss0.5508\n","Epoch 102, acc0.73, loss 0.5544, tol 0, val_acc0.76, val_loss0.5383\n","Epoch 103, acc0.74, loss 0.5484, tol 0, val_acc0.79, val_loss0.5350 -- checkpoint saved\n","Epoch 104, acc0.73, loss 0.5430, tol 0, val_acc0.77, val_loss0.5398\n","Epoch 105, acc0.75, loss 0.5439, tol 0, val_acc0.78, val_loss0.5314\n","Epoch 106, acc0.74, loss 0.5366, tol 0, val_acc0.76, val_loss0.5331\n","Epoch 107, acc0.72, loss 0.5423, tol 0, val_acc0.78, val_loss0.5272\n","Epoch 108, acc0.75, loss 0.5356, tol 0, val_acc0.79, val_loss0.5274\n","Epoch 109, acc0.76, loss 0.5219, tol 0, val_acc0.79, val_loss0.5238 -- checkpoint saved\n","Epoch 110, acc0.75, loss 0.5236, tol 0, val_acc0.78, val_loss0.5291\n","Epoch 111, acc0.76, loss 0.5184, tol 0, val_acc0.76, val_loss0.5227\n","Epoch 112, acc0.75, loss 0.5308, tol 0, val_acc0.79, val_loss0.5174 -- checkpoint saved\n","Epoch 113, acc0.77, loss 0.5156, tol 0, val_acc0.79, val_loss0.5164\n","Epoch 114, acc0.76, loss 0.5233, tol 0, val_acc0.79, val_loss0.5138 -- checkpoint saved\n","Epoch 115, acc0.74, loss 0.5269, tol 0, val_acc0.78, val_loss0.5136\n","Epoch 116, acc0.76, loss 0.5133, tol 0, val_acc0.78, val_loss0.5194\n","Epoch 117, acc0.75, loss 0.5267, tol 0, val_acc0.76, val_loss0.5366\n","Epoch 118, acc0.76, loss 0.5202, tol 0, val_acc0.78, val_loss0.5231\n","Epoch 119, acc0.78, loss 0.5131, tol 0, val_acc0.80, val_loss0.5023 -- checkpoint saved\n","Epoch 120, acc0.76, loss 0.5154, tol 0, val_acc0.78, val_loss0.5071\n","Epoch 121, acc0.76, loss 0.5132, tol 0, val_acc0.80, val_loss0.5007\n","Epoch 122, acc0.78, loss 0.4953, tol 0, val_acc0.77, val_loss0.5099\n","Epoch 123, acc0.76, loss 0.5218, tol 0, val_acc0.77, val_loss0.5040\n","Epoch 124, acc0.76, loss 0.5051, tol 0, val_acc0.81, val_loss0.4905 -- checkpoint saved\n","Epoch 125, acc0.77, loss 0.4945, tol 0, val_acc0.80, val_loss0.4901\n","Epoch 126, acc0.79, loss 0.4858, tol 0, val_acc0.79, val_loss0.4898\n","Epoch 127, acc0.78, loss 0.4843, tol 0, val_acc0.79, val_loss0.4848\n","Epoch 128, acc0.78, loss 0.4925, tol 0, val_acc0.78, val_loss0.4924\n","Epoch 129, acc0.78, loss 0.4959, tol 0, val_acc0.80, val_loss0.4818\n","Epoch 130, acc0.79, loss 0.4821, tol 0, val_acc0.79, val_loss0.4803\n","Epoch 131, acc0.79, loss 0.4860, tol 0, val_acc0.80, val_loss0.4781\n","Epoch 132, acc0.79, loss 0.4809, tol 0, val_acc0.80, val_loss0.4763\n","Epoch 133, acc0.77, loss 0.4874, tol 0, val_acc0.79, val_loss0.4777\n","Epoch 134, acc0.79, loss 0.4661, tol 0, val_acc0.80, val_loss0.4672\n","Epoch 135, acc0.79, loss 0.4684, tol 0, val_acc0.77, val_loss0.5026\n","Epoch 136, acc0.77, loss 0.4895, tol 0, val_acc0.81, val_loss0.4741\n","Epoch 137, acc0.81, loss 0.4670, tol 0, val_acc0.82, val_loss0.4638 -- checkpoint saved\n","Epoch 138, acc0.79, loss 0.4662, tol 0, val_acc0.80, val_loss0.4613\n","Epoch 139, acc0.79, loss 0.4648, tol 0, val_acc0.80, val_loss0.4601\n","Epoch 140, acc0.80, loss 0.4556, tol 0, val_acc0.79, val_loss0.4663\n","Epoch 141, acc0.79, loss 0.4611, tol 0, val_acc0.81, val_loss0.4568\n","Epoch 142, acc0.80, loss 0.4560, tol 0, val_acc0.80, val_loss0.4684\n","Epoch 143, acc0.79, loss 0.4713, tol 0, val_acc0.81, val_loss0.4525\n","Epoch 144, acc0.80, loss 0.4607, tol 0, val_acc0.81, val_loss0.4495\n","Epoch 145, acc0.79, loss 0.4651, tol 0, val_acc0.82, val_loss0.4475 -- checkpoint saved\n","Epoch 146, acc0.82, loss 0.4425, tol 0, val_acc0.81, val_loss0.4478\n","Epoch 147, acc0.80, loss 0.4524, tol 0, val_acc0.82, val_loss0.4488\n","Epoch 148, acc0.82, loss 0.4327, tol 0, val_acc0.80, val_loss0.4445\n","Epoch 149, acc0.79, loss 0.4423, tol 0, val_acc0.81, val_loss0.4508\n","Epoch 150, acc0.80, loss 0.4597, tol 0, val_acc0.82, val_loss0.4426 -- checkpoint saved\n","Epoch 151, acc0.82, loss 0.4403, tol 0, val_acc0.81, val_loss0.4480\n","Epoch 152, acc0.81, loss 0.4397, tol 0, val_acc0.79, val_loss0.4672\n","Epoch 153, acc0.81, loss 0.4325, tol 0, val_acc0.82, val_loss0.4378\n","Epoch 154, acc0.80, loss 0.4533, tol 0, val_acc0.81, val_loss0.4419\n","Epoch 155, acc0.83, loss 0.4198, tol 0, val_acc0.82, val_loss0.4406\n","Epoch 156, acc0.81, loss 0.4455, tol 0, val_acc0.82, val_loss0.4368 -- checkpoint saved\n","Epoch 157, acc0.82, loss 0.4190, tol 0, val_acc0.81, val_loss0.4363\n","Epoch 158, acc0.82, loss 0.4200, tol 0, val_acc0.81, val_loss0.4352\n","Epoch 159, acc0.80, loss 0.4501, tol 0, val_acc0.81, val_loss0.4380\n","Epoch 160, acc0.82, loss 0.4244, tol 0, val_acc0.81, val_loss0.4406\n","Epoch 161, acc0.81, loss 0.4349, tol 0, val_acc0.82, val_loss0.4344\n","Epoch 162, acc0.81, loss 0.4217, tol 0, val_acc0.82, val_loss0.4288\n","Epoch 163, acc0.81, loss 0.4101, tol 0, val_acc0.81, val_loss0.4311\n","Epoch 164, acc0.83, loss 0.4157, tol 0, val_acc0.80, val_loss0.4318\n","Epoch 165, acc0.83, loss 0.4231, tol 0, val_acc0.80, val_loss0.4365\n","Epoch 166, acc0.83, loss 0.4153, tol 0, val_acc0.81, val_loss0.4234\n","Epoch 167, acc0.83, loss 0.4188, tol 0, val_acc0.82, val_loss0.4225\n","Epoch 168, acc0.83, loss 0.4107, tol 0, val_acc0.82, val_loss0.4209\n","Epoch 169, acc0.84, loss 0.3928, tol 0, val_acc0.81, val_loss0.4262\n","Epoch 170, acc0.83, loss 0.4194, tol 0, val_acc0.81, val_loss0.4172\n","Epoch 171, acc0.83, loss 0.4096, tol 0, val_acc0.82, val_loss0.4142 -- checkpoint saved\n","Epoch 172, acc0.82, loss 0.4189, tol 0, val_acc0.82, val_loss0.4140\n","Epoch 173, acc0.82, loss 0.4088, tol 0, val_acc0.82, val_loss0.4124\n","Epoch 174, acc0.82, loss 0.4079, tol 0, val_acc0.83, val_loss0.4172\n","Epoch 175, acc0.83, loss 0.4147, tol 0, val_acc0.82, val_loss0.4183\n","Epoch 176, acc0.84, loss 0.3904, tol 0, val_acc0.82, val_loss0.4210\n","Epoch 177, acc0.82, loss 0.4068, tol 0, val_acc0.83, val_loss0.4180\n","Epoch 178, acc0.83, loss 0.3969, tol 0, val_acc0.82, val_loss0.4174\n","Epoch 179, acc0.84, loss 0.4009, tol 0, val_acc0.82, val_loss0.4060\n","Epoch 180, acc0.84, loss 0.3610, tol 0, val_acc0.83, val_loss0.4062\n","Epoch 181, acc0.83, loss 0.3852, tol 0, val_acc0.82, val_loss0.4063\n","Epoch 182, acc0.84, loss 0.3830, tol 0, val_acc0.82, val_loss0.4222\n","Epoch 183, acc0.83, loss 0.3908, tol 0, val_acc0.82, val_loss0.4093\n","Epoch 184, acc0.84, loss 0.3780, tol 0, val_acc0.82, val_loss0.4092\n","Epoch 185, acc0.84, loss 0.3908, tol 0, val_acc0.83, val_loss0.4007\n","Epoch 186, acc0.84, loss 0.3770, tol 0, val_acc0.83, val_loss0.3983\n","Epoch 187, acc0.85, loss 0.3692, tol 0, val_acc0.83, val_loss0.3997\n","Epoch 188, acc0.84, loss 0.3953, tol 0, val_acc0.81, val_loss0.4022\n","Epoch 189, acc0.83, loss 0.3804, tol 0, val_acc0.83, val_loss0.3979\n","Epoch 190, acc0.84, loss 0.3918, tol 0, val_acc0.82, val_loss0.4030\n","Epoch 191, acc0.85, loss 0.3798, tol 0, val_acc0.82, val_loss0.3933\n","Epoch 192, acc0.84, loss 0.3758, tol 0, val_acc0.83, val_loss0.4001\n","Epoch 193, acc0.84, loss 0.3762, tol 0, val_acc0.82, val_loss0.3915\n","Epoch 194, acc0.86, loss 0.3589, tol 0, val_acc0.83, val_loss0.3949\n","Epoch 195, acc0.84, loss 0.3889, tol 0, val_acc0.83, val_loss0.3897\n","Epoch 196, acc0.85, loss 0.3619, tol 0, val_acc0.83, val_loss0.3902\n","Epoch 197, acc0.85, loss 0.3692, tol 0, val_acc0.83, val_loss0.3904\n","Epoch 198, acc0.84, loss 0.3754, tol 0, val_acc0.81, val_loss0.4110\n","Epoch 199, acc0.83, loss 0.4077, tol 0, val_acc0.83, val_loss0.3872\n","Epoch 200, acc0.85, loss 0.3756, tol 0, val_acc0.82, val_loss0.3851\n","Epoch 201, acc0.86, loss 0.3711, tol 0, val_acc0.83, val_loss0.3884\n","Epoch 202, acc0.85, loss 0.3727, tol 0, val_acc0.83, val_loss0.3841\n","Epoch 203, acc0.84, loss 0.3801, tol 0, val_acc0.83, val_loss0.3825 -- checkpoint saved\n","Epoch 204, acc0.85, loss 0.3644, tol 0, val_acc0.83, val_loss0.3830\n","Epoch 205, acc0.85, loss 0.3590, tol 0, val_acc0.83, val_loss0.3861\n","Epoch 206, acc0.85, loss 0.3657, tol 0, val_acc0.83, val_loss0.3843\n","Epoch 207, acc0.86, loss 0.3704, tol 0, val_acc0.83, val_loss0.3832\n","Epoch 208, acc0.84, loss 0.3653, tol 0, val_acc0.83, val_loss0.3854\n","Epoch 209, acc0.85, loss 0.3572, tol 0, val_acc0.83, val_loss0.3834\n","Epoch 210, acc0.84, loss 0.3794, tol 0, val_acc0.83, val_loss0.3809\n","Epoch 211, acc0.87, loss 0.3471, tol 0, val_acc0.82, val_loss0.3809\n","Epoch 212, acc0.84, loss 0.3758, tol 0, val_acc0.84, val_loss0.3816\n","Epoch 213, acc0.85, loss 0.3638, tol 0, val_acc0.84, val_loss0.3787 -- checkpoint saved\n","Epoch 214, acc0.83, loss 0.3983, tol 0, val_acc0.84, val_loss0.3882\n","Epoch 215, acc0.83, loss 0.3965, tol 0, val_acc0.83, val_loss0.3938\n","Epoch 216, acc0.85, loss 0.3647, tol 0, val_acc0.82, val_loss0.3948\n","Epoch 217, acc0.85, loss 0.3650, tol 0, val_acc0.84, val_loss0.3801\n","Epoch 218, acc0.85, loss 0.3517, tol 0, val_acc0.84, val_loss0.3803\n","Epoch 219, acc0.84, loss 0.3574, tol 0, val_acc0.83, val_loss0.3759\n","Epoch 220, acc0.86, loss 0.3511, tol 0, val_acc0.84, val_loss0.3750\n","Epoch 221, acc0.85, loss 0.3574, tol 0, val_acc0.83, val_loss0.3761\n","Epoch 222, acc0.85, loss 0.3697, tol 0, val_acc0.84, val_loss0.3754\n","Epoch 223, acc0.84, loss 0.3556, tol 0, val_acc0.83, val_loss0.3792\n","Epoch 224, acc0.87, loss 0.3414, tol 0, val_acc0.82, val_loss0.3829\n","Epoch 225, acc0.85, loss 0.3597, tol 0, val_acc0.83, val_loss0.3810\n","Epoch 226, acc0.84, loss 0.3745, tol 0, val_acc0.83, val_loss0.3856\n","Epoch 227, acc0.85, loss 0.3579, tol 0, val_acc0.83, val_loss0.3752\n","Epoch 228, acc0.85, loss 0.3542, tol 0, val_acc0.83, val_loss0.3747\n","Epoch 229, acc0.86, loss 0.3520, tol 0, val_acc0.82, val_loss0.3786\n","Epoch 230, acc0.84, loss 0.3630, tol 0, val_acc0.84, val_loss0.3736\n","Epoch 231, acc0.85, loss 0.3658, tol 0, val_acc0.82, val_loss0.3809\n","Epoch 232, acc0.84, loss 0.3763, tol 0, val_acc0.83, val_loss0.3796\n","Epoch 233, acc0.83, loss 0.3739, tol 0, val_acc0.84, val_loss0.3691\n","Epoch 234, acc0.85, loss 0.3497, tol 0, val_acc0.84, val_loss0.3705\n","Epoch 235, acc0.85, loss 0.3691, tol 0, val_acc0.85, val_loss0.3706\n","Epoch 236, acc0.84, loss 0.3500, tol 0, val_acc0.83, val_loss0.3800\n","Epoch 237, acc0.87, loss 0.3174, tol 0, val_acc0.84, val_loss0.3731\n","Epoch 238, acc0.85, loss 0.3589, tol 0, val_acc0.84, val_loss0.3659\n","Epoch 239, acc0.85, loss 0.3472, tol 0, val_acc0.84, val_loss0.3730\n","Epoch 240, acc0.85, loss 0.3519, tol 0, val_acc0.83, val_loss0.3827\n","Epoch 241, acc0.85, loss 0.3619, tol 0, val_acc0.85, val_loss0.3596 -- checkpoint saved\n","Epoch 242, acc0.85, loss 0.3424, tol 0, val_acc0.84, val_loss0.3610\n","Epoch 243, acc0.87, loss 0.3252, tol 0, val_acc0.84, val_loss0.3646\n","Epoch 244, acc0.86, loss 0.3371, tol 0, val_acc0.83, val_loss0.3705\n","Epoch 245, acc0.88, loss 0.3264, tol 0, val_acc0.84, val_loss0.3644\n","Epoch 246, acc0.87, loss 0.3371, tol 0, val_acc0.84, val_loss0.3707\n","Epoch 247, acc0.86, loss 0.3423, tol 0, val_acc0.84, val_loss0.3662\n","Epoch 248, acc0.86, loss 0.3313, tol 0, val_acc0.83, val_loss0.3756\n","Epoch 249, acc0.86, loss 0.3454, tol 0, val_acc0.83, val_loss0.3601\n","Epoch 250, acc0.85, loss 0.3457, tol 0, val_acc0.84, val_loss0.3572\n","Epoch 251, acc0.87, loss 0.3162, tol 0, val_acc0.84, val_loss0.3655\n","Epoch 252, acc0.87, loss 0.3229, tol 0, val_acc0.84, val_loss0.3730\n","Epoch 253, acc0.85, loss 0.3435, tol 0, val_acc0.84, val_loss0.3538\n","Epoch 254, acc0.87, loss 0.3270, tol 0, val_acc0.84, val_loss0.3599\n","Epoch 255, acc0.86, loss 0.3380, tol 0, val_acc0.84, val_loss0.3651\n","Epoch 256, acc0.86, loss 0.3514, tol 0, val_acc0.84, val_loss0.3686\n","Epoch 257, acc0.87, loss 0.3265, tol 0, val_acc0.84, val_loss0.3592\n","Epoch 258, acc0.87, loss 0.3199, tol 0, val_acc0.84, val_loss0.3615\n","Epoch 259, acc0.87, loss 0.3168, tol 0, val_acc0.83, val_loss0.3756\n","Epoch 260, acc0.86, loss 0.3284, tol 0, val_acc0.84, val_loss0.3569\n","Epoch 261, acc0.85, loss 0.3410, tol 0, val_acc0.84, val_loss0.3504\n","Epoch 262, acc0.87, loss 0.3341, tol 0, val_acc0.84, val_loss0.3545\n","Epoch 263, acc0.86, loss 0.3350, tol 0, val_acc0.84, val_loss0.3531\n","Epoch 264, acc0.87, loss 0.3247, tol 0, val_acc0.84, val_loss0.3605\n","Epoch 265, acc0.86, loss 0.3281, tol 0, val_acc0.83, val_loss0.3598\n","Epoch 266, acc0.85, loss 0.3418, tol 0, val_acc0.83, val_loss0.3565\n","Epoch 267, acc0.88, loss 0.3026, tol 0, val_acc0.84, val_loss0.3587\n","Epoch 268, acc0.86, loss 0.3312, tol 0, val_acc0.84, val_loss0.3583\n","Epoch 269, acc0.86, loss 0.3305, tol 0, val_acc0.84, val_loss0.3547\n","Epoch 270, acc0.86, loss 0.3469, tol 0, val_acc0.84, val_loss0.3488\n","Epoch 271, acc0.86, loss 0.3345, tol 0, val_acc0.83, val_loss0.3570\n","Epoch 272, acc0.87, loss 0.3215, tol 0, val_acc0.83, val_loss0.3690\n","Epoch 273, acc0.86, loss 0.3283, tol 0, val_acc0.84, val_loss0.3598\n","Epoch 274, acc0.88, loss 0.2947, tol 0, val_acc0.84, val_loss0.3582\n","Epoch 275, acc0.86, loss 0.3282, tol 0, val_acc0.84, val_loss0.3595\n","Epoch 276, acc0.86, loss 0.3349, tol 0, val_acc0.84, val_loss0.3529\n","Epoch 277, acc0.86, loss 0.3285, tol 0, val_acc0.84, val_loss0.3560\n","Epoch 278, acc0.86, loss 0.3225, tol 0, val_acc0.83, val_loss0.3561\n","Epoch 279, acc0.86, loss 0.3315, tol 0, val_acc0.83, val_loss0.3640\n","Epoch 280, acc0.87, loss 0.3173, tol 0, val_acc0.84, val_loss0.3512\n","Epoch 281, acc0.87, loss 0.3236, tol 0, val_acc0.84, val_loss0.3535\n","Epoch 282, acc0.87, loss 0.3278, tol 0, val_acc0.84, val_loss0.3474\n","Epoch 283, acc0.86, loss 0.3264, tol 0, val_acc0.85, val_loss0.3452 -- checkpoint saved\n","Epoch 284, acc0.86, loss 0.3261, tol 0, val_acc0.84, val_loss0.3489\n","Epoch 285, acc0.87, loss 0.3144, tol 0, val_acc0.84, val_loss0.3559\n","Epoch 286, acc0.86, loss 0.3196, tol 0, val_acc0.84, val_loss0.3519\n","Epoch 287, acc0.89, loss 0.2881, tol 0, val_acc0.84, val_loss0.3507\n","Epoch 288, acc0.85, loss 0.3193, tol 0, val_acc0.85, val_loss0.3414 -- checkpoint saved\n","Epoch 289, acc0.86, loss 0.3288, tol 0, val_acc0.84, val_loss0.3424\n","Epoch 290, acc0.87, loss 0.3059, tol 0, val_acc0.85, val_loss0.3412 -- checkpoint saved\n","Epoch 291, acc0.87, loss 0.3192, tol 0, val_acc0.84, val_loss0.3407\n","Epoch 292, acc0.87, loss 0.2966, tol 0, val_acc0.84, val_loss0.3653\n","Epoch 293, acc0.87, loss 0.3111, tol 0, val_acc0.84, val_loss0.3525\n","Epoch 294, acc0.88, loss 0.3056, tol 0, val_acc0.85, val_loss0.3457\n","Epoch 295, acc0.88, loss 0.2965, tol 0, val_acc0.84, val_loss0.3488\n","Epoch 296, acc0.86, loss 0.3238, tol 0, val_acc0.84, val_loss0.3322\n","Epoch 297, acc0.89, loss 0.3041, tol 0, val_acc0.84, val_loss0.3333\n","Epoch 298, acc0.87, loss 0.3124, tol 0, val_acc0.84, val_loss0.3383\n","Epoch 299, acc0.88, loss 0.2874, tol 0, val_acc0.84, val_loss0.3467\n","Epoch 300, acc0.87, loss 0.3115, tol 0, val_acc0.84, val_loss0.3366\n","Epoch 301, acc0.87, loss 0.3098, tol 0, val_acc0.84, val_loss0.3411\n","Epoch 302, acc0.88, loss 0.2971, tol 0, val_acc0.85, val_loss0.3432\n","Epoch 303, acc0.88, loss 0.2840, tol 0, val_acc0.85, val_loss0.3416\n","Epoch 304, acc0.87, loss 0.3047, tol 0, val_acc0.84, val_loss0.3370\n","Epoch 305, acc0.89, loss 0.2775, tol 0, val_acc0.85, val_loss0.3418\n","Epoch 306, acc0.87, loss 0.3077, tol 0, val_acc0.85, val_loss0.3432\n","Epoch 307, acc0.88, loss 0.2940, tol 0, val_acc0.85, val_loss0.3390\n","Epoch 308, acc0.87, loss 0.3104, tol 0, val_acc0.85, val_loss0.3550\n","Epoch 309, acc0.87, loss 0.3079, tol 0, val_acc0.85, val_loss0.3335\n","Epoch 310, acc0.89, loss 0.3004, tol 0, val_acc0.85, val_loss0.3296\n","Epoch 311, acc0.88, loss 0.2839, tol 0, val_acc0.85, val_loss0.3396\n","Epoch 312, acc0.87, loss 0.3106, tol 0, val_acc0.85, val_loss0.3411\n","Epoch 313, acc0.88, loss 0.2832, tol 0, val_acc0.84, val_loss0.3500\n","Epoch 314, acc0.87, loss 0.3160, tol 0, val_acc0.85, val_loss0.3292\n","Epoch 315, acc0.87, loss 0.2907, tol 0, val_acc0.86, val_loss0.3258 -- checkpoint saved\n","Epoch 316, acc0.89, loss 0.2820, tol 0, val_acc0.85, val_loss0.3264\n","Epoch 317, acc0.87, loss 0.3015, tol 0, val_acc0.86, val_loss0.3252 -- checkpoint saved\n","Epoch 318, acc0.87, loss 0.2991, tol 0, val_acc0.86, val_loss0.3277\n","Epoch 319, acc0.88, loss 0.2987, tol 0, val_acc0.86, val_loss0.3279\n","Epoch 320, acc0.86, loss 0.3189, tol 0, val_acc0.85, val_loss0.3268\n","Epoch 321, acc0.89, loss 0.2737, tol 0, val_acc0.85, val_loss0.3269\n","Epoch 322, acc0.90, loss 0.2643, tol 0, val_acc0.86, val_loss0.3256\n","Epoch 323, acc0.88, loss 0.3002, tol 0, val_acc0.86, val_loss0.3268\n","Epoch 324, acc0.88, loss 0.2817, tol 0, val_acc0.86, val_loss0.3256\n","Epoch 325, acc0.87, loss 0.2970, tol 0, val_acc0.86, val_loss0.3299\n","Epoch 326, acc0.88, loss 0.2762, tol 0, val_acc0.85, val_loss0.3216\n","Epoch 327, acc0.88, loss 0.2908, tol 0, val_acc0.86, val_loss0.3271\n","Epoch 328, acc0.87, loss 0.2888, tol 0, val_acc0.86, val_loss0.3277\n","Epoch 329, acc0.89, loss 0.2680, tol 0, val_acc0.85, val_loss0.3235\n","Epoch 330, acc0.88, loss 0.2863, tol 0, val_acc0.86, val_loss0.3222\n","Epoch 331, acc0.88, loss 0.2827, tol 0, val_acc0.85, val_loss0.3315\n","Epoch 332, acc0.88, loss 0.2850, tol 0, val_acc0.86, val_loss0.3243\n","Epoch 333, acc0.88, loss 0.2737, tol 0, val_acc0.86, val_loss0.3216\n","Epoch 334, acc0.87, loss 0.2937, tol 0, val_acc0.86, val_loss0.3263\n","Epoch 335, acc0.90, loss 0.2603, tol 0, val_acc0.86, val_loss0.3454\n","Epoch 336, acc0.88, loss 0.2907, tol 0, val_acc0.85, val_loss0.3231\n","Epoch 337, acc0.89, loss 0.2830, tol 0, val_acc0.86, val_loss0.3232\n","Epoch 338, acc0.88, loss 0.2895, tol 0, val_acc0.86, val_loss0.3245\n","Epoch 339, acc0.88, loss 0.2840, tol 0, val_acc0.87, val_loss0.3226\n","Epoch 340, acc0.88, loss 0.2896, tol 0, val_acc0.86, val_loss0.3191\n","Epoch 341, acc0.89, loss 0.2606, tol 0, val_acc0.88, val_loss0.3141 -- checkpoint saved\n","Epoch 342, acc0.88, loss 0.2850, tol 0, val_acc0.85, val_loss0.3268\n","Epoch 343, acc0.87, loss 0.2951, tol 0, val_acc0.86, val_loss0.3197\n","Epoch 344, acc0.89, loss 0.2857, tol 0, val_acc0.86, val_loss0.3182\n","Epoch 345, acc0.89, loss 0.2759, tol 0, val_acc0.86, val_loss0.3254\n","Epoch 346, acc0.88, loss 0.2872, tol 0, val_acc0.86, val_loss0.3232\n","Epoch 347, acc0.89, loss 0.2625, tol 0, val_acc0.87, val_loss0.3237\n","Epoch 348, acc0.88, loss 0.2907, tol 0, val_acc0.87, val_loss0.3284\n","Epoch 349, acc0.88, loss 0.2908, tol 0, val_acc0.87, val_loss0.3338\n","Epoch 350, acc0.88, loss 0.2811, tol 0, val_acc0.86, val_loss0.3410\n","Epoch 351, acc0.89, loss 0.2841, tol 0, val_acc0.86, val_loss0.3296\n","Epoch 352, acc0.89, loss 0.2735, tol 0, val_acc0.86, val_loss0.3325\n","Epoch 353, acc0.90, loss 0.2606, tol 0, val_acc0.85, val_loss0.3389\n","Epoch 354, acc0.87, loss 0.2961, tol 0, val_acc0.85, val_loss0.3734\n","Epoch 355, acc0.88, loss 0.2812, tol 0, val_acc0.85, val_loss0.3385\n","Epoch 356, acc0.89, loss 0.2752, tol 0, val_acc0.86, val_loss0.3352\n","Epoch 357, acc0.88, loss 0.2961, tol 0, val_acc0.86, val_loss0.3260\n","Epoch 358, acc0.87, loss 0.2856, tol 0, val_acc0.87, val_loss0.3174\n","Epoch 359, acc0.88, loss 0.2859, tol 0, val_acc0.86, val_loss0.3239\n","Epoch 360, acc0.89, loss 0.2818, tol 0, val_acc0.86, val_loss0.3098\n","Epoch 361, acc0.88, loss 0.2941, tol 0, val_acc0.87, val_loss0.3112\n","Epoch 362, acc0.89, loss 0.2684, tol 0, val_acc0.86, val_loss0.3093\n","Epoch 363, acc0.89, loss 0.2612, tol 0, val_acc0.85, val_loss0.3564\n","Epoch 364, acc0.90, loss 0.2425, tol 0, val_acc0.85, val_loss0.3486\n","Epoch 365, acc0.87, loss 0.2915, tol 0, val_acc0.86, val_loss0.3194\n","Epoch 366, acc0.90, loss 0.2588, tol 0, val_acc0.86, val_loss0.3305\n","Epoch 367, acc0.90, loss 0.2625, tol 0, val_acc0.87, val_loss0.3170\n","Epoch 368, acc0.89, loss 0.2591, tol 0, val_acc0.85, val_loss0.3230\n","Epoch 369, acc0.87, loss 0.2840, tol 0, val_acc0.87, val_loss0.3169\n","Epoch 370, acc0.89, loss 0.2777, tol 0, val_acc0.86, val_loss0.3108\n","Epoch 371, acc0.89, loss 0.2617, tol 0, val_acc0.86, val_loss0.3146\n","Epoch 372, acc0.89, loss 0.2716, tol 0, val_acc0.87, val_loss0.3132\n","Epoch 373, acc0.88, loss 0.2823, tol 0, val_acc0.86, val_loss0.3143\n","Epoch 374, acc0.89, loss 0.2629, tol 0, val_acc0.86, val_loss0.3207\n","Epoch 375, acc0.90, loss 0.2536, tol 0, val_acc0.87, val_loss0.3121\n","Epoch 376, acc0.89, loss 0.2576, tol 0, val_acc0.86, val_loss0.3311\n","Epoch 377, acc0.88, loss 0.2715, tol 0, val_acc0.85, val_loss0.3522\n","Epoch 378, acc0.87, loss 0.2867, tol 0, val_acc0.87, val_loss0.3121\n","Epoch 379, acc0.90, loss 0.2667, tol 0, val_acc0.86, val_loss0.3192\n","Epoch 380, acc0.87, loss 0.2919, tol 0, val_acc0.87, val_loss0.3127\n","Epoch 381, acc0.89, loss 0.2617, tol 0, val_acc0.86, val_loss0.3209\n","Epoch 382, acc0.89, loss 0.2821, tol 0, val_acc0.87, val_loss0.3134\n","Epoch 383, acc0.89, loss 0.2709, tol 0, val_acc0.86, val_loss0.3353\n","Epoch 384, acc0.89, loss 0.2577, tol 0, val_acc0.87, val_loss0.3147\n","Epoch 385, acc0.89, loss 0.2792, tol 0, val_acc0.87, val_loss0.3142\n","Epoch 386, acc0.91, loss 0.2480, tol 0, val_acc0.86, val_loss0.3231\n","Epoch 387, acc0.89, loss 0.2790, tol 0, val_acc0.86, val_loss0.3170\n","Epoch 388, acc0.89, loss 0.2652, tol 0, val_acc0.87, val_loss0.3177\n","Epoch 389, acc0.89, loss 0.2679, tol 0, val_acc0.86, val_loss0.3153\n","Epoch 390, acc0.89, loss 0.2758, tol 0, val_acc0.87, val_loss0.3186\n","Epoch 391, acc0.90, loss 0.2491, tol 0, val_acc0.86, val_loss0.3244\n","Epoch 392, acc0.90, loss 0.2598, tol 0, val_acc0.86, val_loss0.3366\n","Epoch 393, acc0.89, loss 0.2681, tol 0, val_acc0.86, val_loss0.3277\n","Epoch 394, acc0.90, loss 0.2536, tol 0, val_acc0.86, val_loss0.3218\n","Epoch 395, acc0.88, loss 0.2755, tol 0, val_acc0.86, val_loss0.3333\n","Epoch 396, acc0.89, loss 0.2505, tol 0, val_acc0.87, val_loss0.3167\n","Epoch 397, acc0.90, loss 0.2419, tol 0, val_acc0.86, val_loss0.3361\n","Epoch 398, acc0.89, loss 0.2637, tol 0, val_acc0.87, val_loss0.3214\n","Epoch 399, acc0.90, loss 0.2514, tol 0, val_acc0.87, val_loss0.3058\n","Epoch 400, acc0.89, loss 0.2709, tol 0, val_acc0.86, val_loss0.3075\n","Epoch 401, acc0.90, loss 0.2496, tol 0, val_acc0.86, val_loss0.3315\n","Epoch 402, acc0.90, loss 0.2567, tol 0, val_acc0.86, val_loss0.3421\n","Epoch 403, acc0.89, loss 0.2572, tol 0, val_acc0.86, val_loss0.3206\n","Epoch 404, acc0.90, loss 0.2546, tol 0, val_acc0.88, val_loss0.3189\n","Epoch 405, acc0.89, loss 0.2527, tol 0, val_acc0.87, val_loss0.3174\n","Epoch 406, acc0.90, loss 0.2505, tol 0, val_acc0.86, val_loss0.3347\n","Epoch 407, acc0.91, loss 0.2317, tol 0, val_acc0.87, val_loss0.3140\n","Epoch 408, acc0.89, loss 0.2800, tol 0, val_acc0.87, val_loss0.3085\n","Epoch 409, acc0.90, loss 0.2472, tol 0, val_acc0.87, val_loss0.3221\n","Epoch 410, acc0.90, loss 0.2376, tol 0, val_acc0.85, val_loss0.3417\n","Epoch 411, acc0.90, loss 0.2509, tol 0, val_acc0.87, val_loss0.3083\n","Epoch 412, acc0.91, loss 0.2414, tol 0, val_acc0.87, val_loss0.3069\n","Epoch 413, acc0.91, loss 0.2298, tol 0, val_acc0.87, val_loss0.3127\n","Epoch 414, acc0.90, loss 0.2451, tol 0, val_acc0.87, val_loss0.3165\n","Epoch 415, acc0.88, loss 0.2726, tol 0, val_acc0.86, val_loss0.3073\n","Epoch 416, acc0.91, loss 0.2417, tol 0, val_acc0.87, val_loss0.3101\n","Epoch 417, acc0.91, loss 0.2404, tol 0, val_acc0.87, val_loss0.3071\n","Epoch 418, acc0.91, loss 0.2362, tol 0, val_acc0.87, val_loss0.3036\n","Epoch 419, acc0.92, loss 0.2094, tol 0, val_acc0.86, val_loss0.3381\n","Epoch 420, acc0.90, loss 0.2541, tol 0, val_acc0.87, val_loss0.3114\n","Epoch 421, acc0.89, loss 0.2572, tol 0, val_acc0.87, val_loss0.3127\n","Epoch 422, acc0.90, loss 0.2518, tol 0, val_acc0.86, val_loss0.3050\n","Epoch 423, acc0.89, loss 0.2658, tol 0, val_acc0.86, val_loss0.3033\n","Epoch 424, acc0.91, loss 0.2396, tol 0, val_acc0.87, val_loss0.3104\n","Epoch 425, acc0.89, loss 0.2712, tol 0, val_acc0.88, val_loss0.3077\n","Epoch 426, acc0.91, loss 0.2311, tol 0, val_acc0.88, val_loss0.3048\n","Epoch 427, acc0.91, loss 0.2356, tol 0, val_acc0.87, val_loss0.3125\n","Epoch 428, acc0.90, loss 0.2501, tol 0, val_acc0.87, val_loss0.3175\n","Epoch 429, acc0.90, loss 0.2468, tol 0, val_acc0.86, val_loss0.3102\n","Epoch 430, acc0.90, loss 0.2565, tol 0, val_acc0.86, val_loss0.3056\n","Epoch 431, acc0.91, loss 0.2475, tol 0, val_acc0.87, val_loss0.3163\n","Epoch 432, acc0.90, loss 0.2463, tol 0, val_acc0.87, val_loss0.3085\n","Epoch 433, acc0.89, loss 0.2685, tol 0, val_acc0.87, val_loss0.3028\n","Epoch 434, acc0.91, loss 0.2281, tol 0, val_acc0.87, val_loss0.3087\n","Epoch 435, acc0.90, loss 0.2461, tol 0, val_acc0.87, val_loss0.3096\n","Epoch 436, acc0.91, loss 0.2377, tol 0, val_acc0.86, val_loss0.3098\n","Epoch 437, acc0.90, loss 0.2412, tol 0, val_acc0.87, val_loss0.3027\n","Epoch 438, acc0.90, loss 0.2551, tol 0, val_acc0.87, val_loss0.3030\n","Epoch 439, acc0.91, loss 0.2442, tol 0, val_acc0.86, val_loss0.3036\n","Epoch 440, acc0.90, loss 0.2454, tol 0, val_acc0.86, val_loss0.3124\n","Epoch 441, acc0.90, loss 0.2255, tol 0, val_acc0.86, val_loss0.3088\n","Epoch 442, acc0.90, loss 0.2510, tol 0, val_acc0.86, val_loss0.3010\n","Epoch 443, acc0.89, loss 0.2590, tol 0, val_acc0.87, val_loss0.3002\n","Epoch 444, acc0.90, loss 0.2447, tol 0, val_acc0.86, val_loss0.3093\n","Epoch 445, acc0.90, loss 0.2416, tol 0, val_acc0.86, val_loss0.3039\n","Epoch 446, acc0.90, loss 0.2342, tol 0, val_acc0.86, val_loss0.3108\n","Epoch 447, acc0.91, loss 0.2494, tol 0, val_acc0.86, val_loss0.3227\n","Epoch 448, acc0.89, loss 0.2577, tol 0, val_acc0.87, val_loss0.3058\n","Epoch 449, acc0.89, loss 0.2522, tol 0, val_acc0.86, val_loss0.3089\n","Epoch 450, acc0.90, loss 0.2465, tol 0, val_acc0.87, val_loss0.3035\n","Epoch 451, acc0.91, loss 0.2373, tol 0, val_acc0.87, val_loss0.3023\n","Epoch 452, acc0.89, loss 0.2574, tol 0, val_acc0.87, val_loss0.2973\n","Epoch 453, acc0.90, loss 0.2499, tol 0, val_acc0.86, val_loss0.3158\n","Epoch 454, acc0.90, loss 0.2490, tol 0, val_acc0.87, val_loss0.3059\n","Epoch 455, acc0.90, loss 0.2360, tol 0, val_acc0.87, val_loss0.3037\n","Epoch 456, acc0.90, loss 0.2515, tol 0, val_acc0.86, val_loss0.3161\n","Epoch 457, acc0.89, loss 0.2643, tol 0, val_acc0.87, val_loss0.2991\n","Epoch 458, acc0.90, loss 0.2379, tol 0, val_acc0.87, val_loss0.3005\n","Epoch 459, acc0.90, loss 0.2539, tol 0, val_acc0.88, val_loss0.3015\n","Epoch 460, acc0.91, loss 0.2203, tol 0, val_acc0.87, val_loss0.2997\n","Epoch 461, acc0.89, loss 0.2523, tol 0, val_acc0.87, val_loss0.2980\n","Epoch 462, acc0.91, loss 0.2324, tol 0, val_acc0.87, val_loss0.2969\n","Epoch 463, acc0.89, loss 0.2667, tol 0, val_acc0.86, val_loss0.3282\n","Epoch 464, acc0.89, loss 0.2426, tol 0, val_acc0.87, val_loss0.2961\n","Epoch 465, acc0.89, loss 0.2686, tol 0, val_acc0.87, val_loss0.3011\n","Epoch 466, acc0.91, loss 0.2178, tol 0, val_acc0.87, val_loss0.3003\n","Epoch 467, acc0.90, loss 0.2485, tol 0, val_acc0.87, val_loss0.2986\n","Epoch 468, acc0.91, loss 0.2211, tol 0, val_acc0.87, val_loss0.3049\n","Epoch 469, acc0.90, loss 0.2268, tol 0, val_acc0.87, val_loss0.3052\n","Epoch 470, acc0.91, loss 0.2256, tol 0, val_acc0.86, val_loss0.3053\n","Epoch 471, acc0.89, loss 0.2448, tol 0, val_acc0.86, val_loss0.2964\n","Epoch 472, acc0.90, loss 0.2450, tol 0, val_acc0.86, val_loss0.2997\n","Epoch 473, acc0.90, loss 0.2426, tol 0, val_acc0.86, val_loss0.2966\n","Epoch 474, acc0.90, loss 0.2403, tol 0, val_acc0.87, val_loss0.3112\n","Epoch 475, acc0.91, loss 0.2353, tol 0, val_acc0.87, val_loss0.2932\n","Epoch 476, acc0.90, loss 0.2303, tol 0, val_acc0.87, val_loss0.2996\n","Epoch 477, acc0.90, loss 0.2255, tol 0, val_acc0.87, val_loss0.2989\n","Epoch 478, acc0.90, loss 0.2353, tol 0, val_acc0.87, val_loss0.3037\n","Epoch 479, acc0.90, loss 0.2522, tol 0, val_acc0.86, val_loss0.3122\n","Epoch 480, acc0.92, loss 0.2174, tol 0, val_acc0.87, val_loss0.3039\n","Epoch 481, acc0.89, loss 0.2388, tol 0, val_acc0.87, val_loss0.3125\n","Epoch 482, acc0.92, loss 0.2196, tol 0, val_acc0.88, val_loss0.3074\n","Epoch 483, acc0.91, loss 0.2294, tol 0, val_acc0.87, val_loss0.3184\n","Epoch 484, acc0.91, loss 0.2299, tol 0, val_acc0.87, val_loss0.3107\n","Epoch 485, acc0.92, loss 0.2182, tol 0, val_acc0.87, val_loss0.2963\n","Epoch 486, acc0.91, loss 0.2231, tol 0, val_acc0.88, val_loss0.2952\n","Epoch 487, acc0.91, loss 0.2235, tol 0, val_acc0.87, val_loss0.2963\n","Epoch 488, acc0.92, loss 0.2029, tol 0, val_acc0.87, val_loss0.2971\n","Epoch 489, acc0.90, loss 0.2235, tol 0, val_acc0.87, val_loss0.2978\n","Epoch 490, acc0.90, loss 0.2305, tol 0, val_acc0.87, val_loss0.3175\n","Epoch 491, acc0.91, loss 0.2282, tol 0, val_acc0.88, val_loss0.3046\n","Epoch 492, acc0.89, loss 0.2556, tol 0, val_acc0.86, val_loss0.3068\n","Epoch 493, acc0.90, loss 0.2371, tol 0, val_acc0.87, val_loss0.2926\n","Epoch 494, acc0.92, loss 0.2175, tol 0, val_acc0.86, val_loss0.3291\n","Epoch 495, acc0.92, loss 0.2105, tol 0, val_acc0.86, val_loss0.2946\n","Epoch 496, acc0.91, loss 0.2318, tol 0, val_acc0.88, val_loss0.2956\n","Epoch 497, acc0.91, loss 0.2167, tol 0, val_acc0.87, val_loss0.3030\n","Epoch 498, acc0.90, loss 0.2381, tol 0, val_acc0.87, val_loss0.3105\n","Epoch 499, acc0.89, loss 0.2506, tol 0, val_acc0.87, val_loss0.2992\n","Epoch 500, acc0.91, loss 0.2265, tol 0, val_acc0.88, val_loss0.3030\n","Epoch 501, acc0.91, loss 0.2189, tol 1, val_acc0.87, val_loss0.3069\n","Reach Max Epoch Number\n","+------------------------+------------+\n","|        Modules         | Parameters |\n","+------------------------+------------+\n","| layers.0.0.fc_Q.weight |     45     |\n","| layers.0.0.fc_K.weight |     45     |\n","| layers.0.0.fc_V.weight |     45     |\n","| layers.0.1.fc_Q.weight |     45     |\n","| layers.0.1.fc_K.weight |     45     |\n","| layers.0.1.fc_V.weight |     45     |\n","| layers.1.0.fc_Q.weight |    162     |\n","| layers.1.0.fc_K.weight |    162     |\n","| layers.1.0.fc_V.weight |    162     |\n","| layers.1.1.fc_Q.weight |    162     |\n","| layers.1.1.fc_K.weight |    162     |\n","| layers.1.1.fc_V.weight |    162     |\n","|    classify.weight     |     36     |\n","|     classify.bias      |     2      |\n","+------------------------+------------+\n","Total Trainable Params: 1280\n","Epoch 0, acc0.51, loss 0.7000, tol 0, val_acc0.50, val_loss0.6971 -- checkpoint saved\n","Epoch 1, acc0.48, loss 0.7000, tol 0, val_acc0.53, val_loss0.6933 -- checkpoint saved\n","Epoch 2, acc0.52, loss 0.6926, tol 0, val_acc0.52, val_loss0.6918\n","Epoch 3, acc0.52, loss 0.6903, tol 0, val_acc0.54, val_loss0.6908 -- checkpoint saved\n","Epoch 4, acc0.55, loss 0.6868, tol 0, val_acc0.50, val_loss0.6913\n","Epoch 5, acc0.50, loss 0.6937, tol 0, val_acc0.53, val_loss0.6914\n","Epoch 6, acc0.55, loss 0.6876, tol 0, val_acc0.54, val_loss0.6882 -- checkpoint saved\n","Epoch 7, acc0.55, loss 0.6881, tol 0, val_acc0.55, val_loss0.6878 -- checkpoint saved\n","Epoch 8, acc0.54, loss 0.6871, tol 0, val_acc0.57, val_loss0.6866 -- checkpoint saved\n","Epoch 9, acc0.54, loss 0.6862, tol 0, val_acc0.57, val_loss0.6855 -- checkpoint saved\n","Epoch 10, acc0.55, loss 0.6881, tol 0, val_acc0.59, val_loss0.6842 -- checkpoint saved\n","Epoch 11, acc0.54, loss 0.6871, tol 0, val_acc0.56, val_loss0.6835\n","Epoch 12, acc0.57, loss 0.6840, tol 0, val_acc0.58, val_loss0.6826\n","Epoch 13, acc0.56, loss 0.6844, tol 0, val_acc0.57, val_loss0.6809\n","Epoch 14, acc0.56, loss 0.6828, tol 0, val_acc0.59, val_loss0.6789 -- checkpoint saved\n","Epoch 15, acc0.59, loss 0.6798, tol 0, val_acc0.60, val_loss0.6770 -- checkpoint saved\n","Epoch 16, acc0.59, loss 0.6769, tol 0, val_acc0.55, val_loss0.6806\n","Epoch 17, acc0.58, loss 0.6772, tol 0, val_acc0.62, val_loss0.6743 -- checkpoint saved\n","Epoch 18, acc0.58, loss 0.6787, tol 0, val_acc0.60, val_loss0.6730\n","Epoch 19, acc0.62, loss 0.6710, tol 0, val_acc0.61, val_loss0.6695\n","Epoch 20, acc0.60, loss 0.6747, tol 0, val_acc0.61, val_loss0.6669\n","Epoch 21, acc0.59, loss 0.6670, tol 0, val_acc0.60, val_loss0.6670\n","Epoch 22, acc0.61, loss 0.6626, tol 0, val_acc0.60, val_loss0.6624\n","Epoch 23, acc0.61, loss 0.6592, tol 0, val_acc0.65, val_loss0.6485 -- checkpoint saved\n","Epoch 24, acc0.65, loss 0.6493, tol 0, val_acc0.69, val_loss0.6348 -- checkpoint saved\n","Epoch 25, acc0.65, loss 0.6334, tol 0, val_acc0.69, val_loss0.6198 -- checkpoint saved\n","Epoch 26, acc0.68, loss 0.6159, tol 0, val_acc0.71, val_loss0.6049 -- checkpoint saved\n","Epoch 27, acc0.68, loss 0.6065, tol 0, val_acc0.73, val_loss0.5883 -- checkpoint saved\n","Epoch 28, acc0.71, loss 0.5835, tol 0, val_acc0.73, val_loss0.5770 -- checkpoint saved\n","Epoch 29, acc0.73, loss 0.5612, tol 0, val_acc0.73, val_loss0.5525 -- checkpoint saved\n","Epoch 30, acc0.73, loss 0.5588, tol 0, val_acc0.76, val_loss0.5418 -- checkpoint saved\n","Epoch 31, acc0.74, loss 0.5337, tol 0, val_acc0.77, val_loss0.5189 -- checkpoint saved\n","Epoch 32, acc0.76, loss 0.5252, tol 0, val_acc0.78, val_loss0.5043 -- checkpoint saved\n","Epoch 33, acc0.77, loss 0.4918, tol 0, val_acc0.78, val_loss0.4886 -- checkpoint saved\n","Epoch 34, acc0.80, loss 0.4740, tol 0, val_acc0.77, val_loss0.4804\n","Epoch 35, acc0.79, loss 0.4728, tol 0, val_acc0.77, val_loss0.4734\n","Epoch 36, acc0.81, loss 0.4611, tol 0, val_acc0.78, val_loss0.4595\n","Epoch 37, acc0.81, loss 0.4403, tol 0, val_acc0.80, val_loss0.4476 -- checkpoint saved\n","Epoch 38, acc0.80, loss 0.4464, tol 0, val_acc0.80, val_loss0.4382 -- checkpoint saved\n","Epoch 39, acc0.82, loss 0.4277, tol 0, val_acc0.81, val_loss0.4307 -- checkpoint saved\n","Epoch 40, acc0.81, loss 0.4241, tol 0, val_acc0.81, val_loss0.4197\n","Epoch 41, acc0.82, loss 0.4126, tol 0, val_acc0.81, val_loss0.4196\n","Epoch 42, acc0.83, loss 0.4020, tol 0, val_acc0.81, val_loss0.4131 -- checkpoint saved\n","Epoch 43, acc0.82, loss 0.4068, tol 0, val_acc0.82, val_loss0.4152\n","Epoch 44, acc0.84, loss 0.3882, tol 0, val_acc0.81, val_loss0.4134\n","Epoch 45, acc0.84, loss 0.3848, tol 0, val_acc0.81, val_loss0.4198\n","Epoch 46, acc0.84, loss 0.3905, tol 0, val_acc0.82, val_loss0.4126 -- checkpoint saved\n","Epoch 47, acc0.83, loss 0.3958, tol 0, val_acc0.82, val_loss0.4035 -- checkpoint saved\n","Epoch 48, acc0.85, loss 0.3934, tol 0, val_acc0.83, val_loss0.3896 -- checkpoint saved\n","Epoch 49, acc0.84, loss 0.3802, tol 0, val_acc0.81, val_loss0.3969\n","Epoch 50, acc0.82, loss 0.3919, tol 0, val_acc0.80, val_loss0.4071\n","Epoch 51, acc0.85, loss 0.3558, tol 0, val_acc0.82, val_loss0.4031\n","Epoch 52, acc0.84, loss 0.3600, tol 0, val_acc0.83, val_loss0.3849 -- checkpoint saved\n","Epoch 53, acc0.84, loss 0.3743, tol 0, val_acc0.82, val_loss0.3845\n","Epoch 54, acc0.85, loss 0.3575, tol 0, val_acc0.82, val_loss0.3925\n","Epoch 55, acc0.84, loss 0.3651, tol 0, val_acc0.82, val_loss0.3791\n","Epoch 56, acc0.86, loss 0.3406, tol 0, val_acc0.82, val_loss0.3856\n","Epoch 57, acc0.85, loss 0.3537, tol 0, val_acc0.82, val_loss0.3831\n","Epoch 58, acc0.86, loss 0.3428, tol 0, val_acc0.82, val_loss0.3863\n","Epoch 59, acc0.85, loss 0.3561, tol 0, val_acc0.85, val_loss0.3818\n","Epoch 60, acc0.84, loss 0.3769, tol 0, val_acc0.83, val_loss0.3743\n","Epoch 61, acc0.86, loss 0.3345, tol 0, val_acc0.84, val_loss0.3763\n","Epoch 62, acc0.86, loss 0.3489, tol 0, val_acc0.82, val_loss0.3883\n","Epoch 63, acc0.85, loss 0.3610, tol 0, val_acc0.81, val_loss0.3914\n","Epoch 64, acc0.86, loss 0.3566, tol 0, val_acc0.83, val_loss0.3646\n","Epoch 65, acc0.86, loss 0.3352, tol 0, val_acc0.85, val_loss0.3721\n","Epoch 66, acc0.87, loss 0.3415, tol 0, val_acc0.84, val_loss0.3878\n","Epoch 67, acc0.88, loss 0.3156, tol 0, val_acc0.83, val_loss0.3836\n","Epoch 68, acc0.84, loss 0.3775, tol 0, val_acc0.82, val_loss0.3720\n","Epoch 69, acc0.86, loss 0.3352, tol 0, val_acc0.82, val_loss0.3668\n","Epoch 70, acc0.87, loss 0.3469, tol 0, val_acc0.86, val_loss0.3624 -- checkpoint saved\n","Epoch 71, acc0.87, loss 0.3331, tol 0, val_acc0.85, val_loss0.3579\n","Epoch 72, acc0.86, loss 0.3410, tol 0, val_acc0.82, val_loss0.3682\n","Epoch 73, acc0.87, loss 0.3465, tol 0, val_acc0.85, val_loss0.3660\n","Epoch 74, acc0.87, loss 0.3413, tol 0, val_acc0.86, val_loss0.3597\n","Epoch 75, acc0.88, loss 0.3205, tol 0, val_acc0.85, val_loss0.3570\n","Epoch 76, acc0.88, loss 0.3260, tol 0, val_acc0.85, val_loss0.3558\n","Epoch 77, acc0.88, loss 0.3047, tol 0, val_acc0.86, val_loss0.3590\n","Epoch 78, acc0.87, loss 0.3272, tol 0, val_acc0.83, val_loss0.3581\n","Epoch 79, acc0.87, loss 0.3393, tol 0, val_acc0.85, val_loss0.3591\n","Epoch 80, acc0.87, loss 0.3271, tol 0, val_acc0.85, val_loss0.3492\n","Epoch 81, acc0.87, loss 0.3215, tol 0, val_acc0.86, val_loss0.3642\n","Epoch 82, acc0.89, loss 0.2960, tol 0, val_acc0.83, val_loss0.3683\n","Epoch 83, acc0.86, loss 0.3244, tol 0, val_acc0.84, val_loss0.3710\n","Epoch 84, acc0.87, loss 0.3228, tol 0, val_acc0.85, val_loss0.3631\n","Epoch 85, acc0.89, loss 0.3012, tol 0, val_acc0.86, val_loss0.3553\n","Epoch 86, acc0.88, loss 0.3147, tol 0, val_acc0.85, val_loss0.3588\n","Epoch 87, acc0.88, loss 0.3375, tol 0, val_acc0.86, val_loss0.3462\n","Epoch 88, acc0.90, loss 0.2903, tol 0, val_acc0.84, val_loss0.3512\n","Epoch 89, acc0.87, loss 0.3393, tol 0, val_acc0.84, val_loss0.3524\n","Epoch 90, acc0.88, loss 0.3282, tol 0, val_acc0.84, val_loss0.3539\n","Epoch 91, acc0.89, loss 0.2997, tol 0, val_acc0.86, val_loss0.3530\n","Epoch 92, acc0.87, loss 0.3237, tol 0, val_acc0.85, val_loss0.3437\n","Epoch 93, acc0.88, loss 0.3098, tol 0, val_acc0.87, val_loss0.3452\n","Epoch 94, acc0.88, loss 0.3244, tol 0, val_acc0.87, val_loss0.3440\n","Epoch 95, acc0.88, loss 0.3006, tol 0, val_acc0.86, val_loss0.3455\n","Epoch 96, acc0.87, loss 0.3163, tol 0, val_acc0.86, val_loss0.3438\n","Epoch 97, acc0.88, loss 0.3113, tol 0, val_acc0.85, val_loss0.3488\n","Epoch 98, acc0.87, loss 0.3227, tol 0, val_acc0.86, val_loss0.3447\n","Epoch 99, acc0.88, loss 0.3206, tol 0, val_acc0.84, val_loss0.3594\n","Epoch 100, acc0.88, loss 0.3037, tol 0, val_acc0.86, val_loss0.3452\n","Epoch 101, acc0.92, loss 0.2551, tol 0, val_acc0.87, val_loss0.3476\n","Epoch 102, acc0.89, loss 0.2980, tol 0, val_acc0.86, val_loss0.3522\n","Epoch 103, acc0.88, loss 0.3132, tol 0, val_acc0.86, val_loss0.3564\n","Epoch 104, acc0.87, loss 0.3140, tol 0, val_acc0.86, val_loss0.3451\n","Epoch 105, acc0.88, loss 0.3268, tol 0, val_acc0.85, val_loss0.3412\n","Epoch 106, acc0.89, loss 0.2979, tol 0, val_acc0.87, val_loss0.3345 -- checkpoint saved\n","Epoch 107, acc0.90, loss 0.2812, tol 0, val_acc0.87, val_loss0.3347\n","Epoch 108, acc0.87, loss 0.3207, tol 0, val_acc0.86, val_loss0.3319\n","Epoch 109, acc0.88, loss 0.3077, tol 0, val_acc0.87, val_loss0.3357\n","Epoch 110, acc0.88, loss 0.3113, tol 0, val_acc0.86, val_loss0.3384\n","Epoch 111, acc0.89, loss 0.2932, tol 0, val_acc0.85, val_loss0.3501\n","Epoch 112, acc0.88, loss 0.3050, tol 0, val_acc0.86, val_loss0.3434\n","Epoch 113, acc0.88, loss 0.2998, tol 0, val_acc0.87, val_loss0.3425\n","Epoch 114, acc0.90, loss 0.2725, tol 0, val_acc0.87, val_loss0.3420\n","Epoch 115, acc0.89, loss 0.2915, tol 0, val_acc0.85, val_loss0.3467\n","Epoch 116, acc0.89, loss 0.3004, tol 0, val_acc0.86, val_loss0.3396\n","Epoch 117, acc0.89, loss 0.2947, tol 0, val_acc0.86, val_loss0.3366\n","Epoch 118, acc0.90, loss 0.2893, tol 0, val_acc0.86, val_loss0.3401\n","Epoch 119, acc0.88, loss 0.2974, tol 0, val_acc0.88, val_loss0.3312 -- checkpoint saved\n","Epoch 120, acc0.90, loss 0.2857, tol 0, val_acc0.87, val_loss0.3310\n","Epoch 121, acc0.90, loss 0.3076, tol 0, val_acc0.87, val_loss0.3310\n","Epoch 122, acc0.88, loss 0.2979, tol 0, val_acc0.87, val_loss0.3330\n","Epoch 123, acc0.88, loss 0.3106, tol 0, val_acc0.86, val_loss0.3400\n","Epoch 124, acc0.89, loss 0.2982, tol 0, val_acc0.87, val_loss0.3263\n","Epoch 125, acc0.89, loss 0.3036, tol 0, val_acc0.88, val_loss0.3281\n","Epoch 126, acc0.89, loss 0.2912, tol 0, val_acc0.86, val_loss0.3312\n","Epoch 127, acc0.89, loss 0.2883, tol 0, val_acc0.88, val_loss0.3337\n","Epoch 128, acc0.89, loss 0.3027, tol 0, val_acc0.87, val_loss0.3361\n","Epoch 129, acc0.89, loss 0.2787, tol 0, val_acc0.86, val_loss0.3429\n","Epoch 130, acc0.89, loss 0.2905, tol 0, val_acc0.88, val_loss0.3351\n","Epoch 131, acc0.90, loss 0.2783, tol 0, val_acc0.86, val_loss0.3347\n","Epoch 132, acc0.90, loss 0.2820, tol 0, val_acc0.88, val_loss0.3280\n","Epoch 133, acc0.89, loss 0.2891, tol 0, val_acc0.87, val_loss0.3284\n","Epoch 134, acc0.88, loss 0.2961, tol 0, val_acc0.88, val_loss0.3238 -- checkpoint saved\n","Epoch 135, acc0.89, loss 0.2865, tol 0, val_acc0.88, val_loss0.3237 -- checkpoint saved\n","Epoch 136, acc0.88, loss 0.2951, tol 0, val_acc0.88, val_loss0.3273\n","Epoch 137, acc0.89, loss 0.2901, tol 0, val_acc0.87, val_loss0.3381\n","Epoch 138, acc0.89, loss 0.2856, tol 0, val_acc0.88, val_loss0.3212 -- checkpoint saved\n","Epoch 139, acc0.90, loss 0.2858, tol 0, val_acc0.87, val_loss0.3295\n","Epoch 140, acc0.90, loss 0.2694, tol 0, val_acc0.87, val_loss0.3398\n","Epoch 141, acc0.89, loss 0.2882, tol 0, val_acc0.88, val_loss0.3385\n","Epoch 142, acc0.89, loss 0.2863, tol 0, val_acc0.86, val_loss0.3517\n","Epoch 143, acc0.89, loss 0.2891, tol 0, val_acc0.87, val_loss0.3298\n","Epoch 144, acc0.89, loss 0.2953, tol 0, val_acc0.88, val_loss0.3280\n","Epoch 145, acc0.89, loss 0.2928, tol 0, val_acc0.87, val_loss0.3267\n","Epoch 146, acc0.90, loss 0.2810, tol 0, val_acc0.86, val_loss0.3285\n","Epoch 147, acc0.90, loss 0.2884, tol 0, val_acc0.87, val_loss0.3327\n","Epoch 148, acc0.90, loss 0.2620, tol 0, val_acc0.87, val_loss0.3287\n","Epoch 149, acc0.89, loss 0.2881, tol 0, val_acc0.85, val_loss0.3388\n","Epoch 150, acc0.88, loss 0.2870, tol 0, val_acc0.87, val_loss0.3194\n","Epoch 151, acc0.89, loss 0.2856, tol 0, val_acc0.88, val_loss0.3365\n","Epoch 152, acc0.89, loss 0.2725, tol 0, val_acc0.88, val_loss0.3328\n","Epoch 153, acc0.90, loss 0.2821, tol 0, val_acc0.86, val_loss0.3272\n","Epoch 154, acc0.90, loss 0.2928, tol 0, val_acc0.88, val_loss0.3253\n","Epoch 155, acc0.89, loss 0.2847, tol 0, val_acc0.88, val_loss0.3289\n","Epoch 156, acc0.89, loss 0.2796, tol 0, val_acc0.87, val_loss0.3333\n","Epoch 157, acc0.90, loss 0.2738, tol 0, val_acc0.88, val_loss0.3196\n","Epoch 158, acc0.88, loss 0.3238, tol 0, val_acc0.88, val_loss0.3243\n","Epoch 159, acc0.89, loss 0.2857, tol 0, val_acc0.86, val_loss0.3353\n","Epoch 160, acc0.90, loss 0.2788, tol 0, val_acc0.87, val_loss0.3350\n","Epoch 161, acc0.88, loss 0.2998, tol 0, val_acc0.88, val_loss0.3302\n","Epoch 162, acc0.89, loss 0.2954, tol 0, val_acc0.87, val_loss0.3438\n","Epoch 163, acc0.90, loss 0.2716, tol 0, val_acc0.87, val_loss0.3255\n","Epoch 164, acc0.89, loss 0.2820, tol 0, val_acc0.88, val_loss0.3214\n","Epoch 165, acc0.90, loss 0.2751, tol 0, val_acc0.88, val_loss0.3220\n","Epoch 166, acc0.89, loss 0.2878, tol 0, val_acc0.87, val_loss0.3178\n","Epoch 167, acc0.90, loss 0.2722, tol 0, val_acc0.87, val_loss0.3285\n","Epoch 168, acc0.89, loss 0.2861, tol 0, val_acc0.87, val_loss0.3235\n","Epoch 169, acc0.89, loss 0.2679, tol 0, val_acc0.88, val_loss0.3084 -- checkpoint saved\n","Epoch 170, acc0.90, loss 0.2882, tol 0, val_acc0.88, val_loss0.3192\n","Epoch 171, acc0.90, loss 0.2914, tol 0, val_acc0.87, val_loss0.3344\n","Epoch 172, acc0.89, loss 0.3070, tol 0, val_acc0.87, val_loss0.3230\n","Epoch 173, acc0.90, loss 0.2724, tol 0, val_acc0.87, val_loss0.3160\n","Epoch 174, acc0.90, loss 0.2738, tol 0, val_acc0.88, val_loss0.3188\n","Epoch 175, acc0.89, loss 0.2785, tol 0, val_acc0.88, val_loss0.3207\n","Epoch 176, acc0.91, loss 0.2676, tol 0, val_acc0.87, val_loss0.3153\n","Epoch 177, acc0.90, loss 0.2682, tol 0, val_acc0.88, val_loss0.3167\n","Epoch 178, acc0.90, loss 0.2564, tol 0, val_acc0.87, val_loss0.3361\n","Epoch 179, acc0.89, loss 0.2644, tol 0, val_acc0.87, val_loss0.3290\n","Epoch 180, acc0.88, loss 0.2879, tol 0, val_acc0.88, val_loss0.3213\n","Epoch 181, acc0.91, loss 0.2636, tol 0, val_acc0.88, val_loss0.3171\n","Epoch 182, acc0.90, loss 0.2856, tol 0, val_acc0.87, val_loss0.3270\n","Epoch 183, acc0.89, loss 0.3068, tol 0, val_acc0.87, val_loss0.3298\n","Epoch 184, acc0.90, loss 0.2617, tol 0, val_acc0.87, val_loss0.3153\n","Epoch 185, acc0.89, loss 0.2953, tol 0, val_acc0.88, val_loss0.3224\n","Epoch 186, acc0.90, loss 0.2754, tol 0, val_acc0.88, val_loss0.3190\n","Epoch 187, acc0.89, loss 0.2711, tol 0, val_acc0.88, val_loss0.3190\n","Epoch 188, acc0.91, loss 0.2652, tol 0, val_acc0.87, val_loss0.3204\n","Epoch 189, acc0.89, loss 0.2890, tol 0, val_acc0.88, val_loss0.3173\n","Epoch 190, acc0.91, loss 0.2534, tol 0, val_acc0.87, val_loss0.3200\n","Epoch 191, acc0.89, loss 0.2803, tol 0, val_acc0.88, val_loss0.3202\n","Epoch 192, acc0.90, loss 0.2659, tol 0, val_acc0.88, val_loss0.3170\n","Epoch 193, acc0.90, loss 0.2743, tol 0, val_acc0.87, val_loss0.3228\n","Epoch 194, acc0.91, loss 0.2652, tol 0, val_acc0.87, val_loss0.3292\n","Epoch 195, acc0.90, loss 0.2624, tol 0, val_acc0.87, val_loss0.3386\n","Epoch 196, acc0.90, loss 0.2688, tol 0, val_acc0.86, val_loss0.3365\n","Epoch 197, acc0.90, loss 0.2649, tol 0, val_acc0.88, val_loss0.3160\n","Epoch 198, acc0.89, loss 0.2912, tol 0, val_acc0.87, val_loss0.3198\n","Epoch 199, acc0.90, loss 0.2543, tol 0, val_acc0.88, val_loss0.3214\n","Epoch 200, acc0.92, loss 0.2535, tol 0, val_acc0.88, val_loss0.3121\n","Epoch 201, acc0.90, loss 0.2624, tol 0, val_acc0.88, val_loss0.3146\n","Epoch 202, acc0.90, loss 0.2660, tol 0, val_acc0.87, val_loss0.3292\n","Epoch 203, acc0.90, loss 0.2529, tol 0, val_acc0.88, val_loss0.3316\n","Epoch 204, acc0.91, loss 0.2605, tol 0, val_acc0.88, val_loss0.3165\n","Epoch 205, acc0.90, loss 0.2686, tol 0, val_acc0.88, val_loss0.3177\n","Epoch 206, acc0.91, loss 0.2604, tol 0, val_acc0.87, val_loss0.3330\n","Epoch 207, acc0.89, loss 0.2743, tol 0, val_acc0.88, val_loss0.3152\n","Epoch 208, acc0.89, loss 0.2757, tol 0, val_acc0.87, val_loss0.3208\n","Epoch 209, acc0.90, loss 0.2810, tol 0, val_acc0.88, val_loss0.3154\n","Epoch 210, acc0.89, loss 0.2741, tol 0, val_acc0.88, val_loss0.3179\n","Epoch 211, acc0.89, loss 0.2731, tol 0, val_acc0.88, val_loss0.3153\n","Epoch 212, acc0.90, loss 0.2488, tol 0, val_acc0.87, val_loss0.3261\n","Epoch 213, acc0.89, loss 0.2829, tol 0, val_acc0.88, val_loss0.3242\n","Epoch 214, acc0.89, loss 0.2804, tol 0, val_acc0.88, val_loss0.3091\n","Epoch 215, acc0.89, loss 0.2741, tol 0, val_acc0.88, val_loss0.3074\n","Epoch 216, acc0.90, loss 0.2699, tol 0, val_acc0.86, val_loss0.3330\n","Epoch 217, acc0.90, loss 0.2574, tol 0, val_acc0.87, val_loss0.3124\n","Epoch 218, acc0.89, loss 0.2788, tol 0, val_acc0.87, val_loss0.3126\n","Epoch 219, acc0.88, loss 0.2779, tol 0, val_acc0.87, val_loss0.3165\n","Epoch 220, acc0.91, loss 0.2683, tol 0, val_acc0.87, val_loss0.3242\n","Epoch 221, acc0.91, loss 0.2494, tol 0, val_acc0.88, val_loss0.3100\n","Epoch 222, acc0.91, loss 0.2704, tol 0, val_acc0.87, val_loss0.3134\n","Epoch 223, acc0.92, loss 0.2527, tol 0, val_acc0.87, val_loss0.3244\n","Epoch 224, acc0.91, loss 0.2503, tol 0, val_acc0.88, val_loss0.3122\n","Epoch 225, acc0.91, loss 0.2581, tol 0, val_acc0.88, val_loss0.3122\n","Epoch 226, acc0.90, loss 0.2790, tol 0, val_acc0.87, val_loss0.3142\n","Epoch 227, acc0.90, loss 0.2661, tol 0, val_acc0.88, val_loss0.3122\n","Epoch 228, acc0.90, loss 0.2663, tol 0, val_acc0.88, val_loss0.3174\n","Epoch 229, acc0.90, loss 0.2665, tol 0, val_acc0.88, val_loss0.3108\n","Epoch 230, acc0.90, loss 0.2646, tol 0, val_acc0.87, val_loss0.3180\n","Epoch 231, acc0.90, loss 0.2682, tol 0, val_acc0.87, val_loss0.3089\n","Epoch 232, acc0.91, loss 0.2429, tol 0, val_acc0.88, val_loss0.3098\n","Epoch 233, acc0.90, loss 0.2790, tol 0, val_acc0.87, val_loss0.3331\n","Epoch 234, acc0.90, loss 0.2780, tol 0, val_acc0.87, val_loss0.3153\n","Epoch 235, acc0.90, loss 0.2702, tol 0, val_acc0.88, val_loss0.3138\n","Epoch 236, acc0.91, loss 0.2425, tol 0, val_acc0.87, val_loss0.3140\n","Epoch 237, acc0.90, loss 0.2515, tol 0, val_acc0.87, val_loss0.3190\n","Epoch 238, acc0.91, loss 0.2652, tol 0, val_acc0.87, val_loss0.3196\n","Epoch 239, acc0.90, loss 0.2531, tol 0, val_acc0.87, val_loss0.3063\n","Epoch 240, acc0.90, loss 0.2587, tol 0, val_acc0.88, val_loss0.3188\n","Epoch 241, acc0.91, loss 0.2255, tol 0, val_acc0.88, val_loss0.3234\n","Epoch 242, acc0.91, loss 0.2628, tol 0, val_acc0.87, val_loss0.3170\n","Epoch 243, acc0.90, loss 0.2607, tol 0, val_acc0.88, val_loss0.3166\n","Epoch 244, acc0.90, loss 0.2562, tol 0, val_acc0.87, val_loss0.3253\n","Epoch 245, acc0.89, loss 0.2678, tol 0, val_acc0.87, val_loss0.3241\n","Epoch 246, acc0.89, loss 0.2939, tol 0, val_acc0.87, val_loss0.3224\n","Epoch 247, acc0.90, loss 0.2628, tol 0, val_acc0.88, val_loss0.3058\n","Epoch 248, acc0.91, loss 0.2590, tol 0, val_acc0.87, val_loss0.3039\n","Epoch 249, acc0.91, loss 0.2496, tol 0, val_acc0.88, val_loss0.3050\n","Epoch 250, acc0.91, loss 0.2362, tol 0, val_acc0.88, val_loss0.3180\n","Epoch 251, acc0.90, loss 0.2557, tol 0, val_acc0.87, val_loss0.3297\n","Epoch 252, acc0.90, loss 0.2594, tol 0, val_acc0.87, val_loss0.3169\n","Epoch 253, acc0.90, loss 0.2711, tol 0, val_acc0.87, val_loss0.3189\n","Epoch 254, acc0.91, loss 0.2557, tol 0, val_acc0.87, val_loss0.3078\n","Epoch 255, acc0.91, loss 0.2512, tol 0, val_acc0.87, val_loss0.3126\n","Epoch 256, acc0.90, loss 0.2769, tol 0, val_acc0.87, val_loss0.3274\n","Epoch 257, acc0.90, loss 0.2667, tol 0, val_acc0.87, val_loss0.3043\n","Epoch 258, acc0.91, loss 0.2504, tol 0, val_acc0.87, val_loss0.3166\n","Epoch 259, acc0.90, loss 0.2569, tol 0, val_acc0.86, val_loss0.3173\n","Epoch 260, acc0.91, loss 0.2423, tol 0, val_acc0.88, val_loss0.3070\n","Epoch 261, acc0.90, loss 0.2603, tol 0, val_acc0.87, val_loss0.3170\n","Epoch 262, acc0.90, loss 0.2547, tol 0, val_acc0.88, val_loss0.3153\n","Epoch 263, acc0.91, loss 0.2545, tol 0, val_acc0.87, val_loss0.3186\n","Epoch 264, acc0.90, loss 0.2651, tol 0, val_acc0.88, val_loss0.3143\n","Epoch 265, acc0.90, loss 0.2641, tol 0, val_acc0.88, val_loss0.3133\n","Epoch 266, acc0.90, loss 0.2621, tol 0, val_acc0.86, val_loss0.3160\n","Epoch 267, acc0.90, loss 0.2564, tol 0, val_acc0.87, val_loss0.3051\n","Epoch 268, acc0.91, loss 0.2473, tol 0, val_acc0.88, val_loss0.3005\n","Epoch 269, acc0.91, loss 0.2600, tol 0, val_acc0.87, val_loss0.3252\n","Epoch 270, acc0.91, loss 0.2651, tol 0, val_acc0.88, val_loss0.3170\n","Epoch 271, acc0.91, loss 0.2364, tol 0, val_acc0.87, val_loss0.3205\n","Epoch 272, acc0.91, loss 0.2378, tol 0, val_acc0.87, val_loss0.3107\n","Epoch 273, acc0.91, loss 0.2520, tol 0, val_acc0.87, val_loss0.3197\n","Epoch 274, acc0.91, loss 0.2275, tol 0, val_acc0.87, val_loss0.3106\n","Epoch 275, acc0.92, loss 0.2346, tol 0, val_acc0.88, val_loss0.3063\n","Epoch 276, acc0.90, loss 0.2472, tol 0, val_acc0.87, val_loss0.3152\n","Epoch 277, acc0.90, loss 0.2598, tol 0, val_acc0.86, val_loss0.3475\n","Epoch 278, acc0.91, loss 0.2471, tol 0, val_acc0.88, val_loss0.3197\n","Epoch 279, acc0.89, loss 0.2733, tol 0, val_acc0.87, val_loss0.3065\n","Epoch 280, acc0.91, loss 0.2623, tol 0, val_acc0.88, val_loss0.3025\n","Epoch 281, acc0.91, loss 0.2609, tol 0, val_acc0.87, val_loss0.3008\n","Epoch 282, acc0.92, loss 0.2497, tol 0, val_acc0.87, val_loss0.3051\n","Epoch 283, acc0.91, loss 0.2613, tol 0, val_acc0.88, val_loss0.3099\n","Epoch 284, acc0.91, loss 0.2449, tol 0, val_acc0.88, val_loss0.3128\n","Epoch 285, acc0.91, loss 0.2439, tol 0, val_acc0.88, val_loss0.3044\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WPe1tQjjj-Sl"},"source":[""],"execution_count":null,"outputs":[]}]}