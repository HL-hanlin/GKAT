{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"InducedCycle_GKAT_2layer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"KU5so-4hURax"},"source":["# link colab to google drive directory where this project data is placed\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sLiADzRYUDtm"},"source":["################ Need to set project path here !!  #################\n","projectpath = # \"/content/gdrive/MyDrive/GraphAttnProject/SpanTree [with start node]_[walklen=3]_[p=1,q=1]_[num_walks=50]/NIPS_Submission/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0yY7zMECTaI5"},"source":["import os\n","os.chdir(projectpath)\n","os.getcwd()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lDTFxrCdpk6a"},"source":["! pip install dgl\n","import dgl"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GhSvd4yHmmaJ"},"source":["# Load data"]},{"cell_type":"code","metadata":{"id":"xIXBUNULmxtV"},"source":["\n","from tqdm.notebook import tqdm, trange\n","import networkx as nx\n","import pickle\n","import numpy as np\n","import tensorflow as tf\n","import torch\n","print(tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dVBtU5vAm7Ik"},"source":["# load all train and validation graphs\n","train_graphs = pickle.load(open(f'graph_data/train_graphs.pkl', 'rb'))\n","val_graphs = pickle.load(open(f'graph_data/val_graphs.pkl', 'rb'))\n","\n","# load all labels\n","train_labels = np.load('graph_data/train_labels.npy')\n","val_labels = np.load('graph_data/val_labels.npy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i1RHKaJMnKpl"},"source":["#################. NEED TO SPECIFY THE RANDOM WALK LENGTH WE WANT TO USE ################\n","walk_len = 6 # we use GKAT with random walk length of 6 in this code file \n","# we could also change this parameter to load GKAT kernel generated from random walks with different lengths from 2 to 10.\n","#########################################################################################"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rzPVFV3lntjd"},"source":["# here we load the frequency matriies (we could use this as raw data to do random feature mapping )\n","\n","train_freq_mat = pickle.load(open(f'graph_data/GKAT_freq_mats_train_len={walk_len}.pkl', 'rb'))\n","val_freq_mat = pickle.load(open(f'graph_data/GKAT_freq_mats_val_len={walk_len}.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ydogONEsntqC"},"source":["# here we load the pre-calculated GKAT kernel\n","\n","train_GKAT_kernel = pickle.load(open(f'graph_data/GKAT_dot_kernels_train_len={walk_len}.pkl', 'rb'))\n","val_GKAT_kernel = pickle.load(open(f'graph_data/GKAT_dot_kernels_val_len={walk_len}.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hcFF2MIuot12"},"source":["GKAT_masking = [train_GKAT_kernel, val_GKAT_kernel]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YRCTLcQdot17"},"source":["train_graphs = [ dgl.from_networkx(g) for g in train_graphs]\n","val_graphs = [ dgl.from_networkx(g) for g in val_graphs]\n","info = [train_graphs, train_labels, val_graphs, val_labels, GKAT_masking]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X92f3fstqGTC"},"source":["# START Training"]},{"cell_type":"code","metadata":{"id":"RCTlzoRDhafQ"},"source":["import networkx as nx\n","import matplotlib.pyplot as plt \n","import time\n","import random\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from tqdm.notebook import tqdm, trange\n","import seaborn as sns\n","\n","from random import shuffle\n","from multiprocessing import Pool\n","import multiprocessing\n","from functools import partial\n","from networkx.generators.classic import cycle_graph\n","\n","import sys\n","import scipy\n","import scipy.sparse\n","\n","#from CodeZip_ST import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F-PpelEHvaUF"},"source":["from prettytable import PrettyTable\n","\n","# this function will count the number of parameters in GKAT (will be used later in this code file)\n","def count_parameters(model):\n","    table = PrettyTable([\"Modules\", \"Parameters\"])\n","    total_params = 0\n","    for name, parameter in model.named_parameters():\n","        if not parameter.requires_grad: continue\n","        param = parameter.numel()\n","        table.add_row([name, param])\n","        total_params+=param\n","    print(table)\n","    print(f\"Total Trainable Params: {total_params}\")\n","    return total_params\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sHofaHfFnlmQ"},"source":["# GKAT Testing"]},{"cell_type":"markdown","metadata":{"id":"1_6nPfVFXe25"},"source":["## GKAT model"]},{"cell_type":"code","metadata":{"id":"bRkNVQccXg3U"},"source":["# this is the GKAT version adapted from the paper \"graph attention networks\"\n","class GKATLayer(nn.Module):\n","    def __init__(self,\n","                 in_dim,\n","                 out_dim,\n","                 feat_drop=0.,\n","                 attn_drop=0.,\n","                 alpha=0.2,\n","                 agg_activation=F.elu):\n","        super(GKATLayer, self).__init__()\n","\n","        self.feat_drop = nn.Dropout(feat_drop)\n","        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n","        #torch.nn.init.xavier_uniform_(self.fc.weight)\n","        #torch.nn.init.zeros_(self.fc.bias)\n","        self.attn_l = nn.Parameter(torch.ones(size=(out_dim, 1)))\n","        self.attn_r = nn.Parameter(torch.ones(size=(out_dim, 1)))\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.activation = nn.LeakyReLU(alpha)\n","        self.softmax = nn.Softmax(dim = 1)\n","\n","        self.agg_activation=agg_activation\n","\n","    def clean_data(self):\n","        ndata_names = ['ft', 'a1', 'a2']\n","        edata_names = ['a_drop']\n","        for name in ndata_names:\n","            self.g.ndata.pop(name)\n","        for name in edata_names:\n","            self.g.edata.pop(name)\n","\n","    def forward(self, feat, bg, counting_attn):\n","\n","        self.g = bg\n","        h = self.feat_drop(feat)\n","        head_ft = self.fc(h).reshape((h.shape[0], -1))\n","        \n","        a1 = torch.mm(head_ft, self.attn_l)    # V x 1\n","        a2 = torch.mm(head_ft, self.attn_r)     # V x 1\n","        a = self.attn_drop(a1 + a2.transpose(0, 1))\n","        a = self.activation(a)\n","\n","        maxes = torch.max(a, 1, keepdim=True)[0]\n","        a_ = a - maxes # we could subtract max to make the attention matrix bounded. (not feasible for random feature mapping decomposition)\n","        a_nomi = torch.mul(torch.exp(a_), counting_attn.float())\n","        a_deno = torch.sum(a_nomi, 1, keepdim=True)\n","        a_nor = a_nomi/(a_deno+1e-9)\n","\n","        ret = torch.mm(a_nor, head_ft)\n","\n","        if self.agg_activation is not None:\n","            ret = self.agg_activation(ret)\n","\n","        return ret\n","\n","# this is the GKAT version adapted from the paper \"attention is all you need\"\n","class GKATLayer(nn.Module):\n","    def __init__(self, in_dim, out_dim, feat_drop=0., attn_drop=0., alpha=0.2, agg_activation=F.elu):\n","        super(GKATLayer, self).__init__()\n","\n","        self.feat_drop = feat_drop  #nn.Dropout(feat_drop, training=self.training)\n","        self.attn_drop = attn_drop  #nn.Dropout(attn_drop)\n","        \n","        self.fc_Q = nn.Linear(in_dim, out_dim, bias=False)\n","        self.fc_K = nn.Linear(in_dim, out_dim, bias=False)\n","        self.fc_V = nn.Linear(in_dim, out_dim, bias=False)\n","        \n","        self.softmax = nn.Softmax(dim = 1)\n","\n","        self.agg_activation=agg_activation\n","\n","            \n","    def forward(self, feat, bg, counting_attn):\n","        h = F.dropout(feat, p=self.feat_drop, training=self.training)\n","\n","        Q = self.fc_Q(h).reshape((h.shape[0], -1))\n","        K = self.fc_K(h).reshape((h.shape[0], -1))\n","        V = self.fc_V(h).reshape((h.shape[0], -1))\n","        \n","        logits = F.dropout( torch.matmul( Q, torch.transpose(K,0,1) ) , p=self.attn_drop, training=self.training) / np.sqrt(Q.shape[1])\n","\n","        maxes = torch.max(logits, 1, keepdim=True)[0]\n","        logits =  logits - maxes\n","        \n","        a_nomi = torch.mul(torch.exp( logits  ), counting_attn.float())\n","        a_deno = torch.sum(a_nomi, 1, keepdim=True)\n","        a_nor = a_nomi/(a_deno+1e-9)\n","\n","        ret = torch.mm(a_nor, V)\n","        if self.agg_activation is not None:\n","            ret = self.agg_activation(ret)\n","\n","        return ret"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vE9B2J45XiJd"},"source":["class GKATClassifier(nn.Module):\n","    def __init__(self, in_dim, hidden_dim, num_heads, n_classes, feat_drop_=0.,\n","                 attn_drop_=0.,):\n","        super(GKATClassifier, self).__init__()\n","\n","        self.num_heads = num_heads\n","        self.hidden_dim = hidden_dim\n","        self.layers = nn.ModuleList([\n","            nn.ModuleList([GKATLayer(in_dim, hidden_dim[0], feat_drop = feat_drop_, attn_drop = attn_drop_, agg_activation=F.elu) for _ in range(num_heads)]),\n","            nn.ModuleList([GKATLayer(hidden_dim[0] * num_heads, hidden_dim[-1], feat_drop = feat_drop_, attn_drop = attn_drop_, agg_activation=F.elu) for _ in range(1)])\n","        ])\n","        self.classify = nn.Linear(hidden_dim[-1] * 1, n_classes)\n","        self.softmax = nn.Softmax(dim = 1)\n","\n","    def forward(self, bg, counting_attn, normalize = 'normal'):\n","\n","        h = bg.in_degrees().view(-1, 1).float() # use degree as features \n","        num_nodes = h.shape[0]\n","        features = h.numpy().flatten()\n","        \n","        if normalize == 'normal':\n","            mean_ = np.mean(features)\n","            std_ = np.std(features)\n","            h = (h - mean_)/(std_+1e-9)\n","\n","        for i, gnn in enumerate(self.layers):\n","            all_h = []\n","            for j, att_head in enumerate(gnn):\n","                all_h.append(att_head(h, bg, counting_attn))   \n","            h = torch.squeeze(torch.cat(all_h, dim=1))\n","\n","        bg.ndata['h'] = h\n","        hg = dgl.mean_nodes(bg, 'h')\n","        return self.classify(hg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eQY1EcS7xtYT"},"source":["# the following are the parameters we used in GKAT version adapted from \"graph attention networks\"\n","\n","method = 'GKAT'\n","\n","runtimes = 15 # the number of repeats \n","\n","num_classes = 2 \n","num_features = [4, 4] # use hidden dimension of 4 in each attention head\n","num_heads = 8 # use 8 heads\n","num_layers = 2 # use a two layer GKAT model\n","\n","feature_drop = 0\n","atten_drop = 0\n","\n","epsilon = 1e-4\n","\n","start_tol = 499\n","tolerance = 80\n","max_epoch = 500\n","batch_size = 128\n","learning_rate = 0.005"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TwRYodt8zf7S"},"source":["# the following are the parameters we used in GKAT version adapted from \"attention is all you need\"\n","\n","method = 'GKAT'\n","\n","runtimes = 15\n","num_classes = 2\n","num_features = [4, 2]\n","num_heads = 7\n","num_layers = 2\n","feature_drop = 0\n","atten_drop = 0\n","\n","epsilon = 1e-4\n","\n","start_tol = 499\n","tolerance = 80\n","max_epoch = 500\n","batch_size = 128\n","learning_rate = 0.005"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LT0BHPfQ0IWZ"},"source":["all_GKAT_train_losses = []\n","all_GKAT_train_acc = []\n","all_GKAT_val_losses = []\n","all_GKAT_val_acc = []\n","\n","ckpt_file = f'results_{num_layers}layers/{method}/{method}_ckpt.pt'\n","\n","for runtime in trange(runtimes):\n","    \n","\n","    train_graphs, train_labels, val_graphs, val_labels, GKAT_masking = info\n","    train_GKAT_masking, val_GKAT_masking = GKAT_masking\n","                                                 \n","    # Create model\n","    model = GKATClassifier(1, num_features, num_heads, num_classes, feat_drop_ = feature_drop, attn_drop_ = atten_drop)\n","\n","    for p in model.parameters():\n","      if p.dim() > 1:\n","          nn.init.xavier_uniform(p)\n","\n","    count_parameters(model)\n","\n","\n","    #model.apply(init_weights)\n","    loss_func = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n","    model.train()\n","\n","    epoch_train_losses_GKAT = []\n","    epoch_train_acc_GKAT = []\n","    epoch_val_losses_GKAT = []\n","    epoch_val_acc_GKAT = []\n","\n","    num_batches = int(len(train_graphs)/batch_size)\n","\n","    epoch = 0\n","    nan_found = 0\n","    tol = 0 \n","\n","    while True:\n","        if nan_found:\n","          break\n","        \n","        epoch_loss = 0\n","        epoch_acc = 0\n","\n","        ''' Training '''\n","        for iter in range(num_batches):\n","        #for iter in range(2): \n","            predictions = []\n","            labels = torch.empty(batch_size)\n","            rand_indices = np.random.choice(len(train_graphs), batch_size, replace=False)\n","\n","            for b in range(batch_size): \n","                predictions.append(model(train_graphs[rand_indices[b]], torch.Tensor(train_GKAT_masking[rand_indices[b]])))\n","                \n","                if torch.isnan(predictions[b][0])[0]:\n","                  print('NaN found.')\n","                  break\n","                #print(predictions[b].detach().numpy())\n","                \n","                labels[b] = train_labels[rand_indices[b]]\n","            \n","            acc = 0\n","            for k in range(len(predictions)):\n","              if predictions[k][0][0]>predictions[k][0][1] and labels[k]==0:\n","                acc += 1\n","              elif predictions[k][0][0]<=predictions[k][0][1] and labels[k]==1:\n","                acc += 1\n","            acc /= len(predictions)  \n","            epoch_acc += acc  \n","            \n","            predictions = torch.squeeze(torch.stack(predictions))\n","            if torch.any(torch.isnan(predictions)):\n","                  print('NaN found.')\n","                  nan_found = 1\n","                  break\n","            \n","            loss = loss_func(predictions, labels.long())\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.detach().item()\n","\n","        epoch_acc /= (iter + 1)\n","        epoch_loss /= (iter + 1)\n","\n","        val_acc = 0\n","        val_loss = 0\n","        predictions_val = []\n","\n","        for b in range(len(val_graphs)): \n","\n","            predictions_val.append(model(val_graphs[b], torch.Tensor(val_GKAT_masking[b])))\n","        \n","        for k in range(len(predictions_val)):\n","          if predictions_val[k][0][0]>predictions_val[k][0][1] and val_labels[k]==0:\n","            val_acc += 1\n","          elif predictions_val[k][0][0]<=predictions_val[k][0][1] and val_labels[k]==1:\n","            val_acc += 1\n","              \n","        val_acc /= len(val_graphs)\n","\n","        predictions_val = torch.squeeze(torch.stack(predictions_val))    \n","        loss = loss_func(predictions_val, torch.tensor(val_labels).long())\n","        val_loss += loss.detach().item()\n","\n","\n","        if len(epoch_val_losses_GKAT) ==0:\n","          try:\n","            os.remove(f'{projectpath}{ckpt_file}')\n","          except:\n","            pass\n","          torch.save(model, f'{projectpath}{ckpt_file}')\n","          print('Epoch {}, acc{:.2f}, loss {:.4f}, tol {}, val_acc{:.2f}, val_loss{:.4f} -- checkpoint saved'.format(epoch, epoch_acc, epoch_loss, tol, val_acc, val_loss))\n","        elif (np.min(epoch_val_losses_GKAT) >= val_loss) and (np.max(epoch_val_acc_GKAT) <= val_acc): \n","          torch.save(model, f'{projectpath}{ckpt_file}')\n","          print('Epoch {}, acc{:.2f}, loss {:.4f}, tol {}, val_acc{:.2f}, val_loss{:.4f} -- checkpoint saved'.format(epoch, epoch_acc, epoch_loss, tol, val_acc, val_loss))\n","        else:\n","          print('Epoch {}, acc{:.2f}, loss {:.4f}, tol {}, val_acc{:.2f}, val_loss{:.4f}'.format(epoch, epoch_acc, epoch_loss, tol, val_acc, val_loss))\n","\n","\n","        if epoch > start_tol:\n","          if np.min(epoch_val_losses_GKAT) <= val_loss: \n","            tol += 1\n","            if tol == tolerance: \n","                print('Loss do not decrease')\n","                break\n","          else:\n","            if np.abs(epoch_val_losses_GKAT[-1] - val_loss)<epsilon:\n","                print('Converge steadily')\n","                break\n","            tol = 0\n","\n","                    \n","        if epoch > max_epoch:\n","            print(\"Reach Max Epoch Number\")\n","            break\n","\n","        epoch += 1\n","        epoch_train_acc_GKAT.append(epoch_acc)\n","        epoch_train_losses_GKAT.append(epoch_loss)\n","        epoch_val_acc_GKAT.append(val_acc)\n","        epoch_val_losses_GKAT.append(val_loss)\n","\n","    all_GKAT_train_acc.append(epoch_train_acc_GKAT)\n","    all_GKAT_train_losses.append(epoch_train_losses_GKAT)\n","    all_GKAT_val_acc.append(epoch_val_acc_GKAT)\n","    all_GKAT_val_losses.append(epoch_val_losses_GKAT)\n","\n","    # save results from current repeat to the following file\n","    np.save(f'{projectpath}results_{num_layers}layers/epoch_train_acc_{method}_walklen{walk_len}_run{runtime}.npy', epoch_train_acc_GKAT)\n","    np.save(f'{projectpath}results_{num_layers}layers/epoch_val_acc_{method}_walklen{walk_len}_run{runtime}.npy', epoch_val_acc_GKAT)\n","    np.save(f'{projectpath}results_{num_layers}layers/epoch_train_losses_{method}_walklen{walk_len}_run{runtime}.npy', epoch_train_losses_GKAT)\n","    np.save(f'{projectpath}results_{num_layers}layers/epoch_val_losses_{method}_walklen{walk_len}_run{runtime}.npy', epoch_val_losses_GKAT)\n","\n","# all all results to the following file\n","np.save(f'{projectpath}results_{num_layers}layers/all_{method}_walklen{walk_len}_train_losses.npy', all_GKAT_train_losses)\n","np.save(f'{projectpath}results_{num_layers}layers/all_{method}_walklen{walk_len}_train_acc.npy', all_GKAT_train_acc)\n","np.save(f'{projectpath}results_{num_layers}layers/all_{method}_walklen{walk_len}_val_losses.npy', all_GKAT_val_losses)\n","np.save(f'{projectpath}results_{num_layers}layers/all_{method}_walklen{walk_len}_val_acc.npy', all_GKAT_val_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"16gycyQwFWDt"},"source":[""],"execution_count":null,"outputs":[]}]}